{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5bc642a",
   "metadata": {},
   "source": [
    "## Experiment 1[<Insert title of Experiment>]\n",
    "### Purpose\n",
    "- Setting up Classification problem and visualizing the model accuracy based on permutations of 3-tuple that references the model parameters. \n",
    "### Hypothesis\n",
    "- I believe that the ratio of constants in the 3-tuple should be <> of the ML Model paramters for the ML Model's accurarcy to be greater than the ML Model's accuracy when the ratio is not used. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99547592",
   "metadata": {},
   "source": [
    "### Instructions to Run\n",
    "Instructions: 1) Setting up Model, 2) Using three situations where the 3-tupeles are different.Context: First we run code that creates the datasets needed to be fed to the model: [note, make habit to employ content-codeSnippet pairs!]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f18a7f",
   "metadata": {},
   "source": [
    "- Context: First we do the data preprocessing before it meets with the model[part a)]: [note, make habit to employ content-codeSnippet pairs!]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0962fa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing Dependencies and or code for setting up virtual environment: \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# End of Installing Dependencies and or code for setting up virtual environment: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303986b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Fill in this portion with data engineering procedure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf72116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This file will be built based on the contents of the purpose.md file. As of now, the amount of functiosn here, should be the number of subsystems, and then those subsystems' steps should have their own functions. This will help with doing component-based debugging opposed to dealing with all dependencies at once. If functions are nested, make sure to denote them as such. \n",
    "\n",
    "# Reports: [10/23/25] part 1) At this point, finished writing abstract rep of how prog will acheive goal. Started using a functional prog format to acheive goals. In next sesh, focus should be on finishing up setting up coding env programatically FIRST, and then inserting the neccessary pandas stuff, whilst simulatnatneously doing more research on pandas as well. part 2) Continued learning pandas stuff, and continued seting up code env programatically. In next sesh, continue the aforementioned process. When significant prog is made, begin inserting code related to data engineering. Also, when establishing pipeline this search query may be useful: `start chrome \"? is the relative path for a filepath in python script relative to the place it is executed?\"`. [10/24/25] part 1) Continued seting of programmatic env. At this point, I believe Subssytem2's programmatic rep should be worked on next, whilst simulataneously implementing more pandas-specific code. part 2) Continued working on programming code. Adhere to aforemetioend request for next session. part 3) Starting setting up Subsystem2's programmatic representation. In next sesh, make sure to implement a quick algo that seraches for the right table to pull from both the real-time data and the historical data, in addition to the aforementioned tasks from previous report(s). part 4) Started process for integrating external dataframes. Did not adhere to aformentioned objs directly. In next sesh, make sure to adhere to aformentioned objectives. part 5) Started wriritng logic code to facilitate optimality algo. Also worked on algo for obtaining the right attrs from the Ticker Object. In next sesh, take time to finish up Pandas tutorial. part 6) Continued where I left off. In next sesh, start testing out certain portions of program. part 7) Got the algorithms implemented, began testingg parts of program. Now, the only problem is having the varaibels reference the right columns. These places are marked with REPLACEMENT PENDING. In next sesh, go back to learning some more pandas and then work on finishing up the rest of the stuff. May be wise to jump to Time Series data chapter from Python for Data Analysis Textbook to interface with dates properly.\n",
    "# Reports: [10/26/25] part 1) Made more progress by fixing contents relative to error. Had to put the dataframes needd into the python list to be accessed later in the script. In next sesh, continue this process and addresss the 'REPLACEMENT PENDING' notes and make neccessary replacements. part 2) Added a few more things. Added nomenclature for documentation w.r.t comments, to ease the process of problem solving. Addressed a few of the requirements. part 2) Started testing other stuff. Didn't do anything related to time series learning for pandas. In next sesh, focus on learning pandas from geeksforgeeks and going through times series chapter from that textbook. Then, finish this file to integrate to model with this egnienered dataset. Also, there is an error, use code output for reference. part 3) Found some logistical issues wiht some parts. Started process of getting 2nd cond implemented. Really, want to get this proj finished by end of 10/27/25 to get a decent grade.  [10/27/25] part 1) Worked on outermost pipeline script. Going forward, working in reverse order from end of data prep pipeline to beginning to ensure that the nested function(s) and function(s) work properly. part 2) Have some issues getting condition 2. part 3) Got dataframe for condition 2. Think now I need to designate the date. Will be KEY for making decisions. part 3) Made a decent amount of progress. Used comments to update some steps that need to be taken in order to ensure smooth integartion between data prep and model development. In next sesh, continue solving problems to get data prepped for model dev. Particularly, write code that applys the subsystems iteratively to each company. part 4) got subsystem1(compA()) complete, in terms of logic. However, will be imperative to handle presence of null values and omitting them when possible. **PN: I believe it has something to do with the fact that the indices for the row tuples is date-based**. In next sesh, continue to the subsystem2 to ensure that they work logically. part 5) Started working on logic for subsys2. Noticed that I need to handle the creation of dataframe with all companies on it properly. Need to figure out how to do that in subsys2. [10/29/25] part 1) Got the class for subsy2 indented properly for testing. In next sesh, continue runnign tests to ensure that subsy2's components work properly.  [10/31/25] part 1) Started working more on model file to facilitate process for running experiments via Model pipeline. In Next sesh, continue learning more about pandas, and datetime object so you can sync up the times properly to get values lined up to get perimissible data output. Also, may want to make time working on reasons why these milestone were so late. [11/7/25] part 1) Made significant progress. Finally understanding debugging process better now. **Focus is now shifting more towards experimental setups, and creating environment to load images to web application**. \n",
    "\n",
    "# Notable things: a) This search query specifies that it's possible to access attributes for a class. THis allows me to programatically identify the stuff that I need opposed to doing things brute force: `start chrome \"? is there a list of attribues for a particular class in Python?\" `b) Make sure to make copy of this python file and then run it in the MLLifecycle directory. c) NOTE: Think it'd be wise to insert some breakpoints/places to use pdb to ensure that things run as intended. I believe it will make debugging more efficient. d) As of 11/3/25, my new problem is syncing the dates for the data. Iniiat soln consisted of using start and end parameters to specify the bounds needed and then syncing the dates that way. Before this is implemented, I need to ensure that the rest of pipeline is provably correct still so my it soln can be plugged and played. e) As of 11/6/25, the solution is nearly complte. At this point, the problem is ensuring that n companies are allowed to be entered, and ensuring that at the end, each company has its own row-tuple.  f) Everything works up until after checkpoint #2 from a provabiilty perspective as of 11/6/25 . ALSO, NEED TO ADD A STATE THAT ALLOWS USER TO SPECIFY THE STOCKS THEY WANT TO CHOOSE. \n",
    "\n",
    "# IMPORTANT: As of 11/22/25: The main focus now is ensuring that the Price from P/B and P/E are pulled from correct data. After that, everything should work properly! To verify, review code again and add any updates here: a) Learned that open should be replaced with closing since, according to net, it refers to the actual price. \n",
    "# Necessary imports\n",
    "import pandas as pd;\n",
    "import numpy as np;\n",
    "import yfinance as yf; #<-- Needed to access Yahoo Finance Dataset(s)\n",
    "# Below is needed to get the real-time data\n",
    "from datetime import datetime, timedelta, date\n",
    "# UPDATE: timedelta will be used to programmatically do the optimality algorithm. Refer to notes on timedelta class for reference.  \n",
    "import pdb as pb #<-- NOTE: Need to use this to test certain sections of code to debug and make neccessary\n",
    "# End of Necessary imports\n",
    "\n",
    "# NOTE: Make sure certain vars declared are nonlocal so they can be used in other functions!\n",
    "global seriesVersionOn\n",
    "seriesVersionOn = True\n",
    "global resultantDataFrame\n",
    "resultantDataFrame: pd.DataFrame = pd.DataFrame(columns=[\"P/B\", \"P/E\", \"NCAV\", \"Date For Eval\", \"Company\", \"Optimality\"])\n",
    "resultantDataFrame[\"P/B\"].astype(\"Float64\")\n",
    "resultantDataFrame[\"P/E\"].astype(\"Float64\")\n",
    "resultantDataFrame[\"NCAV\"].astype(\"Float64\")\n",
    "# resultantDataFrame[\"Date for Eval\"].astype(datetime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538c08f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class subsys1:\n",
    "    def __init__(self,param1 = \"\"\"Insert any params that may be sufficient here!\"\"\"):\n",
    "        print(\"---Subsystem 1 in Progress---\")\n",
    "        # self.compA(param1)\n",
    "    # component a) [mapping formulas using dep vars and indep vars]\n",
    "    def compA(self,company): #<-- NOTE: Thought about passing in dataframe as a parameter as well for instances where multiple companies need to be added to resultant dataframe. \n",
    "        # PRatioBug = False\n",
    "        PRatioBug = True\n",
    "        print(\"--Component a[Subsystem 1] in progress--\")\n",
    "        global arrOfDataFramesNeeded \n",
    "        #arrOfDataFramesNeeded: list[pd.DataFrame] = []\n",
    "        arrOfDataFramesNeeded = []\n",
    "         \n",
    "        # Obtaining the dataframes from the yfinance api for company i\n",
    "        print(\"-Obtaining the DataFrames from the yfinance api for company i-\")\n",
    "        timePeriod = \"4mo\"\n",
    "        timeInterval = \"1mo\"\n",
    "        # PN: Since Price, Books will be used will need real-time data. \n",
    "        # Obtaining real-time data\n",
    "        if (company == None):\n",
    "            company = \"MSFT\"\n",
    "        \n",
    "        companyAlias = company or \"MSFT\"#<-- For Microsoft[using default operator here to prep function to be generalized to add other companies]\n",
    "        # pb.set_trace()\n",
    "        ticker = yf.Ticker(f\"{companyAlias}\") #<-- NOTE: Will need to replace this with multiple tickers later on. \n",
    "        #ticker.info\n",
    "        #ticker.info.keys() #<-- Cotnains p/b ratio but doesn't have p/e ratio. So, will not be using it. \n",
    "        \"\"\"\n",
    "        listOfAttrs = dir(ticker)\n",
    "        for attrib in listOfAttrs:\n",
    "            # psuedosteps: a) Accessing curr atrib, b) if it rets a dataframe, then checking columns to see if it matches what I Pneed. \n",
    "            #for j in range(6): #<-- replacing later on, just a placeholder rn.\n",
    "           if \"High\" in getattr(ticker,attrib).columns:\n",
    "           # Means that the attrib should be used to obtain data needed:\n",
    "           arrOfAttrsFromExtDataFramesNeeded.append(attrib)\n",
    "           # ^^ NOTE: Above needs to be generalized[may need a nested for loop before this if statement to go through ALL requested col ids]\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        global end_date #<-- Set gloabl so end_date can be accessed. \n",
    "        start_date: datetime = datetime.today() - timedelta(days=10) ; end_date = datetime.today() #+ timedelta(days=1*365) <-- UPDATE: Took this out b/c start_date needs to begin in past, and end date needs to be in present. \n",
    "        yearsRelToDateInQ: datetime =  start_date - timedelta(days=5*365)\n",
    "        start_date = start_date.date().isoformat()\n",
    "        end_date = end_date.date().isoformat()\n",
    "        # UPDATE: Below, I setup environmetn such that, when start_date and end_date are entered, then the pipeline works as expected. \n",
    "        # historical_data = ticker.history(period=timePeriod, interval=timeInterval).tz_localize(None) if start_date == None and end_date == None else ticker.history(start=start_date.date().isoformat(), end=end_date.date().isoformat()).tz_localize(None) <-- commented out for obvious reasons. \n",
    "        historical_data = ticker.history(period=timePeriod, interval=timeInterval).tz_localize(None) if start_date == None and end_date == None else ticker.history(start=start_date, end=end_date).tz_localize(None)\n",
    "        print(historical_data)\n",
    "        # end of Obtaining real-time data\n",
    "        \n",
    "        # PN: I believe NCAV == Net Tangible Assets\n",
    "        # Obtaining historical data\n",
    "        # quarterlyBalanceSheet = ticker.balance_sheet.T \n",
    "        quarterlyBalanceSheet = ticker.quarterly_balance_sheet.T \n",
    "        # UPDATE[NOTE]: Will need body that queries for row tuples in the start_date and end_date range\n",
    "        \n",
    "        arrOfDataFramesNeeded = [historical_data, quarterlyBalanceSheet]\n",
    "        print(quarterlyBalanceSheet)\n",
    "        print(quarterlyBalanceSheet.columns)\n",
    "        # Below involves getting 5 yr data to support condition #2. \n",
    "        # ISSUE #1[complete]\n",
    "        #conditionForDataFrame2 = ticker.history(start=arrOfDataFramesNeeded[0].index[0],period=\"1y\", interval=\"5y\").tz_localize(None) #<-- Here, I set start date to first tuple's date from historical_data var and then get the data 5 yrs BACK relative to that date. \n",
    "        conditionForDataFrame2 = ticker.history(period=\"5y\").tz_localize(None) #<-- Here, I set start date to first tuple's date from historical_data var and then get the data 5 yrs BACK relative to that date. \n",
    "        balanceSheet = ticker.balance_sheet.T \n",
    "        # Body of assigning P/E column\n",
    "\n",
    "        \n",
    "        # End of Body of assigning P/E column\n",
    "        # Below, the dates within the bounds of start date and 5 years succeeding start date index is queried to obtain relevant data to be used later. \n",
    "        print(\"---Setting checkpoint to see what happens to conditionForDataFrame2 variable---\")\n",
    "        # pb.set_trace()\n",
    "        # conditionForDataFrame2 = conditionForDataFrame2 if start_date == None and end_date == None else conditionForDataFrame2.loc[start_date:yearsRelToDateInQ, :];\n",
    "        # NOTE: ^^ Above caused an error due to else body of ternary operator. Thus, statement has been commented out. \n",
    "        \n",
    "        # End of body of testing out bounds interval ideas\n",
    "        print(conditionForDataFrame2)\n",
    "        # END OF ISSUE #1\n",
    "        arrOfDataFramesNeeded.append([conditionForDataFrame2])\n",
    "        # end of Obtaining historical data\n",
    "        print(\"-End of Obtaining the DataFrames from the yfinance api for company i-\")\n",
    "        print(\"Entering Debuging Mode for Checkpoint #1\")\n",
    "        # pb.set_trace() #<-- Will use this   tcheck for vals of variable(s), as well as set certain vars to certain values to cause different behavior.   \n",
    "        # CHECKPOINT #1: In proof, at this point, datafmres will be obtained to begin process of assigning formulas.           # For Everything else, I just need historical data. Correct me if wrong here: .\n",
    "        # End of Obtaining the dataframes from the yfinance api for company i\n",
    "\n",
    "        # Debugging Report [11/13/25] part 1) When running through debugger, found that the columns are assigned properly, BUT some of the computations for the data engineering results in NaN values. Need to figure out why, think it can be solved by syncing datetime(s). Continue progress on line 111 via the debugger. Jump to that line by finding command to jump to line 111. \n",
    "        #return\n",
    "        boolExpMain = \"\"\"Can insert a bool exp that checks for number of values in company column, indicating that dataFrame still exists\"\"\"\n",
    "        alreadyExists = False if boolExpMain == True else True  #<-- setting to 0 by default\n",
    "        print(\"-Beginning of assigning formulas-\")\n",
    "        global independentVars\n",
    "        \n",
    "        # independentVars = [\"High\" if PRatioBug == False else \"Share Issued\",\"Tangible Book Value\",\"High\",\"Retained Earnings\",\"Net Tangible Assets\"]\n",
    "        independentVars = [\"Close\" if PRatioBug == False else \"Open\",\"Tangible Book Value\",\"High\",\"Retained Earnings\",\"Net Tangible Assets\"]\n",
    "        global DependentVars\n",
    "        DependentVars = [\"P/B\", \"P/E\", \"NCAV\", \"Date For Eval\"] #<-- PN: Date for Eval was created to reference the date to use to make relevant decisions.  \n",
    "        DependentVars.append(\"Share Price\")\n",
    "        global dataFramesToBeShipped\n",
    "        dataFramesToBeShipped = [pd.DataFrame(), pd.DataFrame()]\n",
    "        DependentVars.append(\"Company\")\n",
    "        # NOTE: alreadyExists prevents currentDataframe from being updated allowing other companies to be added to resultant dataframe. \n",
    "        dataFramesToBeShipped[0] = pd.DataFrame(columns=DependentVars) if alreadyExists == False else dataFramesToBeShipped[0] #<-- TrainingData\n",
    "        print(dataFramesToBeShipped[0])\n",
    "        DependentVars.append(\"Optimality\")\n",
    "        dataFramesToBeShipped[1] = pd.DataFrame(columns=DependentVars) if alreadyExists == False else dataFramesToBeShipped[1] #<-- TestingData\n",
    "        print(dataFramesToBeShipped[1])\n",
    "        # Assigning the formulas\n",
    "        print(\"-Assigning formulas-\")\n",
    "        # pb.set_trace() <-- COMPLETE![Assinging formulas worked as intended now!]\n",
    "        # NOTE: When assigning, it makes sense to only apply formula to most recent data from quarterlyBalanceSheet aka the first tuple. \n",
    "        # dataFramesToBeShipped[0][DependentVars[0]] = arrOfDataFramesNeeded[0][independentVars[0]]/arrOfDataFramesNeeded[1][independentVars[1]] #<-- Causing error due to: KeyError: 'Tangible Book Value' .[UPDATE: To solve, incremented index in second operand from 0 to 1. part 2) IT WORKED!] #<-- NOTE: References old version. New version adheres to update. \n",
    "        # dataFramesToBeShipped[0][DependentVars[0]] = arrOfDataFramesNeeded[0].loc[arrOfDataFramesNeeded[0].index[0].date().isoformat(),independentVars[0]]/arrOfDataFramesNeeded[1].loc[arrOfDataFramesNeeded[0].index[0].date().isoformat(),independentVars[1]] #<-- Causing error due to: KeyError: 'Tangible Book Value' .[UPDATE: To solve, incremented index in second operand from 0 to 1. part 2) IT WORKED!][UPDATE as of 11/19/25: Error involving key entry. Not sure why, make sure this is focus in next session][part 2) Removing isoformat to see what happens]\n",
    "        # NOTE: Need a conditional change here, based on PBRatio boolean. \n",
    "        pullingFromTickerInfo = True\n",
    "        if not PRatioBug:\n",
    "            dataFramesToBeShipped[0].loc[0,DependentVars[0]] = arrOfDataFramesNeeded[0].loc[arrOfDataFramesNeeded[0].index[0].date().isoformat(),independentVars[0]]/arrOfDataFramesNeeded[1].loc[arrOfDataFramesNeeded[1].index[0].date().isoformat(),independentVars[1]] #<-- Causing error due to: KeyError: 'Tangible Book Value' .[UPDATE: To solve, incremented index in second ope rand from 0 to 1. part 2) IT WORKED!][UPDATE as of 11/19/25: Error involving key entry. Not sure why, make sure this is focus in next session]\n",
    "            print(\"---\")\n",
    "            print(arrOfDataFramesNeeded[0].columns)\n",
    "            print(\"---\")\n",
    "            print(arrOfDataFramesNeeded[0].loc[arrOfDataFramesNeeded[0].index[0].date().isoformat(),independentVars[0]])\n",
    "            print(arrOfDataFramesNeeded[1].loc[arrOfDataFramesNeeded[1].index[0].date().isoformat(),independentVars[1]])\n",
    "            dataFramesToBeShipped[0].loc[0,DependentVars[1]] = arrOfDataFramesNeeded[0].loc[arrOfDataFramesNeeded[0].index[0].date().isoformat(),independentVars[2]]/arrOfDataFramesNeeded[1].loc[arrOfDataFramesNeeded[1].index[0].date().isoformat(),independentVars[3]]\n",
    "            dataFramesToBeShipped[0].loc[0,DependentVars[2]] = arrOfDataFramesNeeded[1].loc[arrOfDataFramesNeeded[1].index[0].date().isoformat(),independentVars[4]] #<-- Here, NCAV comes from balance sheet. \n",
    "            print(dataFramesToBeShipped[0])\n",
    "            # seriesVersionOn = True <-- This is now a global variable. \n",
    "            # end of Assigning the formulas\n",
    "        elif PRatioBug and pullingFromTickerInfo == True:\n",
    "            # Need to chang below based on fact that High turns into Shares Issued from arrOfDataFramesNeeded[1] . \n",
    "            # print(\"--DEBUGGING CHECKPOINT: Adressing behavior causing error involving Share Price issue---\")\n",
    "            # pb.set_trace()\n",
    "            # Decided to replace Price with ticker.info[\"regularMarketPrice\"]\n",
    "            # dataFramesToBeShipped[0].loc[0,DependentVars[0]] = ticker.info[\"regularMarketPrice\"]/arrOfDataFramesNeeded[1].loc[arrOfDataFramesNeeded[1].index[0].date().isoformat(),independentVars[1]] #<-- Causing error due to: KeyError: 'Tangible Book Value' .[UPDATE: To solve, incremented index in second ope rand from 0 to 1. part 2) IT WORKED!][UPDATE as of 11/19/25: Error involving key entry. Not sure why, make sure this is focus in next session]\n",
    "            dataFramesToBeShipped[0].loc[0,DependentVars[0]] = ticker.info[\"priceToBook\"] #<-- Causing error due to: KeyError: 'Tangible Book Value' .[UPDATE: To solve, incremented index in second ope rand from 0 to 1. part 2) IT WORKED!][UPDATE as of 11/19/25: Error involving key entry. Not sure why, make sure this is focus in next session]\n",
    "            print(\"---\")\n",
    "            print(arrOfDataFramesNeeded[0].columns)\n",
    "            print(\"---\")\n",
    "            print(ticker.info[\"regularMarketPrice\"])\n",
    "            print(arrOfDataFramesNeeded[1].loc[arrOfDataFramesNeeded[1].index[0].date().isoformat(),independentVars[1]])\n",
    "            dataFramesToBeShipped[0].loc[0,DependentVars[1]] = ticker.info[\"regularMarketPrice\"]/ticker.info[\"earningsQuarterlyGrowth\"]\n",
    "\n",
    "            \n",
    "            dataFramesToBeShipped[0].loc[0,DependentVars[2]] = arrOfDataFramesNeeded[1].loc[arrOfDataFramesNeeded[1].index[0].date().isoformat(),independentVars[4]] #<-- Here, NCAV comes from balance sheet. \n",
    "            # dataFramesToBeShipped[0].loc[0, DependentVars[3]] = ticker.info[\"regularMarketPrice\"]\n",
    "            print(dataFramesToBeShipped[0])\n",
    "            print(\"--DEBUGGING CHECKPOINT: Ensuring that share price returns corrrect value aka ensuring that attrubte is populated properly--\")\n",
    "            # pb.set_trace()\n",
    "            dataFramesToBeShipped[0].loc[0,\"Share Price\"] = ticker.info[\"regularMarketPrice\"]\n",
    "            # seriesVersionOn = True <-- This is now a global variable. \n",
    "            # end of Assigning the formulas\n",
    "        else:\n",
    "            # Need to chang below based on fact that High turns into Shares Issued from arrOfDataFramesNeeded[1] . \n",
    "            print(\"--DEBUGGING CHECKPOINT: Adressing behavior causing error involving Share Price issue---\")\n",
    "            # pb.set_trace()\n",
    "            # Decided to replace Price with ticker.info[\"regularMarketPrice\"]\n",
    "            dataFramesToBeShipped[0].loc[0,DependentVars[0]] = ticker.info[\"regularMarketPrice\"]/arrOfDataFramesNeeded[1].loc[arrOfDataFramesNeeded[1].index[0].date().isoformat(),independentVars[1]] #<-- Causing error due to: KeyError: 'Tangible Book Value' .[UPDATE: To solve, incremented index in second ope rand from 0 to 1. part 2) IT WORKED!][UPDATE as of 11/19/25: Error involving key entry. Not sure why, make sure this is focus in next session]\n",
    "            print(\"---\")\n",
    "            print(arrOfDataFramesNeeded[0].columns)\n",
    "            print(\"---\")\n",
    "            print(ticker.info[\"regularMarketPrice\"])\n",
    "            print(arrOfDataFramesNeeded[1].loc[arrOfDataFramesNeeded[1].index[0].date().isoformat(),independentVars[1]])\n",
    "            dataFramesToBeShipped[0].loc[0,DependentVars[1]] = ticker.info[\"regularMarketPrice\"]/arrOfDataFramesNeeded[1].loc[arrOfDataFramesNeeded[1].index[0].date().isoformat(),independentVars[3]]\n",
    "\n",
    "            \n",
    "            dataFramesToBeShipped[0].loc[0,DependentVars[2]] = arrOfDataFramesNeeded[1].loc[arrOfDataFramesNeeded[1].index[0].date().isoformat(),independentVars[4]] #<-- Here, NCAV comes from balance sheet. \n",
    "            print(dataFramesToBeShipped[0])\n",
    "            # seriesVersionOn = True <-- This is now a global variable. \n",
    "            # end of Assigning the formulas\n",
    "        print(\"-End of assigning formulas-\")\n",
    "        if(seriesVersionOn):\n",
    "            dataFramesToBeShipped[0].loc[0, \"Company\"] = companyAlias\n",
    "        print(\"--Entering Debuging Mode for Checkpoint #2--\")\n",
    "        # pb.set_trace() #<-- Will use this to check for vals of variable(s), as well as set certain vars to certain values to cause different behavior.   \n",
    "        # CHECKPOINT #2: In proof, at this point, formulas will be assigned to neccessary dependent variables for datafrmae that will be output. \n",
    "        # At this point, the testSet and trainingSet dataframes will be shipped off. \n",
    "        print(dataFramesToBeShipped[0].columns)\n",
    "        print(\"--End of Component a[Subsystem 1]--\")\n",
    "        # resultantSeries = dataFramesToBeShipped[0].loc[0, :]\n",
    "        resultantSeries = dataFramesToBeShipped[0]\n",
    "        # resultantSeries = <-- will reference a row-tuple involving \"P/B\", \"P/E\", \"NCAV\", \"Date For Eval\", \"company\" \n",
    "        print(\"---Debugging checkpoint seeing if P/E key assignment workds for condition #2---\")\n",
    "        # pb.set_trace()\n",
    "        # NOTE: To solve issue, need to cast dates to only referenced years instead!\n",
    "        # Body of casting dates to cause assignment to be successful \n",
    "        conditionForDataFrame2.index = conditionForDataFrame2.index.year\n",
    "        balanceSheet.index = balanceSheet.index.year\n",
    "        # End of Body of casting dates to cause assignment to be successful \n",
    "        # NOTE: To solve issue, need to cast dates to only referenced years instead!\n",
    "        conditionForDataFrame2[\"P/E\"] = conditionForDataFrame2[independentVars[0]]/balanceSheet.loc[:, \"Retained Earnings\"]\n",
    "        # Below returns values that are not null, allowing conditional to work properly. FUTURE NOTE: Once rest of pipeline is verified, then may need to change what the PRICE should be. I think it should be share price opposed to high, but we shall see....\n",
    "        conditionForDataFrame2 = conditionForDataFrame2[conditionForDataFrame2[\"P/E\"].notnull()]\n",
    "        # Proposed solution: conditionForDataFrame2[conditionForDataFrame2[\"P/E\"].notnull()] test out in next session![UPDATE: This works properly!]\n",
    "        \n",
    "        \n",
    "        newVersion = True\n",
    "        return (dataFramesToBeShipped if seriesVersionOn == False else resultantSeries) if newVersion == False else (dataFramesToBeShipped if seriesVersionOn == False else [resultantSeries, conditionForDataFrame2])\n",
    "        # NOTE: compA() is logically correct! Refer to reports for additional steps regarding this!\n",
    "        # end of component a)\n",
    "        # component b) [Populating the new dataframe(s) and preparing dataFrames to be written to files[PN: Will mean that resultant dataframes must be copied to nonlocal variables so subsystem1 can access them(UPDATE: Decided to return the array of these dataframes instead)]\n",
    "        # end of component b)\n",
    "\n",
    "    # NOTE: Below was replaced by adding system to class's constructor!\n",
    "    #compB() <-- THis component was abandoned!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20667709",
   "metadata": {},
   "outputs": [],
   "source": [
    "class subsys2:\n",
    "    def __init__(self,param1 = \"\"\"Insert any params that may be sufficient here!\"\"\"):\n",
    "        print(\"---Subsystem 2 in Progress---\")\n",
    "        # self.compA()\n",
    "    # component a)[creating algorithm for assigning optimality for each company's value]\n",
    "    def compA(self, param1 = \"\"\"Insert any params that may be sufficient here!\"\"\", arrOfCompanies = []): #<-- Not sure if I Need a parameter for this one since I am relying on global variables. AND since its dependent on subsys1. \n",
    "        iterators = len(arrOfCompanies) +1 if len(arrOfCompanies) < 1 else len(arrOfCompanies) #2 #<-- Here, just in case for loop is used to iterate through each company. \n",
    "        # NOTE: Need to add a condition that ensures that dataframe is updated based on companies passed in. \n",
    "        # UPDATE: replacing compA with version where only each company is required. \n",
    "        global dataFrameReffingCompanyData\n",
    "        version0 = False\n",
    "        if version0 == True:\n",
    "            for i in range(iterators):\n",
    "                print(\"--Component a[Subsystem 2] in progress--\")\n",
    "                # Body of executing algorithm for strategy responsible for assigning optimality[NOTE: This only applies to retrievedDataFrame[0] since that'll be the training set one]\n",
    "                # Insert psuedosteps here: .\n",
    "                # predicate wffs for conditions: i) stock(P/B) <= 1.0, ii) stock(P/E) < max(stockPE(P/E,5)), iii) stock(share price) < 0.67*tangible per-share book value[which can be found in historical_data], iv)  (cont here). where stockPE(x,y) = stock's x ratio in the past 5 years and returns z_i, where z_i = x ratio in year i and i \\le y .  [predicate wffs written!]\n",
    "                # NOTE: Share Price == Share Issued and Tangible per-share book value == Tangible Book Value which are both located in the historical_data dataframe. \n",
    "                # Body of creating boolExp Array for respective conditions\n",
    "                arrOfCompanies[i] = param1 #<-- CROSSROADS #1: Here, in the event that I make the iteration take place in the class opposed to main function. \n",
    "                # currCompany = \"MSFT\" or arrOfCompanies[i] #<-- Used default operator here again, thought about replacing None with a param referencing company i.[UPDATE: replaced noe wtih arrOfCompanies[i]] \n",
    "                currCompany = arrOfCompanies[i] \n",
    "                dataFrameWithCompanyAsAColumnId = subsys1.compA(company=currCompany)[0] #<-- Returns the dataframe referencing company data. [IDEA: THink I need to replace dateInQuestion with companyName. The date should still be included but it shouldn't determine the P/B value. It is only relevant when pulling data]\n",
    "                # pb.set_trace()\n",
    "                dataFrameReffingCompanyData = pd.DataFrame() or dataFrameWithCompanyAsAColumnId #<-- REPLACEMENT PENDING: This will be changed to dataframe containing the relevant data. [replacement complete][old version: \"\"\"retrievedDataFrames[0]\"\"\" ][UPDATE: Here, dataFrmaeWithCompanyAsAColumnId refers to the resultant dataframe after assinging rest of columns EXCEPT the optimality]\n",
    "                dataFrameReffingCompanyData = pd.DataFrame() or dataFrameWithCompanyAsAColumnId #<-- REPLACEMENT PENDING: This will be changed to dataframe containing the relevant data. [replacement complete][old version: \"\"\"retrievedDataFrames[0]\"\"\" ][UPDATE: Here, dataFrmaeWithCompanyAsAColumnId refers to the resultant dataframe after assinging rest of columns EXCEPT the optimality]\n",
    "                dateInQuestion: datetime = end_date.fromisoformat(end_date) or \"\" #<-- Using this since vetting process is date-specific. [UPDATE: May not need this variable, but I believe the date should be included as a column with one value?]\n",
    "                yr = 365;\n",
    "                yearsRelToDateInQ: datetime = dateInQuestion - timedelta(days=5*yr)\n",
    "                    # Obtaining stocks highest P/E over previous five years[for assistance, use this search query: `? does ticker object have a start date parameter`]\n",
    "                    # end of Obtaining Stocks highest P/E over previous five years\n",
    "                # arrForCondTwo: pd.DataFrame = arrOfDataFramesNeeded[len(arrOfDataFramesNeeded) - 1] #<-- Did this since I added the dataframe specifically for cond 2 at the END of arrOfDataFramesNeeded. \n",
    "                arrForCondTwo: pd.DataFrame = arrOfDataFramesNeeded[len(arrOfDataFramesNeeded) - 1] #<-- Did this since I added the dataframe specifically for cond 2 at the END of arrOfDataFramesNeeded. [UPDATE: Need to replace this]\n",
    "            #    return #<-- Testing version of return statement[anything above this return statement is sucessful and everything below hasn't been tested yet. THis is relative to each function][for subsys2(compA())][\n",
    "                highestP_EOvrFiveYrs: float = 5 or arrForCondTwo[\"P/E\"].max() #<-- REPLACEMENT PENDING: will be replaced when above is filled in. [UPDATE: Using default operator logic to set val to trusy val if ither operand is undef. Will involve querying dataframe for maximum val in column]\n",
    "                # POTENTIAL IMPROVEMENT PENDING: Need to work on modifying bool exps below by ensuring that they reference the dep var attribute for that PARTICULAR day in which decision must be made. \n",
    "                #highestP_EOvrFiveYrs = \n",
    "                # UPDATE: THe inclusion of ternary operations below allows me to create env for determining optimality based on the day these decisions are being made. \n",
    "                # NOTE: Below will reference command for replacing dateInQuestion with currCompany: \n",
    "                # . `157m a; `.,'as/dateInQ\\w\\+/currCompany/g`  by running this on line of boolExps. [Replaced, in the case where multiply companies are iterated. \n",
    "                boolExps: list[bool] = [\n",
    "                        dataFrameReffingCompanyData[\"P/B\" if currCompany == None else (\"P/B\",currCompany)] <= 1.0,\n",
    "                    dataFrameReffingCompanyData[\"P/E\" if currCompany == None else (\"P/E\",currCompany)] <= 0.4*highestP_EOvrFiveYrs,\n",
    "                    dataFrameReffingCompanyData[\"Share Price\" if currCompany == None else (\"Share Price\",currCompany)] < 0.67*dataFrameReffingCompanyData[\"Tangible Book Value\" if currCompany == None else (\"Tangible Book Value\",currCompany)]\n",
    "                ]\n",
    "                    # End of Body of creating boolExp Array for respective conditions\n",
    "                score = 0\n",
    "                if boolExps[0]:\n",
    "                    score += 1\n",
    "                if boolExps[1]:\n",
    "                    score += 1\n",
    "                if boolExps[2]:\n",
    "                    score += 1\n",
    "                #scoreTable[f\"{currCompany}\"] = score; #<-- Will be used later to assign optimality [UPDATE: May not be needed, need to make sure it is added to optimality col for comapny name[also, will make sense to have companies be the index!]\n",
    "                # dataFrameReffingCompanyData[i, \"company\"] = f\"{currCompany}\"; # NOTE: This is under assumption that this refers to row tuple i and assigns company to that tuple. DEBUGGING OPPORTUNITY #1: May need to set a breakpoint/pdb.set_trace command below this! [UPDATE: This is not needed anymore since subsys1(compA) covers this already!]\n",
    "                dataFrameReffingCompanyData[\"Optimality\"] = score; #<-- CROSSROADS OPPORTUNITY: a) This process works on assumption that each company is iteratively having the optimality algorithm applied to it. \n",
    "                # End of Body of executing algorithm for strategy responsible for assigning optimality\n",
    "               # NOTE: This component will have to take in a company as a parameter. \n",
    "        else:\n",
    "            # Version of code where only company[i] is proprioritezed and a series is returned with optimality there. \n",
    "            print(\"--Component a[Subsystem 2] in progress--[VERSION #2]\")\n",
    "            # Body of executing algorithm for strategy responsible for assigning optimality[NOTE: This only applies to retrievedDataFrame[0] since that'll be the training set one]\n",
    "            # Insert psuedosteps here: .\n",
    "            # predicate wffs for conditions: i) stock(P/B) <= 1.0, ii) stock(P/E) < max(stockPE(P/E,5)), iii) stock(share price) < 0.67*tangible per-share book value[which can be found in historical_data], iv)  (cont here). where stockPE(x,y) = stock's x ratio in the past 5 years and returns z_i, where z_i = x ratio in year i and i \\le y .  [predicate wffs written!]\n",
    "            # NOTE: Share Price == Share Issued and Tangible per-share book value == Tangible Book Value which are both located in the historical_data dataframe. \n",
    "            # Body of creating boolExp Array for respective conditions\n",
    "            # arrOfCompanies[i] = param1 #<-- CROSSROADS #1: Here, in the event that I make the iteration take place in the class opposed to main function. [UPDATE: Decided to have iteration take place in the main function opposed to this function]\n",
    "            # currCompany = \"MSFT\" or arrOfCompanies[i] #<-- Used default operator here again, thought about replacing None with a param referencing company i.[UPDATE: replaced noe wtih arrOfCompanies[i]] \n",
    "            currCompany = param1\n",
    "            # dataFrameWithCompanyAsAColumnId = subsys1.compA(company=currCompany)[0] #<-- Returns the dataframe referencing company data. [IDEA: THink I need to replace dateInQuestion with companyName. The date should still be included but it shouldn't determine the P/B value. It is only relevant when pulling data] <-- UPDATE: THis may not be needed. \n",
    "            # dataFrameReffingCompanyData = pd.Series() or dataFrameWithCompanyAsAColumnId #<-- REPLACEMENT PENDING: This will be changed to dataframe containing the relevant data. [replacement complete][old version: \"\"\"retrievedDataFrames[0]\"\"\" ][UPDATE: Here, dataFrmaeWithCompanyAsAColumnId refers to the resultant dataframe after assinging rest of columns EXCEPT the optimality]\n",
    "            # dataFrameWithCompanyAsAColumnId = subsys1().compA(company=currCompany) #<-- Returns the dataframe referencing company data. [IDEA: THink I need to replace dateInQuestion with companyName. The date should still be included but it shouldn't determine the P/B value. It is only relevant when pulling data] [UPDATE: Trying to have compA(subsys1) return seriess AND the dataframe used for conditions!]\n",
    "            \"\"\"\n",
    "            dataFrameWithCompanyAsAColumnId = subsys1().compA(company=currCompany) #<-- Returns the dataframe referencing company data. [IDEA: THink I need to replace dateInQuestion with companyName. The date should still be included but it shouldn't determine the P/B value. It is only relevant when pulling data]\n",
    "            dataFrameReffingCompanyData = dataFrameWithCompanyAsAColumnId\n",
    "            # Body of version where compA(subsys1) returns TWO things[uncomment below once version is established!] \n",
    "            \"\"\"\n",
    "            # pb.set_trace()\n",
    "            arrOfDataFramesNeeded = subsys1().compA(company=currCompany) \n",
    "            dataFrameWithCompanyAsAColumnId = arrOfDataFramesNeeded[0] #<-- Returns the dataframe referencing company data. [IDEA: THink I need to replace dateInQuestion with companyName. The date should still be included but it shouldn't determine the P/B value. It is only relevant when pulling data]\n",
    "            dataFrameReffingCompanyData = arrOfDataFramesNeeded[1]\n",
    "            arrForCondTwo = arrOfDataFramesNeeded[1]            \n",
    "            \n",
    "            \n",
    "            # Body of version where compA(subsys1) returns TWO things\n",
    "            # pb.set_trace()\n",
    "            dateInQuestion: datetime = datetime.fromisoformat(end_date) or \"\" #<-- Using this since vetting process is date-specific. [UPDATE: May not need this variable, but I believe the date should be included as a column with one value?]\n",
    "            yr = 365;\n",
    "            yearsRelToDateInQ: datetime = dateInQuestion - timedelta(days=5*yr)\n",
    "                # Obtaining stocks highest P/E over previous five years[for assistance, use this search query: `? does ticker object have a start date parameter`]\n",
    "                # end of Obtaining Stocks highest P/E over previous five years\n",
    "            # arrForCondTwo: pd.DataFrame = arrOfDataFramesNeeded[len(arrOfDataFramesNeeded) - 1] #<-- Did this since I added the dataframe specifically for cond 2 at the END of arrOfDataFramesNeeded. \n",
    "            # ticker = yf.Ticker(f\"{currCompany}\") #<-- NOTE: Will need to replace this with multiple tickers later on.            \n",
    "            # arrForCondTwo: pd.DataFrame = ticker.history(period=\"5y\").tz_localize(None)\n",
    "            # arrForCondTwo: pd.DataFrame = [<-- may or may not need to use this again. Mentioned it a few lines above] \n",
    "            #    return #<-- Testing version of return statement[anything above this return statement is sucessful and everything below hasn't been tested yet. THis is relative to each function][for subsys2(compA())][\n",
    "            # highestP_EOvrFiveYrs: float = 5 or arrForCondTwo[\"P/E\"].max() #<-- REPLACEMENT PENDING: will be replaced when above is filled in. [UPDATE: Using default operator logic to set val to trusy val if ither operand is undef. Will involve querying dataframe for maximum val in column]\n",
    "            # print(\"---Debugging Checkpoint: Checking if P/E is a key in arrForCOndTwo\")\n",
    "            # pb.set_trace() <-- NOTE: P/E solution works properly!\n",
    "            highestP_EOvrFiveYrs = arrForCondTwo[\"P/E\"].max() #<-- REPLACEMENT PENDING: will be replaced when above is filled in. [UPDATE: Using default operator logic to set val to trusy val if ither operand is undef. Will involve querying dataframe for maximum val in column]\n",
    "            # POTENTIAL IMPROVEMENT PENDING: Need to work on modifying bool exps below by ensuring that they reference the dep var attribute for that PARTICULAR day in which decision must be made. \n",
    "            #highestP_EOvrFiveYrs = \n",
    "            # UPDATE: THe inclusion of ternary operations below allows me to create env for determining optimality based on the day these decisions are being made. \n",
    "            # NOTE: Below will reference command for replacing dateInQuestion with currCompany: \n",
    "            # . `157m a; `.,'as/dateInQ\\w\\+/currCompany/g`  by running this on line of boolExps. [Replaced, in the case where multiply companies are iterated. \n",
    "            \"\"\" boolExps: list[bool] = [\n",
    "                    dataFrameReffingCompanyData[\"P/B\" if currCompany == None else (\"P/B\",currCompany)] <= 1.0,\n",
    "                dataFrameReffingCompanyData[\"P/E\" if currCompany == None else (\"P/E\",currCompany)] <= 0.4*highestP_EOvrFiveYrs,\n",
    "                dataFrameReffingCompanyData[\"Share Price\" if currCompany == None else (\"Share Price\",currCompany)] < 0.67*dataFrameReffingCompanyData[\"Tangible Book Value\" if currCompany == None else (\"Tangible Book Value\",currCompany)]\n",
    "                ]\"\"\"\n",
    "                # NOTE: No errors occur until up to THIS point. Following error is prod by the boolExps: pandas.errors.IndexingError: Too many indexers\n",
    "            \"\"\"             boolExps: list[bool] = [\n",
    "                    dataFrameReffingCompanyData.loc[\"P/B\" if currCompany == None else (\"P/B\",currCompany)] <= 1.0,\n",
    "                dataFrameReffingCompanyData.loc[\"P/E\" if currCompany == None else (\"P/E\",currCompany)] <= 0.4*highestP_EOvrFiveYrs,\n",
    "                dataFrameReffingCompanyData.loc[\"Share Price\" if currCompany == None else (\"Share Price\",currCompany)] < 0.67*dataFrameReffingCompanyData[\"Tangible Book Value\" if currCompany == None else (\"Tangible Book Value\",currCompany)]\n",
    "            ]\n",
    "            \"\"\" \n",
    "            print(\"---DEBUGGING SUBCHECKPOINT: CHecking for the Boolean Expressions---\")\n",
    "            # pb.set_trace()\n",
    "            # NOTE: dataFrameReffingCompanyData is a pd.Series so the querying used Above is NOT needed!\n",
    "            # dataFrameThatRefsSharePrice = pd.DataFrame() #<-- This variable is self explanatory. Need to modify soon. \n",
    "            dataFrameThatRefsSharePrice = dataFrameWithCompanyAsAColumnId #<-- This variable is self explanatory. Need to modify soon. \n",
    "            \"\"\" boolExps: list[bool] = [\n",
    "                    dataFrameReffingCompanyData[\"P/B\"] <= 1.0,\n",
    "                dataFrameReffingCompanyData[\"P/E\"] <= 0.4*highestP_EOvrFiveYrs,\n",
    "                dataFrameReffingCompanyData[\"Share Price\"] < 0.67*dataFrameThatRefsSharePrice[\"Tangible Book Value\"]\n",
    "            ] \"\"\"\n",
    "            \"\"\" boolExps: list[bool] = [\n",
    "                    dataFrameWithCompanyAsAColumnId[\"P/B\"] <= 1.0,\n",
    "                dataFrameWithCompanyAsAColumnId[\"P/E\"] <= 0.4*highestP_EOvrFiveYrs,\n",
    "                dataFrameWithCompanyAsAColumnId[\"Share Price\"] < 0.67*dataFrameThatRefsSharePrice[\"Tangible Book Value\"]\n",
    "            ] \"\"\"\n",
    "            boolExps: list[bool] = [\n",
    "                    dataFrameWithCompanyAsAColumnId[\"P/B\"] <= 1.0,\n",
    "                dataFrameWithCompanyAsAColumnId[\"P/E\"] <= 0.4*highestP_EOvrFiveYrs,\n",
    "                dataFrameWithCompanyAsAColumnId[\"Share Price\"] < 0.67*(dataFrameWithCompanyAsAColumnId[\"Share Price\"]/dataFrameThatRefsSharePrice[\"P/B\"])\n",
    "            ]\n",
    "            # UPDATE: Problem originates from dataFrameThatRefsSharePrice[\"Tangible Book Value\"][UPDATE: Problem fixed!]\n",
    "                # End of Body of creating boolExp Array for respective conditions\n",
    "            \"\"\"             \n",
    "            if boolExps[0]:\n",
    "                score += 1\n",
    "            if boolExps[1]:\n",
    "                score += 1\n",
    "            if boolExps[2]:\n",
    "                score += 1\n",
    "            \"\"\"            \n",
    "            # `N`OTE: May need to change some things in future...NOT sure[particularly, involving ][NOTE: Will need to work on validating boolean exps since all optimality cols are equal to 0]\n",
    "            score = 0\n",
    "            if boolExps[0].any():\n",
    "                score += 1\n",
    "            if boolExps[1].any():\n",
    "                score += 1\n",
    "            if boolExps[2].any():\n",
    "                score += 1\n",
    "            #scoreTable[f\"{currCompany}\"] = score; #<-- Will be used later to assign optimality [UPDATE: May not be needed, need to make sure it is added to optimality col for comapny name[also, will make sense to have companies be the index!]\n",
    "            # dataFrameReffingCompanyData[i, \"company\"] = f\"{currCompany}\"; # NOTE: This is under assumption that this refers to row tuple i and assigns company to that tuple. DEBUGGING OPPORTUNITY #1: May need to set a breakpoint/pdb.set_trace command below this![UPDATE as of 11/21/25: May NOT need this at all!]\n",
    "            # print(\"---Testing if the optimality actually works[at least the assignments]---\")\n",
    "            # pb.set_trace()\n",
    "            # dataFrameReffingCompanyData[f\"{currCompany}\", \"Optimality\"] = score; #<-- CROSSROADS OPPORTUNITY: a) This process works on assumption that each company is iteratively having the optimality algorithm applied to it. [UPDATE: Not needed anymore since series are being returned opposed to entire dataframe] \n",
    "            # Below's verison references the pd.Series() so .loc isn't needed here. \n",
    "            dataFrameWithCompanyAsAColumnId[\"Optimality\"] = score; # UPDATE: Works as intended. Focu sshould shift back towards ensuring that Share Price thing works properly. \n",
    "            # pb.set_trace()\n",
    "            # End of Body of executing algorithm for strategy responsible for assigning optimality\n",
    "            # NOTE[EXTREMELY IMPORTANT]: Succeeding this point, another component will be responsible for combining the resultant dataframes, ordering the companies by optimality, and then assigning ranks to the companies, and then finishing off my adding the rank numbers to the resultant dataframe. \n",
    "            # NOTE: This component will have to take in a company as a parameter. \n",
    "\n",
    "            print(\"--End of Component a[Subsystem 2] in progress--\")\n",
    "            return dataFrameWithCompanyAsAColumnId\n",
    "# end of component a)\n",
    "    # component b)\n",
    "    def compB(): # NOTE: I believe compB(subsystem2) is complete! \n",
    "        print(\"--Component b[Subsystem 2] in progress--\")\n",
    "        global retreivedDataFrames \n",
    "        #retreivedDataFrames = dataFramesReffingCompanyData or pd.DataFrame()#compA() #<-- Will be replaced soon[REPLACEMENT PENDING][UPDATE: Replaced with global var referencign resultant dataframe(s) with optimality columns]. \n",
    "        retreivedDataFrames = [dataFrameReffingCompanyData,dataFrameReffingCompanyData] or pd.DataFrame()#compA() #<-- Will be replaced soon[REPLACEMENT PENDING][UPDATE: Replaced with global var referencign resultant dataframe(s) with optimality columns]. [UPDATE: Used an array of two insts of Company Data, since one of them will be used for trainingSet and testSet respectively(assuming it isn't done in Model Dev file)]\n",
    "        # Body of writing resp dataframes to files[need to use to_csv I believe]\n",
    "        filePathToTrainingSetDir: str = \"MLLifecycle/ModelDevelopment/TrainingSets\" #<-- fill in later[complete]\n",
    "        filePathToTestSetDir: str = \"MLLifecycle/ModelDevelopment/TestingSets\" #<-- fill in later[complete]\n",
    "        print(\"--End of Component b[Subsystem 2]--\")\n",
    "        print(\"Entering Debuging Mode for Checkpoint #3\")\n",
    "        # pb.set_trace() #<-- Will use this to check for vals of variable(s), as well as set certain vars to certain values to cause different behavior.   \n",
    "        # CHECKPOINT #3: In proof, at this point, dataframes will be written to the neccessary files to be ingested by the Model.  \n",
    "        print(\"--Writing Dataframes for Model Ingestion--\")\n",
    "        retreivedDataFrames[0].to_csv(f'{filePathToTrainingSetDir}/trainingSet{setNum if setNum != None else 1}')\n",
    "        retreivedDataFrames[1].to_csv(f'{filePathToTrainingSetDir}/testSet{setNum if setNum != None else 1}')\n",
    "        # End of Body of writing resp dataframes to files\n",
    "        print(\"--End of Writing Dataframes for Model Ingestion--\")\n",
    "        print(\"--End of Component b[Subsystem 2]--\")\n",
    "        return #<-- Testing version of return statement[anything above this return statement is sucessful and everything below hasn't been tested yet. THis is relative to each function]\n",
    "    # end of component b)\n",
    "\n",
    "    # NOTE: Below was replaced by adding system to class's constructor!\n",
    "    #return\n",
    "    #compA() \n",
    "    #compB()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5b5e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"---Starting Data Prep Process---\")\n",
    "\n",
    "    companies: list[str] = [\"GOOG\",\"AAPL\", \"AMZN\", \"MSFT\"]\n",
    "    \n",
    "    listOfSeriesToCreateDataFrame = []\n",
    "    for i in range(len(companies)):\n",
    "        print(f\"----Adding company {companies[i]} to engineered dataset----\")\n",
    "        # \"\"\"\n",
    "        # NOTE: Will uncomment, once everything with the functions used here is situated[add a checklist here: ]\n",
    "        print(\"---Starting Subsystem 1---\")\n",
    "        # pb.set_trace(); #<-- Adding breakpoint here to see what happens. \n",
    "        # Call function referencing topmost subsystem #1 here: \n",
    "        a: str = \"\"; b: str = \"\"\n",
    "        # subsys1(param1=company_i)\n",
    "        # subsys1().compA(company=company_i)\n",
    "        # resultantDataFrame = pd.concat([resultantDataFrame, subsys1().compA(company=companies[i])])\n",
    "        # resultantDataFrame.loc[i, [\"P/B\", \"P/E\", \"NCAV\", \"Company\"]] = subsys1().compA(company=companies[i])\n",
    "        subsys1().compA(company=companies[i])\n",
    "        # companySeries= subsys1().compA(company=companies[i])\n",
    "        # pb.set_trace()\n",
    "        # subsys2().compA(param1=companySeries)\n",
    "        subsys2().compA(param1=companies[i])\n",
    "        retSeries = True\n",
    "        if retSeries:\n",
    "            # ^^ NOTE: Above is returning a series each time[at least this is the assumption]\n",
    "            companySeries = subsys2().compA(param1=companies[i])\n",
    "            listOfSeriesToCreateDataFrame.append(companySeries)\n",
    "        else:\n",
    "            # ^^ NOTE: Above is NOT retruning a series\n",
    "            print(\"---Assumption that series is NOT returned---\")\n",
    "            # NOTE: Not sure what to put here. Will default to returning series from compA(subsys2)\n",
    "            # companySeries = subsys2().compA(param1=companies[i])\n",
    "            # listOfSeriesToCreateDataFrame.append(companySeries)\n",
    "            \n",
    "        # resultantDataFrame.loc[i, :] = subsys1().compA(company=companies[i]) #<-- This line is still causing problems. \n",
    "        # Will replace above with this: subsys1(company_i)\n",
    "        \n",
    "        print(\"---End of Subsystem 1---\")\n",
    "        \n",
    "        \"\"\"\n",
    "        print(\"---Starting Subsystem 2---\")\n",
    "        # Call function referencing topmost subsystem #2 here: \n",
    "        # subsys2(b)\n",
    "        # subsys2.compA(b) #<-- Here, need to ensure that \n",
    "        # Will replace above with this: subsys2(company_i)\n",
    "        print(\"---End of Subsystem 2---\")\n",
    "        \"\"\"\n",
    "        print(f\"----End of Adding company {companies[i]} to engineered dataset----\")\n",
    "\n",
    "    pb.set_trace()\n",
    "    # resultantDataFrame = pd.DataFrame(listOfSeriesToCreateDataFrame) #<-- Causing following error: *** ValueError: Must pass 2-d input. shape=(1, 1, 6)[need to figure out how to resolve error][UPDATE: Error fixed! Imp below resolved issue. NOW: Main focus goes back to setting up optimality!]\n",
    "    resultantDataFrame = pd.concat([pd.DataFrame(x) for x in listOfSeriesToCreateDataFrame]).reset_index() #<-- used list comprehension to transform listOfSeries to resultantDataFrame. \n",
    "    del resultantDataFrame['index']\n",
    "    print(resultantDataFrame) #<-- THis dataframe will reference the dataframe that adheres to the follwowing object: company(CompanyName, \"P/B\", \"P/E\", \"NCAV\", \"Date For Eval\", \"Optimality\", (cont here if applicable))[NOTE: Will be wise to make a Entity via ERDs for documentation when writing paper at end]\n",
    "    inNoteBook = False\n",
    "    filePathToModelDir = \"C:/Users/adoct/Notes for CSCE Classes[Fall 2025]/Notes for CSCE 585/ProjectRepo/projectCode/MLLifecycle/ModelDevelopment/preparedDataset.csv\" if inNoteBook == False else \"preparedDataset.csv\"\n",
    "    # Body of handling edge case where all of them are same optimality\n",
    "\n",
    "    resultantDataFrame.to_csv(f\"{filePathToModelDir}\")\n",
    "    if((resultantDataFrame[\"Optimality\"] == 0).all() == True):\n",
    "        # Setting optimality column to be based on alphabetical ordering \n",
    "        print(\"---DEBUGGING CHECKPOINT #3: Validating process of creating resultant dataframe and sorting columns and assigning optimality, iff all stocks chosen all have same optimality---\")\n",
    "        pb.set_trace()\n",
    "        # UPDATE: Below works as expected!\n",
    "        resultantDataFrame.sort_values(by='Company',inplace=True)\n",
    "        resultantDataFrame = resultantDataFrame.set_index(np.arange(4))\n",
    "        resultantDataFrame.loc[:,\"Optimality\"] = pd.Series(np.arange(resultantDataFrame[\"Optimality\"].shape[0]))\n",
    "\n",
    "        # End of Setting optimality column to be based on alphabetical ordering \n",
    "        resultantDataFrame.to_csv(f\"{filePathToModelDir}\")\n",
    "    \n",
    "     \n",
    "    \n",
    "    \n",
    "    # End of Body of handling edge case where all of them are same optimality\n",
    "\n",
    "main()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# End of portion referencing data engineering procedure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd7da25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: As of 11/30/25, MODEL DEV AND TRAINING IS COMPELTE! [Need to partition below as well. Ensure that respective plots come out one at a time AT LEAST!] \n",
    "# NOTE: In regards to testing out program, use following command. Refer to its comment for reference: $ find ProjectCode/components ProjectCode/routes -name \"*.py\" | xargs --delimiter=\"\\n\" grep -E \"^(import|from)\" #<-- NOTE: Use this code to find the files to: a) determine neccessary downloads for pip, and b) determine where to call model and send output etc from. \n",
    "\n",
    "# Purpose: This file will contain code that helps with Model Development. \n",
    "\n",
    "# Body of neccessary imports\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np #<-- May be optional not sure as of 10/13/25. \n",
    "import pdb as pb\n",
    "# import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Input\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "\n",
    "# end of body of neccessary imports\n",
    "\n",
    "# Important Steps in Model Development: 1) Obtaining the training data, 2) Create the model containing initialized weights and a bais[which would be very involved with using numbers from certain columns], 3) Observe model's performance before training, 4) Defining a loss function for model, 5) Write a basic Training Loop\n",
    "def attempt1():\n",
    "    # Section 1\n",
    "    # Purpose of section: 1) Obtaining the training data\n",
    "\n",
    "    # end of purpose of section\n",
    "\n",
    "    # Psuedosteps for exercising purpose of section\n",
    "    # - Need to make sure data is clean\n",
    "    # - Need to have an object that references clean dataset\n",
    "    # - Training data \n",
    "    # end of psuedosteps for exercising purpose of section \n",
    "\n",
    "    # body of section 1\n",
    "\n",
    "\n",
    "\n",
    "    # end of body of section 1\n",
    "    # End of Section 1\n",
    "\n",
    "\n",
    "    # Section 2\n",
    "    # Purpose of section: 2) Create the model containing initialized weights and bias\n",
    "\n",
    "    # end of purpose of section\n",
    "\n",
    "    # Psuedosteps for exercising purpose of section\n",
    "\n",
    "    # end of psuedosteps for exercising purpose of section \n",
    "\n",
    "    # body of section 2\n",
    "    # NOTE: Code is subject to change, just acting as starting point right now.\n",
    "    class Model(tf.Module):\n",
    "        def __init__(self):\n",
    "            # \"Randomly genearte weight and bias terms\"\n",
    "            rand_init = tf.random.uniform(shape=[3], minval=0., maxval=5., seed = 22)\n",
    "            # \"Initialize Model Parameters\"\n",
    "            self.w_q = tf.Variable(rand_init[0])\n",
    "            self.w_l = tf.Variable(rand_init[1])\n",
    "            self.b = tf.Variable(rand_init[2])\n",
    "        @tf.function\n",
    "        def __call__(self,x):\n",
    "            # \"Quadratic Model: quadratic_weight * x^2 + linear_weight*x + bias\"\n",
    "            return self.w_q * (x**2) + self.w_l * x + self.b\n",
    "\n",
    "    # end of body of section 2\n",
    "\n",
    "    # End of Section 2\n",
    "\n",
    "\n",
    "    # Section 3\n",
    "    # Purpose of section:  3) Observe model's performance before training\n",
    "\n",
    "    # end of purpose of section\n",
    "\n",
    "    # Psuedosteps for exercising purpose of section\n",
    "\n",
    "    # end of psuedosteps for exercising purpose of section \n",
    "\n",
    "    # body of section 3\n",
    "\n",
    "    # NOTE: Code is subject to change, just acting as starting point right now.\n",
    "    quad_model = Model() #<-- instantiation of model\n",
    "    def plot_preds(x,y,f,model,title):\n",
    "       plt.figure() \n",
    "       plt.plot(x,y, '.', label='Data')\n",
    "       plt.plot(x,f(x), label='Ground Truth')\n",
    "       plt.plot(x,model(x), label='Predictions')\n",
    "       plt.title(title)\n",
    "       plt.legend()\n",
    "\n",
    "    plot_preds(x,y,f,quad_model,'Before Tranining')\n",
    "\n",
    "\n",
    "\n",
    "    # End of body of section 3\n",
    "    # End of Section 3\n",
    "\n",
    "\n",
    "    # Section 4\n",
    "    # Purpose of section: 4) Defining the loss function\n",
    "\n",
    "    # end of purpose of section\n",
    "\n",
    "    # Psuedosteps for exercising purpose of section\n",
    "\n",
    "    # end of psuedosteps for exercising purpose of section \n",
    "\n",
    "    # End of Section 4\n",
    "\n",
    "\n",
    "    # Section 5\n",
    "    # Purpose of section: 5) Write a basic Training Loop\n",
    "\n",
    "    # end of purpose of section\n",
    "\n",
    "    # Psuedosteps for exercising purpose of section\n",
    "\n",
    "    # end of psuedosteps for exercising purpose of section \n",
    "\n",
    "    # Body of Section 5\n",
    "    batch_size = 32\n",
    "    # NOTE: This may be a good reference for sending in clean training data for model to use. \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "    dataset = dataset.shuffle(buffer_size=x.shape[0].batch(batch_size))\n",
    "\n",
    "    # a) \"Set Training Parameters\"\n",
    "    epochs = 100\n",
    "    learning_rate = 1e-2\n",
    "    losses= []\n",
    "\n",
    "    # b) \"Format Training Loop\"\n",
    "    for epoch in range(epochs):\n",
    "        for x_batch, y_batch in dataset:\n",
    "            with tf.GradientTape() as tape:\n",
    "                batch_loss = mse_loss(quad_model(x_batch),y_batch) \n",
    "            # 1) \"Update parameters with respect to the gradient calculations\"\n",
    "            grads = tape.gradient(batch_loss,quad_model.variables)\n",
    "            for g,v in zip(grads, quad_model.variables):\n",
    "                v.assign_sub(learning_rate*g)\n",
    "        # 2) \"Keep track of model loss per epoch\"\n",
    "        loss = mse_loss(quad_model(x), y)\n",
    "        losses.append(loss)\n",
    "        if epoch %10 == 0:\n",
    "            print(f'Mean squared error for step {epoch}: {loss.numpy():0.3f}')\n",
    "\n",
    "    # c) \"Plot Model Results\"\n",
    "    print(\"\\n\")\n",
    "    plt.plot(range(epochs), losses)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Mean Squared Error (MSE)\")\n",
    "    plt.title(\"MSE loss vs Training Iterations\");\n",
    "\n",
    "    plt.show()\n",
    "    # Now, observe your model's performacne after training: \n",
    "    plot_preds(x,y,f,quad_model, 'After Training')\n",
    "    # End of Body of Section 5\n",
    "    # End of Section 5\n",
    "\n",
    "\n",
    "    ## NOTE: Below references an alternative approach. This ranges from getting data to providing statistcis for model eval. Does not include data visualization portion. \n",
    "\"\"\"\n",
    "# Attempt 2\n",
    "# Modificaitons needed: a) Need to find place where to put modified dataset, b) Need to ensure that a training data set is created from the dataset(s) that we have[need to make sure an additional row or column is provided for the classification], (cont here if applicable) \n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "\n",
    "# \"Helper Libraries\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "# 1) Import Dataset\n",
    "#fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "#(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "filepathToTrainingSet: str\n",
    "filepathToTestSet: str\n",
    "filePathToTrainingSet += f\"/testSetStrat{1}.py\" #<-- number will change based on strat being used.\n",
    "train_stocks = pd.read_csv(f\"{filepathToTrainingSet}); train_labels = test_labels = [\"will reference company names\"];\n",
    "test_stocks = pd.read_csv(f\"{filepathToTrainingSet}); test_labels = [\"will reference company names\"];\n",
    "\n",
    "# end of 1)\n",
    "# 2) Provide class names for ML Model to make predictions\n",
    "#class_names = ['T-shirt/top', 'Trourser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot']\n",
    " class_names = ['Apple', 'Google', 'Amazon']\n",
    "\n",
    "# end of 2)\n",
    "\n",
    "# 3) Exploring the data\n",
    "len(train_labels)\n",
    "\n",
    "test_images.shape\n",
    "\n",
    "\n",
    "len(test_labels)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(train_images[0])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i],cmap=plt.cm.binary)\n",
    "    plt.xlabel(class_names[train_labels[i]])\n",
    "\n",
    "#plt.show()\n",
    "# end of 3)\n",
    "# 4) Creating the Model[can be created using keras OR created manually by inherting the tf.Module object][aka the Neural Network][UPDATE: Think it'd make sense to create aa FNN first using things below]\n",
    "model = tf.keras.Sequential([\n",
    "tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "tf.keras.layers.Dense(128,activation='relu'),\n",
    "tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "metrics=['accuracy']) #<-- NOTE: Metrics here, can probably be modified[UPDATE: No need, accuracy is what is important right now]\n",
    "\n",
    "# end of 4)\n",
    "# 5) Evaluating the Model\n",
    "model.fit(train_images, train_labels, epochs=10)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels,verbose=2)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)\n",
    "\n",
    "\n",
    "probability_model = tf.keras.Sequential([model, \n",
    "tf.keras.layers.Softmax()])\n",
    "\n",
    "# a) Obtaining the accuarcy of the predictions\n",
    "predictions = probability_model.predict(test_images)\n",
    "\n",
    "predictions[0]\n",
    "\n",
    "\n",
    "np.argmax(predictions[0]) #<-- Returns the prediction that Neural Network was most comfortable with. \n",
    "\n",
    "# 6) Verifying and Visualzing the Predictions\n",
    "def plot_image(i, predictions_array, true_label, img):\n",
    "    true_label, img = true_label[i], img[i]\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(img, cmap=plt.cm.binary)\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "\n",
    "    if predicted_label == true_label:\n",
    "        color = 'blue'\n",
    "    else:\n",
    "        color = 'blue'\n",
    "    \n",
    "    plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label], 100*np.max(predicitons_array), class_names[true_label], color=color)\n",
    "\n",
    "def plot_image(i, predictions_array, true_label):\n",
    "    true_label = true_label[i]\n",
    "    plt.grid(False)\n",
    "    plt.xticks(range(10))\n",
    "    plt.yticks([])\n",
    "    thisplot = plt.bar(range(10), predictions_array, color=\"#7777777\")\n",
    "    plt.ylim([0,1])\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "\n",
    "    thisplot[predicted_label].set_color('red')\n",
    "    thisplot[true_label].set_color('blue')\n",
    "\n",
    "i = 0\n",
    "plt.figure(figsize(6,3))\n",
    "plt.subplot(1,2,1)\n",
    "plot_image(i,predictions[i],test_labels,test_images)\n",
    "plt.subplot(1,2,2)\n",
    "plot_image(i,predictions[i],test_labels)\n",
    "plt.show()\n",
    "\n",
    "# end of 6)\n",
    "\n",
    "\"\"\"\n",
    "def attempt3(): #<-- NOTE: This attempt is what will be used for Model Portion. \n",
    "    inNoteBook = False\n",
    "    \n",
    "    import numpy as np\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "    from tensorflow.keras.utils import to_categorical, plot_model\n",
    "    from tensorflow.keras.datasets import mnist\n",
    "    # NOTE: Make sure to NOT worry about imports. \n",
    "    # NOTE: Below is used to modify network parameters using experiment setup func(s). \n",
    "    global x_train; global x_test\n",
    "\n",
    "    # \"load mnist dataset\"\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    # filePathToTrainingSetDir: str = \"MLLifecycle/ModelDevelopment/TrainingSets\" #<-- fill in later[complete] UPDATE as of 11/10/25: Replaced since I am using script.py for running pipeline. \n",
    "    \"\"\"     \n",
    "filePathToTrainingSetDir: str = \"./ModelDevelopment/TrainingSets\" #<-- fill in later[complete]\n",
    "    filepathToTrainingSet: str = filePathToTrainingSetDir\n",
    "    filePathToTrainingSet += f\"/trainingSetStrat{1}.py\" #<-- number will change based on strat being used.\n",
    "    filePathToTestSetDir: str = \"MLLifecycle/ModelDevelopment/TestingSets\" #<-- fill in later[complete]\n",
    "    filepathToTestSet: str = filePathToTestSetDir\n",
    "    filePathToTestSet += f\"/testSetStrat{1}.py\" #<-- number will change based on strat being used.\n",
    "    \"\"\"    \n",
    "    filePathToModelDir = \"C:/Users/adoct/Notes for CSCE Classes[Fall 2025]/Notes for CSCE 585/ProjectRepo/projectCode/MLLifecycle/ModelDevelopment/preparedDataset.csv\" if inNoteBook == False else \"preparedDataset.csv\"\n",
    "    # print(\"---DEBUGGING CHECKPOINT #1: Ensuring that train_stocks & train_labels reference the right things---\")\n",
    "    # pb.set_trace() [COMPLETE]\n",
    "    # train_stocks = pd.read_csv(f\"{filePathToModelDir}\"); train_labels = test_labels = train_stocks.loc[\"Company\"] or [\"will reference company names\"];#<-- References labels which are derived from custom engineered dataset. \n",
    "    train_stocks = pd.read_csv(f\"{filePathToModelDir}\"); train_labels = test_labels = train_stocks.loc[:,\"Company\"] #<-- References labels which are derived from custom engineered dataset. \n",
    "    del train_stocks[\"Unnamed: 0\"]\n",
    "    test_stocks = pd.read_csv(f\"{filePathToModelDir}\"); test_labels = train_stocks.loc[:,\"Company\"] # or [\"will reference company names\"]; #<-- References labels which are derived from custom engineered dataset. \n",
    "    del test_stocks[\"Unnamed: 0\"]\n",
    "    x_train = train_stocks.loc[:,train_stocks.columns != \"Optimality\"]\n",
    "    y_train = train_labels\n",
    "    x_test = test_stocks.loc[:,train_stocks.columns != \"Optimality\"]\n",
    "    y_test = test_labels\n",
    "    # ^^ Above ensures that prediction labels are y and x references the data used to make said decision. [UPDATE: Due to this, will need to make a change in data prep folder]\n",
    "            \n",
    "\n",
    "    # \"Compute the number of labels\"\n",
    "    num_labels = len(np.unique(y_train))\n",
    "\n",
    "    # \"Convert to one-hot vector\"[we converted the labels to one-hot vectors using to_categorical]\n",
    "\n",
    "    # resultantDataFrame = pd.concat([pd.DataFrame(x) for x in listOfSeriesToCreateDataFrame]).reset_index() #<-- used list comprehension to transform listOfSeries to resultantDataFrame. \n",
    "    # print(\"---DEBUGGING CHECKPOINT #1: Ensuring that train_stocks & train_labels reference the right things---\")\n",
    "    # pb.set_trace() [COMPLETE!]\n",
    "    # y_train = to_categorical([range(y_train.shape[0]) for x in y_train])\n",
    "    # y_test = to_categorical([ range(y_test.shape[0]) for x in y_test])\n",
    "    y_train = to_categorical(y_train.index)\n",
    "    y_test = to_categorical(y_test.index)\n",
    "    \"\"\"\n",
    "    NOTE: Below is NOT needed\n",
    "    listOfHotEncodings = list(range(y_train.shape[0]))\n",
    "    # y_train = to_categorical([ dict[x] for x in y_train])\n",
    "    for i in range(y_train.shape[0]):\n",
    "        # listOfHotEncodings[i] = 2**(y_train.shape[0]) - 2**i #<-- Uses one hot encoding by using decimal value that refs binary value. 1 = 0001, 2 = 0010, 4 = 0100, 8 = 1000 and so forth. \n",
    "        listOfHotEncodings[i] = 2**i #<-- Uses one hot encoding by using decimal value that refs binary value. 1 = 0001, 2 = 0010, 4 = 0100, 8 = 1000 and so forth. \n",
    "\n",
    "    y_train = listOfHotEncodings\n",
    "    # y_test = to_categorical([ range(y_test.shape[0]) for x in y_test])\n",
    "    for i in range(y_test.shape[0]):\n",
    "        # listOfHotEncodings[i] = 2**(y_test.shape[0]) - 2**i #<-- Uses one hot encoding by using decimal value that refs binary value. 1 = 0001, 2 = 0010, 4 = 0100, 8 = 1000 and so forth. \n",
    "        listOfHotEncodings[i] = 2**i #<-- Uses one hot encoding by using decimal value that refs binary value. 1 = 0001, 2 = 0010, 4 = 0100, 8 = 1000 and so forth. \n",
    "    y_test = listOfHotEncodings\n",
    "    \"\"\"\n",
    "    \n",
    "    # NOTE: Currently here as far as debugging. [ALSO, whilst working make sure that you know that: NOTE: Need to find a dataset that references ranked optimality of stocks based on value investing strategy to use as a benchmark for model results. ]\n",
    "\n",
    "    # ----May snippet below may be optional?---\n",
    "    # \"image dimensions(assumed square)\"\n",
    "    # image_size = x_train.shape[1]\n",
    "    # input_size = image_size * image_size\n",
    "    input_size = len(train_stocks.columns) # #<-- UPDATE: input_size is NOT optional. Set this assuming that input_dim is number of dims/attributes for each feature which refers to number of columns.  However, I believe this is \n",
    "\n",
    "    # \"Resize and Normalize\"\n",
    "    \"\"\" \n",
    "    x_train = np.reshape(x_train, [-1,input_size])\n",
    "    x_train = x_train.astype('float32')/255\n",
    "    x_test = np.reshape(x_test, [-1,input_size])\n",
    "    x_test = x_test.astype('float32')/255\n",
    "    \"\"\"\n",
    "    # ----May snippet above may be optional?[UPDATE: Above is required, need to come up with normalization process]---\n",
    "    # \"Network Parameters\"\n",
    "    # NOTE: The following will reference a list of tuple(s) that will be used to facilitate first experiment. \n",
    "    listOfExperimentSetupFuncs = [\"\"\"Plan to insert functions for doing experiment(s) setup here\"\"\"]\n",
    "    global batch_size, hidden_units, dropout\n",
    "    global model #<-- Have this here, so experimental setup functions can tweak model as needed. \n",
    "    batch_size = 128\n",
    "    # batch_size = experiment1Tuples[0][0] \n",
    "    hidden_units = 256\n",
    "    # hidden_units = experiment1Tuples[0][1]\n",
    "    dropout = 0.45\n",
    "    # dropout  = experiment1Tuples[0][2]\n",
    "    isExperimentSetup3Active = False\n",
    "    # model = Sequential()\n",
    "    model = Sequential()\n",
    "    def experimentSetup0():\n",
    "       # This will reference the default settings irrespective to experiments. \n",
    "        print(\"---Undergoing Experiment Setup #0---\")\n",
    "        # model.add(Dense(hidden_units,input_dim=input_size))\n",
    "        # model.add(Dense(hidden_units,input_shape=(input_size-1,)))\n",
    "        # print(\"---DEBUGGING CHECKPOINT: Checking model's reactions---\")\n",
    "        # pb.set_trace()\n",
    "        # model.add(Input((input_size,)))\n",
    "        model.add(Input((x_train.shape[1],)))\n",
    "        # model.add(Dense(hidden_units,input_shape=(input_size,)))\n",
    "        model.add(Dense(hidden_units))\n",
    "        # NOTE: Above is causing following error: \"ValueError: Exception encountered when calling Sequential.call(). Invalid input shape for input Tensor(\"data:0\", shape=(5,), dtype=float32). Expected shape (None, 6), but input has incompatible shape (5,)\" [UPDATE: Error is originating from fact that x_trainCopy and x_testCopy 's shapes are (5,) and (5,)]\n",
    "        model.add(Activation('relu')) #<-- This is used to add activation function to model[UPDATE: May need to replace Activation('relu') by making them default...not sure to facilitate experimentSetup3 OR I can simply see what happens when the 2nd to LAST acivation is changed] \n",
    "        model.add(Dropout(dropout))\n",
    "        model.add(Dense(hidden_units))\n",
    "        model.add(Activation('relu')) if experimentSetup3() == None else experimentSetup3() #<-- This changes the Activation function, adhering to experimentSetup0![Thus, as of 11/23/25: There are two experiments ready to go for Milestone 1]\n",
    "        model.add(Dropout(dropout))\n",
    "        model.add(Dense(num_labels))\n",
    "        print(\"---End of Experiment Setup #0---\")\n",
    "        return\n",
    "\n",
    "    def experimentSetup1():\n",
    "        # Goal of exp: Want to see how model params affect the acuaracy of the model by modifying batch size and hidden units and dropout[pn: make sure to have a subset of the powerset of params be modified in this exp. ]\n",
    "        print(\"---Undergoing Experiment Setup #1---\")\n",
    "        setN = 0 #<-- Change number for this to get values from resp sets. \n",
    "        experiment1Tuples: list[tuple] = [(128,256,0.45), (64,128,0.45), (\"\"\"NOTE: Other tuples can change one or more parameters whilst keeping at least one constant\"\"\")]\n",
    "        batch_size = experiment1Tuples[setN][0] \n",
    "        hidden_units = experiment1Tuples[setN][1]\n",
    "        dropout = experiment1Tuples[setN][2]\n",
    "        \n",
    "        print(\"---End of Experiment Setup #1---\")\n",
    "        return\n",
    "    def experimentSetup2(numQuantLvls = 2):\n",
    "        # Goal of exp: Want to see model performance based on degree of quantanization of data\n",
    "        print(\"---Undergoing Experiment Setup #2---\")\n",
    "        # NOTE: Below will involve replacing 255 with a different number based on degree of quantanization of data. \n",
    "        # UPDATE: Only thing left is pulling maximum value from x_train and min value from train and test respectively. \n",
    "        widthsOfQuant = []\n",
    "        \n",
    "        widthsOfQuant[0] = (x_trainMax - x_trainMin)/(numQuantLvls - 1)\n",
    "        x_train = np.reshape(x_train, [-1,input_size])\n",
    "        x_trainMax = 0;\n",
    "        x_trainMin = 0;\n",
    "        \n",
    "        x_test = np.reshape(x_test, [-1,input_size])\n",
    "        x_testMax = 0;\n",
    "        x_testMin = 0;\n",
    "        widthsOfQuant[1] = (x_testMax - x_testMin)/(numQuantLvls - 1); \n",
    "        x_train = x_train.astype('float32')/widthsOfQuant[0]\n",
    "        x_test = x_test.astype('float32')//widthsOfQuant[1]\n",
    "        print(\"---End of Experiment Setup #2---\")\n",
    "        return\n",
    "\n",
    "    def experimentSetup3():\n",
    "        # Goal of exp: Want to see model performance based on type of activation function from a subset of all possible activation functions. \n",
    "        print(\"---Undergoing Experiment Setup #3---\" if isExperimentSetup3Active else \"---Experiment Setup #3 was skipped---\")\n",
    "        activationFuncs = ['elu', 'sigmoid', 'tanh' ]\n",
    "        # model.add(Activation(activationFuncs[0])) #<-- May need this since I have a classification problem that I'm wokring on. [UPDATE: Made this a return value instead] \n",
    "\n",
    "        print(\"---End of Experiment Setup #3---\" if isExperimentSetup3Active else \"---End of Experiment Setup #3 was skipped---\")\n",
    "\n",
    "        return model.add(Activation(activationFuncs[0])) if isExperimentSetup3Active == True else None #<-- May need this since I have a classification problem that I'm wokring on. \n",
    "\n",
    "    def experimentSetup4():\n",
    "        print(\"---Undergoing Experiment Setup #4---\")\n",
    "        print(\"---End of Experiment Setup #4---\")\n",
    "        return\n",
    "\n",
    "\n",
    "    # \"Model is a 3-layer ML with ReLU and dropout after each layer\": \n",
    "    #model = Sequential()\n",
    "    #model.add(Dense(hidden_units,input_dim=input_size))\n",
    "    #model.add(Activation('relu')) #<-- This is used to add activation function to model\n",
    "    #model.add(Dropout(dropout))\n",
    "    #model.add(Dense(hidden_units))\n",
    "    #model.add(Activation('relu'))\n",
    "    #model.add(Dropout(dropout))\n",
    "    #model.add(Dense(num_labels))\n",
    "    # \"This is the output for one-hot vector\"\n",
    "    #model.add(Activation('softmax')) #<-- May need this since I have a classification problem that I'm wokring on. \n",
    "    #model.summary()\n",
    "    #plot_model(model,to_file='mlp-mnist.png', show_shapes=True)\n",
    "\n",
    "    # \"Loss Function for one-hot vector\"\n",
    "    # \"Use of adam optimizer\"\n",
    "    # \"Accuracy is good metric for classification tasks\": \n",
    "    # print(\"---DEBUGGING CHECKPOINT #2: Making attempt to test before running Experiment 0---\")\n",
    "    # pb.set_trace()#[experimentSetup0 was successful]\n",
    "    \n",
    "    experimentSetup0() \n",
    "    experimentSetup1() \n",
    "    # \"This is the output for one-hot vector\"\n",
    "    model.add(Activation('softmax')) #<-- May need this since I have a classification problem that I'm wokring on. \n",
    "    model.summary()\n",
    "    #plot_model(model,to_file='mlp-mnist.png', show_shapes=True)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "    # \"Train the network\"\n",
    "    # BUG: There is a bug that occurs here that prods the following message: ValueError: Unrecognized data type: x=  [Apparently, there is an unrecognized data type in x_train variable]\n",
    "    x_train.loc[:,\"Company\"] = pd.Series([2**i for i in range(x_train.shape[0])]) \n",
    "    x_test.loc[:,\"Company\"] = pd.Series([2**i for i in range(x_train.shape[0])]) \n",
    "    x_train = x_train.astype(\"float64\")\n",
    "    x_test = x_test.astype(\"float64\")\n",
    "    x_train.loc[:,\"Company\"] = x_train.loc[:,\"Company\"].astype(\"int32\")\n",
    "    x_test.loc[:,\"Company\"] = x_test.loc[:,\"Company\"].astype(\"int32\")\n",
    "    # y_train = pd.Series(y_train)\n",
    "    # y_test = pd.Series(y_test)\n",
    "    # NOTE: Body of Problem is coming from x_train and x_test!\n",
    "    x_trainCopy = tf.data.Dataset.from_tensor_slices((x_train.values.astype(np.float32),tf.convert_to_tensor(y_train).numpy().astype(np.float32)))\n",
    "    x_testCopy = tf.data.Dataset.from_tensor_slices((x_test.values.astype(np.float32),tf.convert_to_tensor(y_test).numpy().astype(np.float32)))\n",
    "    # NOTE: Body of Problem is coming from x_train and x_test!\n",
    "    # x_train = x_trainCopy \n",
    "    # x_test = x_testCopy \n",
    "\n",
    "    print(\"---DEBUGGING CHECKPOINT #3: Making attempt to test before training network---\")\n",
    "    pb.set_trace()\n",
    "    # UPDATE: FINISHED MODEL!!\n",
    "    train_history = model.fit(x_train,y_train,epochs=20, batch_size=batch_size) #<-- Removing this since I converted dataframe into tensorflow version of dataframe. [UPDATE: Have a problem now coming from mismatching shapes for x_train and y_train. Error is as follows: \"ValueError: Arguments `target` and `output` must have the same shape. Received: target.shape=(None, 1), output.shape=(None, 4)\"]\n",
    "    # train_history = model.fit(x_trainCopy,epochs=20, batch_size=batch_size) #<-- Removing this since I converted dataframe into tensorflow version of dataframe. \n",
    "    acc = model.evaluate(x_test, y_test, batch_size=batch_size,verbose=0)\n",
    "    # acc = model.evaluate(x_testCopy)\n",
    "    # print(\"---DEBUGGING CHECKPOINT #4: Obtaining Test Accuracy---\")\n",
    "    # pb.set_trace() [COMPLETE!]\n",
    "    # print(\"\\nTest accuracy: %.1f%%\" % (100.0 * acc))\n",
    "    print(\"\\nTest accuracy: %.1f%%\" % (100.0 * acc[1]))\n",
    "    test_stocks[\"Company\"] = test_stocks.index; test_stocks.loc[:,\"Company\"] = test_stocks.loc[:,\"Company\"].astype(\"int32\")\n",
    "\n",
    "    # 6) Verifying and Visualzing the Predictions\n",
    "    # a) Obtaining accuarrcy of the predictions: \n",
    "    \n",
    "    # predictions = model.predict(test_stocks) #<-- NOTE: This retunrns an array of probabilities for each class. Thus, for future consumption by person, could assign these predictions to a column added to test_stocks?\n",
    "    predictions = model.predict(x_test) #<-- NOTE: This retunrns an array of probabilities for each class. Thus, for future consumption by person, could assign these predictions to a column added to test_stocks?\n",
    "    \n",
    "    predictions[0]\n",
    "    \n",
    "    \n",
    "    np.argmax(predictions[0]) #<-- Returns the prediction that Neural Network was most comfortable with. \n",
    "    # def plot_image(i, predictions_array, true_label, img):\n",
    "        # true_label, img = true_label[i], img[i]\n",
    "        # plt.grid(False)\n",
    "        # plt.xticks([])\n",
    "        # plt.yticks([])\n",
    "        # plt.imshow(img, cmap=plt.cm.binary)\n",
    "        # predicted_label = np.argmax(predictions_array)\n",
    "\n",
    "        # if predicted_label == true_label:\n",
    "        #     color = 'blue'\n",
    "        # else:\n",
    "        #     color = 'blue'\n",
    "        \n",
    "        # plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label], 100*np.max(predictions_array), class_names[true_label], color=color))\n",
    "                   # NOTE as of 10/28/25: plot_image function will NOT be needed\n",
    "    # Body of original idea for plotting model's output: \n",
    "    def plot_value_array(i, predictions_array, true_label):\n",
    "        true_label = true_label[i]\n",
    "        plt.grid(False)\n",
    "        plt.xticks(range(10))\n",
    "        plt.yticks([])\n",
    "        thisplot = plt.bar(range(10), predictions_array, color=\"#7777777\")\n",
    "        plt.ylim([0,1])\n",
    "        predicted_label = np.argmax(predictions_array)\n",
    "\n",
    "        thisplot[predicted_label].set_color('red')\n",
    "        thisplot[true_label].set_color('blue')\n",
    "        return\n",
    "\n",
    "    # Below can potentially be done iteratively?\n",
    "    \"\"\" \n",
    "    i = 0\n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.subplot(1,2,1)\n",
    "    # plot_image(i,predictions[i],test_labels,test_images) <-- Not needed. \n",
    "    plt.subplot(1,2,2)\n",
    "    plot_value_array(i,predictions[i],test_labels)\n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "    # End of Body of original idea for plotting model's output: [UPDATE: Commented out this stuff above]\n",
    "\n",
    "\n",
    "    # Alternate way of doing plotting above[for clarity, alternative is referenced BELOW]. \n",
    "    print(\"---DEBUGGING CHECKPOINT #5: Tesiting plotting!---\")\n",
    "    pb.set_trace() # [COMPLETE!]\n",
    "    # Body of plotting model[Link for plotting is here: https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/] \n",
    "\n",
    "    ## Listing all data in history: \n",
    "    print(train_history.history.keys())\n",
    "\n",
    "    ## summarize train_history for accuracy\n",
    "    plt.plot(train_history.history['accuracy'])\n",
    "    # print(train_history.history['val_accuracy']) #<-- NOTE: val_accuracy and val_loss are NOT part of the history object's keys.  \n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    ## summarize history for loss\n",
    "    plt.plot(train_history.history['loss'])\n",
    "    # print(train_history.history['val_loss']) #<-- NOTE: val_accuracy and val_loss are NOT part of the history object's keys. \n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "    # End of Body of plotting model \n",
    "    # b) Printing Model Summary, which can be good to go into detail about: \n",
    "    model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # end of 6)\n",
    "# \"\"\"\n",
    "attempt3()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b0ff61",
   "metadata": {},
   "source": [
    "- Context: Then we set up the model parameters[part b)]: [note, make habit to employ content-codeSnippet pairs!]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e3d8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # ----May snippet above may be optional?[UPDATE: Above is required, need to come up with normalization process]---\n",
    "# \"Network Parameters\"\n",
    "# NOTE: The following will reference a list of tuple(s) that will be used to facilitate first experiment. \n",
    "listOfExperimentSetupFuncs = [\"\"\"Plan to insert functions for doing experiment(s) setup here\"\"\"]\n",
    "global batch_size, hidden_units, dropout\n",
    "global model #<-- Have this here, so experimental setup functions can tweak model as needed. \n",
    "batch_size = 128\n",
    "# batch_size = experiment1Tuples[0][0] \n",
    "hidden_units = 256\n",
    "# hidden_units = experiment1Tuples[0][1]\n",
    "dropout = 0.45\n",
    "# dropout  = experiment1Tuples[0][2]\n",
    "isExperimentSetup3Active = False\n",
    "# model = Sequential()\n",
    "model = Sequential()\n",
    "def experimentSetup0():\n",
    "    # This will reference the default settings irrespective to experiments. \n",
    "    print(\"---Undergoing Experiment Setup #0---\")\n",
    "    # model.add(Dense(hidden_units,input_dim=input_size))\n",
    "    # model.add(Dense(hidden_units,input_shape=(input_size-1,)))\n",
    "    # print(\"---DEBUGGING CHECKPOINT: Checking model's reactions---\")\n",
    "    # pb.set_trace()\n",
    "    # model.add(Input((input_size,)))\n",
    "    model.add(Input((x_train.shape[1],)))\n",
    "    # model.add(Dense(hidden_units,input_shape=(input_size,)))\n",
    "    model.add(Dense(hidden_units))\n",
    "    # NOTE: Above is causing following error: \"ValueError: Exception encountered when calling Sequential.call(). Invalid input shape for input Tensor(\"data:0\", shape=(5,), dtype=float32). Expected shape (None, 6), but input has incompatible shape (5,)\" [UPDATE: Error is originating from fact that x_trainCopy and x_testCopy 's shapes are (5,) and (5,)]\n",
    "    model.add(Activation('relu')) #<-- This is used to add activation function to model[UPDATE: May need to replace Activation('relu') by making them default...not sure to facilitate experimentSetup3 OR I can simply see what happens when the 2nd to LAST acivation is changed] \n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(hidden_units))\n",
    "    model.add(Activation('relu')) if experimentSetup3() == None else experimentSetup3() #<-- This changes the Activation function, adhering to experimentSetup0![Thus, as of 11/23/25: There are two experiments ready to go for Milestone 1]\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(num_labels))\n",
    "    print(\"---End of Experiment Setup #0---\")\n",
    "    return\n",
    "\n",
    "def experimentSetup1():\n",
    "    # Goal of exp: Want to see how model params affect the acuaracy of the model by modifying batch size and hidden units and dropout[pn: make sure to have a subset of the powerset of params be modified in this exp. ]\n",
    "    print(\"---Undergoing Experiment Setup #1---\")\n",
    "    setN = 0 #<-- Change number for this to get values from resp sets. \n",
    "    experiment1Tuples: list[tuple] = [(128,256,0.45), (64,128,0.45), (\"\"\"NOTE: Other tuples can change one or more parameters whilst keeping at least one constant\"\"\")]\n",
    "    batch_size = experiment1Tuples[setN][0] \n",
    "    hidden_units = experiment1Tuples[setN][1]\n",
    "    dropout = experiment1Tuples[setN][2]\n",
    "    \n",
    "    print(\"---End of Experiment Setup #1---\")\n",
    "    return\n",
    "def experimentSetup2(numQuantLvls = 2):\n",
    "    # Goal of exp: Want to see model performance based on degree of quantanization of data\n",
    "    print(\"---Undergoing Experiment Setup #2---\")\n",
    "    # NOTE: Below will involve replacing 255 with a different number based on degree of quantanization of data. \n",
    "    # UPDATE: Only thing left is pulling maximum value from x_train and min value from train and test respectively. \n",
    "    widthsOfQuant = []\n",
    "    \n",
    "    widthsOfQuant[0] = (x_trainMax - x_trainMin)/(numQuantLvls - 1)\n",
    "    x_train = np.reshape(x_train, [-1,input_size])\n",
    "    x_trainMax = 0;\n",
    "    x_trainMin = 0;\n",
    "    \n",
    "    x_test = np.reshape(x_test, [-1,input_size])\n",
    "    x_testMax = 0;\n",
    "    x_testMin = 0;\n",
    "    widthsOfQuant[1] = (x_testMax - x_testMin)/(numQuantLvls - 1); \n",
    "    x_train = x_train.astype('float32')/widthsOfQuant[0]\n",
    "    x_test = x_test.astype('float32')//widthsOfQuant[1]\n",
    "    print(\"---End of Experiment Setup #2---\")\n",
    "    return\n",
    "\n",
    "def experimentSetup3():\n",
    "    # Goal of exp: Want to see model performance based on type of activation function from a subset of all possible activation functions. \n",
    "    print(\"---Undergoing Experiment Setup #3---\" if isExperimentSetup3Active else \"---Experiment Setup #3 was skipped---\")\n",
    "    activationFuncs = ['elu', 'sigmoid', 'tanh' ]\n",
    "    # model.add(Activation(activationFuncs[0])) #<-- May need this since I have a classification problem that I'm wokring on. [UPDATE: Made this a return value instead] \n",
    "\n",
    "    print(\"---End of Experiment Setup #3---\" if isExperimentSetup3Active else \"---End of Experiment Setup #3 was skipped---\")\n",
    "\n",
    "    return model.add(Activation(activationFuncs[0])) if isExperimentSetup3Active == True else None #<-- May need this since I have a classification problem that I'm wokring on. \n",
    "\n",
    "def experimentSetup4():\n",
    "    print(\"---Undergoing Experiment Setup #4---\")\n",
    "    print(\"---End of Experiment Setup #4---\")\n",
    "    return\n",
    "\n",
    "\n",
    "# \"Model is a 3-layer ML with ReLU and dropout after each layer\": \n",
    "#model = Sequential()\n",
    "#model.add(Dense(hidden_units,input_dim=input_size))\n",
    "#model.add(Activation('relu')) #<-- This is used to add activation function to model\n",
    "#model.add(Dropout(dropout))\n",
    "#model.add(Dense(hidden_units))\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(Dropout(dropout))\n",
    "#model.add(Dense(num_labels))\n",
    "# \"This is the output for one-hot vector\"\n",
    "#model.add(Activation('softmax')) #<-- May need this since I have a classification problem that I'm wokring on. \n",
    "#model.summary()\n",
    "#plot_model(model,to_file='mlp-mnist.png', show_shapes=True)\n",
    "\n",
    "# \"Loss Function for one-hot vector\"\n",
    "# \"Use of adam optimizer\"\n",
    "# \"Accuracy is good metric for classification tasks\": \n",
    "# print(\"---DEBUGGING CHECKPOINT #2: Making attempt to test before running Experiment 0---\")\n",
    "# pb.set_trace()#[experimentSetup0 was successful]\n",
    "\n",
    "experimentSetup0() \n",
    "# \"This is the output for one-hot vector\"\n",
    "model.add(Activation('softmax')) #<-- May need this since I have a classification problem that I'm wokring on. \n",
    "model.summary()\n",
    "#plot_model(model,to_file='mlp-mnist.png', show_shapes=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer='adam',\n",
    "metrics=['accuracy'])\n",
    "\n",
    "# \"Train the network\"\n",
    "# BUG: There is a bug that occurs here that prods the following message: ValueError: Unrecognized data type: x=  [Apparently, there is an unrecognized data type in x_train variable]\n",
    "x_train.loc[:,\"Company\"] = pd.Series([2**i for i in range(x_train.shape[0])]) \n",
    "x_test.loc[:,\"Company\"] = pd.Series([2**i for i in range(x_train.shape[0])]) \n",
    "x_train = x_train.astype(\"float64\")\n",
    "x_test = x_test.astype(\"float64\")\n",
    "x_train.loc[:,\"Company\"] = x_train.loc[:,\"Company\"].astype(\"int32\")\n",
    "x_test.loc[:,\"Company\"] = x_test.loc[:,\"Company\"].astype(\"int32\")\n",
    "# y_train = pd.Series(y_train)\n",
    "# y_test = pd.Series(y_test)\n",
    "# NOTE: Body of Problem is coming from x_train and x_test!\n",
    "x_trainCopy = tf.data.Dataset.from_tensor_slices((x_train.values.astype(np.float32),tf.convert_to_tensor(y_train).numpy().astype(np.float32)))\n",
    "x_testCopy = tf.data.Dataset.from_tensor_slices((x_test.values.astype(np.float32),tf.convert_to_tensor(y_test).numpy().astype(np.float32)))\n",
    "# NOTE: Body of Problem is coming from x_train and x_test!\n",
    "# x_train = x_trainCopy \n",
    "# x_test = x_testCopy \n",
    "\n",
    "print(\"---DEBUGGING CHECKPOINT #3: Making attempt to test before training network---\")\n",
    "pb.set_trace()\n",
    "# UPDATE: FINISHED MODEL!!\n",
    "train_history = model.fit(x_train,y_train,epochs=20, batch_size=batch_size) #<-- Removing this since I converted dataframe into tensorflow version of dataframe. [UPDATE: Have a problem now coming from mismatching shapes for x_train and y_train. Error is as follows: \"ValueError: Arguments `target` and `output` must have the same shape. Received: target.shape=(None, 1), output.shape=(None, 4)\"]\n",
    "# train_history = model.fit(x_trainCopy,epochs=20, batch_size=batch_size) #<-- Removing this since I converted dataframe into tensorflow version of dataframe. \n",
    "acc = model.evaluate(x_test, y_test, batch_size=batch_size,verbose=0)\n",
    "# acc = model.evaluate(x_testCopy)\n",
    "# print(\"---DEBUGGING CHECKPOINT #4: Obtaining Test Accuracy---\")\n",
    "# pb.set_trace() [COMPLETE!]\n",
    "# print(\"\\nTest accuracy: %.1f%%\" % (100.0 * acc))\n",
    "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * acc[1]))\n",
    "test_stocks[\"Company\"] = test_stocks.index; test_stocks.loc[:,\"Company\"] = test_stocks.loc[:,\"Company\"].astype(\"int32\")\n",
    "\n",
    "# 6) Verifying and Visualzing the Predictions\n",
    "# a) Obtaining accuarrcy of the predictions: \n",
    "\n",
    "# predictions = model.predict(test_stocks) #<-- NOTE: This retunrns an array of probabilities for each class. Thus, for future consumption by person, could assign these predictions to a column added to test_stocks?\n",
    "predictions = model.predict(x_test) #<-- NOTE: This retunrns an array of probabilities for each class. Thus, for future consumption by person, could assign these predictions to a column added to test_stocks?\n",
    "\n",
    "predictions[0]\n",
    "\n",
    "\n",
    "np.argmax(predictions[0]) #<-- Returns the prediction that Neural Network was most comfortable with. \n",
    "# def plot_image(i, predictions_array, true_label, img):\n",
    "    # true_label, img = true_label[i], img[i]\n",
    "    # plt.grid(False)\n",
    "    # plt.xticks([])\n",
    "    # plt.yticks([])\n",
    "    # plt.imshow(img, cmap=plt.cm.binary)\n",
    "    # predicted_label = np.argmax(predictions_array)\n",
    "\n",
    "    # if predicted_label == true_label:\n",
    "    #     color = 'blue'\n",
    "    # else:\n",
    "    #     color = 'blue'\n",
    "    \n",
    "    # plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label], 100*np.max(predictions_array), class_names[true_label], color=color))\n",
    "                # NOTE as of 10/28/25: plot_image function will NOT be needed\n",
    "# Body of original idea for plotting model's output: \n",
    "def plot_value_array(i, predictions_array, true_label):\n",
    "    true_label = true_label[i]\n",
    "    plt.grid(False)\n",
    "    plt.xticks(range(10))\n",
    "    plt.yticks([])\n",
    "    thisplot = plt.bar(range(10), predictions_array, color=\"#7777777\")\n",
    "    plt.ylim([0,1])\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "\n",
    "    thisplot[predicted_label].set_color('red')\n",
    "    thisplot[true_label].set_color('blue')\n",
    "    return\n",
    "\n",
    "# Below can potentially be done iteratively?\n",
    "\"\"\" \n",
    "i = 0\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.subplot(1,2,1)\n",
    "# plot_image(i,predictions[i],test_labels,test_images) <-- Not needed. \n",
    "plt.subplot(1,2,2)\n",
    "plot_value_array(i,predictions[i],test_labels)\n",
    "plt.show()\n",
    "\"\"\"\n",
    "# End of Body of original idea for plotting model's output: [UPDATE: Commented out this stuff above]\n",
    "\n",
    "\n",
    "# Alternate way of doing plotting above[for clarity, alternative is referenced BELOW]. \n",
    "# print(\"---DEBUGGING CHECKPOINT #5: Tesiting plotting!---\")\n",
    "# pb.set_trace() # [COMPLETE!]\n",
    "\"\"\" \n",
    "NOTE: Not needed below since I need to have model plotting be visible everywhere. \n",
    "# Body of plotting model[Link for plotting is here: https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/] \n",
    "\n",
    "## Listing all data in history: \n",
    "print(train_history.history.keys())\n",
    "\n",
    "## summarize train_history for accuracy\n",
    "plt.plot(train_history.history['accuracy'])\n",
    "# print(train_history.history['val_accuracy']) #<-- NOTE: val_accuracy and val_loss are NOT part of the history object's keys.  \n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "## summarize history for loss\n",
    "plt.plot(train_history.history['loss'])\n",
    "# print(train_history.history['val_loss']) #<-- NOTE: val_accuracy and val_loss are NOT part of the history object's keys. \n",
    "plt.title('model loss')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    " \"\"\"\n",
    "\n",
    "\n",
    "# End of Body of plotting model \n",
    "# b) Printing Model Summary, which can be good to go into detail about: \n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# end of 6)\n",
    "# \"\"\"\n",
    "# attempt3() <-- NOT needed for jupyter notebook!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed44084",
   "metadata": {},
   "source": [
    "### Results \n",
    "- Context: [note, make habit to employ content-codeSnippet pairs![Make sure results section consists of python code that utilizes data visualization]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4fef56",
   "metadata": {},
   "outputs": [],
   "source": [
    "## summarize history for loss\n",
    "plt.plot(train_history.history['loss'])\n",
    "# print(train_history.history['val_loss']) #<-- NOTE: val_accuracy and val_loss are NOT part of the history object's keys. \n",
    "plt.title('model loss')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# End of Body of plotting model \n",
    "# b) Printing Model Summary, which can be good to go into detail about: \n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047955d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Body of plotting model[Link for plotting is here: https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/] \n",
    "\n",
    "## Listing all data in history: \n",
    "print(train_history.history.keys())\n",
    "\n",
    "## summarize train_history for accuracy\n",
    "plt.plot(train_history.history['accuracy'])\n",
    "# print(train_history.history['val_accuracy']) #<-- NOTE: val_accuracy and val_loss are NOT part of the history object's keys.  \n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c1ff16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code that displays data visualization here[COMPLETE, can be seen above. At this point, need to copy and paste above, as template for next experiment below!]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7010e0c",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "- Context: [make sure this portion consists of summary of results and determine if hypothesis was met or unmet]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119ab6ef",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b40c52c4",
   "metadata": {},
   "source": [
    "## Experiment 2[<Insert title of Experiment>]\n",
    "### Purpose\n",
    "- Setting up Classification problem and visualizing the model accuracy based on permutations of 3-tuple that references the model parameters. \n",
    "### Hypothesis\n",
    "- I believe that the ratio of constants in the 3-tuple should be <> of the ML Model paramters for the ML Model's accurarcy to be greater than the ML Model's accuracy when the ratio is not used. \n",
    "\n",
    "[NOTE: Need to replace both purpoe and hypothesis]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3d0b50",
   "metadata": {},
   "source": [
    "### Instructions to Run\n",
    "Instructions: 1) Setting up Model, 2) Using three situations where the 3-tupeles are different.Context: First we run code that creates the datasets needed to be fed to the model: [note, make habit to employ content-codeSnippet pairs!]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040e8932",
   "metadata": {},
   "source": [
    "- Context: First we do the data preprocessing before it meets with the model[part a)]: [note, make habit to employ content-codeSnippet pairs!]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5710291d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing Dependencies and or code for setting up virtual environment: \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# End of Installing Dependencies and or code for setting up virtual environment: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8736f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Fill in this portion with data engineering procedure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f63366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This file will be built based on the contents of the purpose.md file. As of now, the amount of functiosn here, should be the number of subsystems, and then those subsystems' steps should have their own functions. This will help with doing component-based debugging opposed to dealing with all dependencies at once. If functions are nested, make sure to denote them as such. \n",
    "\n",
    "# Reports: [10/23/25] part 1) At this point, finished writing abstract rep of how prog will acheive goal. Started using a functional prog format to acheive goals. In next sesh, focus should be on finishing up setting up coding env programatically FIRST, and then inserting the neccessary pandas stuff, whilst simulatnatneously doing more research on pandas as well. part 2) Continued learning pandas stuff, and continued seting up code env programatically. In next sesh, continue the aforementioned process. When significant prog is made, begin inserting code related to data engineering. Also, when establishing pipeline this search query may be useful: `start chrome \"? is the relative path for a filepath in python script relative to the place it is executed?\"`. [10/24/25] part 1) Continued seting of programmatic env. At this point, I believe Subssytem2's programmatic rep should be worked on next, whilst simulataneously implementing more pandas-specific code. part 2) Continued working on programming code. Adhere to aforemetioend request for next session. part 3) Starting setting up Subsystem2's programmatic representation. In next sesh, make sure to implement a quick algo that seraches for the right table to pull from both the real-time data and the historical data, in addition to the aforementioned tasks from previous report(s). part 4) Started process for integrating external dataframes. Did not adhere to aformentioned objs directly. In next sesh, make sure to adhere to aformentioned objectives. part 5) Started wriritng logic code to facilitate optimality algo. Also worked on algo for obtaining the right attrs from the Ticker Object. In next sesh, take time to finish up Pandas tutorial. part 6) Continued where I left off. In next sesh, start testing out certain portions of program. part 7) Got the algorithms implemented, began testingg parts of program. Now, the only problem is having the varaibels reference the right columns. These places are marked with REPLACEMENT PENDING. In next sesh, go back to learning some more pandas and then work on finishing up the rest of the stuff. May be wise to jump to Time Series data chapter from Python for Data Analysis Textbook to interface with dates properly.\n",
    "# Reports: [10/26/25] part 1) Made more progress by fixing contents relative to error. Had to put the dataframes needd into the python list to be accessed later in the script. In next sesh, continue this process and addresss the 'REPLACEMENT PENDING' notes and make neccessary replacements. part 2) Added a few more things. Added nomenclature for documentation w.r.t comments, to ease the process of problem solving. Addressed a few of the requirements. part 2) Started testing other stuff. Didn't do anything related to time series learning for pandas. In next sesh, focus on learning pandas from geeksforgeeks and going through times series chapter from that textbook. Then, finish this file to integrate to model with this egnienered dataset. Also, there is an error, use code output for reference. part 3) Found some logistical issues wiht some parts. Started process of getting 2nd cond implemented. Really, want to get this proj finished by end of 10/27/25 to get a decent grade.  [10/27/25] part 1) Worked on outermost pipeline script. Going forward, working in reverse order from end of data prep pipeline to beginning to ensure that the nested function(s) and function(s) work properly. part 2) Have some issues getting condition 2. part 3) Got dataframe for condition 2. Think now I need to designate the date. Will be KEY for making decisions. part 3) Made a decent amount of progress. Used comments to update some steps that need to be taken in order to ensure smooth integartion between data prep and model development. In next sesh, continue solving problems to get data prepped for model dev. Particularly, write code that applys the subsystems iteratively to each company. part 4) got subsystem1(compA()) complete, in terms of logic. However, will be imperative to handle presence of null values and omitting them when possible. **PN: I believe it has something to do with the fact that the indices for the row tuples is date-based**. In next sesh, continue to the subsystem2 to ensure that they work logically. part 5) Started working on logic for subsys2. Noticed that I need to handle the creation of dataframe with all companies on it properly. Need to figure out how to do that in subsys2. [10/29/25] part 1) Got the class for subsy2 indented properly for testing. In next sesh, continue runnign tests to ensure that subsy2's components work properly.  [10/31/25] part 1) Started working more on model file to facilitate process for running experiments via Model pipeline. In Next sesh, continue learning more about pandas, and datetime object so you can sync up the times properly to get values lined up to get perimissible data output. Also, may want to make time working on reasons why these milestone were so late. [11/7/25] part 1) Made significant progress. Finally understanding debugging process better now. **Focus is now shifting more towards experimental setups, and creating environment to load images to web application**. \n",
    "\n",
    "# Notable things: a) This search query specifies that it's possible to access attributes for a class. THis allows me to programatically identify the stuff that I need opposed to doing things brute force: `start chrome \"? is there a list of attribues for a particular class in Python?\" `b) Make sure to make copy of this python file and then run it in the MLLifecycle directory. c) NOTE: Think it'd be wise to insert some breakpoints/places to use pdb to ensure that things run as intended. I believe it will make debugging more efficient. d) As of 11/3/25, my new problem is syncing the dates for the data. Iniiat soln consisted of using start and end parameters to specify the bounds needed and then syncing the dates that way. Before this is implemented, I need to ensure that the rest of pipeline is provably correct still so my it soln can be plugged and played. e) As of 11/6/25, the solution is nearly complte. At this point, the problem is ensuring that n companies are allowed to be entered, and ensuring that at the end, each company has its own row-tuple.  f) Everything works up until after checkpoint #2 from a provabiilty perspective as of 11/6/25 . ALSO, NEED TO ADD A STATE THAT ALLOWS USER TO SPECIFY THE STOCKS THEY WANT TO CHOOSE. \n",
    "\n",
    "# IMPORTANT: As of 11/22/25: The main focus now is ensuring that the Price from P/B and P/E are pulled from correct data. After that, everything should work properly! To verify, review code again and add any updates here: a) Learned that open should be replaced with closing since, according to net, it refers to the actual price. \n",
    "# Necessary imports\n",
    "import pandas as pd;\n",
    "import numpy as np;\n",
    "import yfinance as yf; #<-- Needed to access Yahoo Finance Dataset(s)\n",
    "# Below is needed to get the real-time data\n",
    "from datetime import datetime, timedelta, date\n",
    "# UPDATE: timedelta will be used to programmatically do the optimality algorithm. Refer to notes on timedelta class for reference.  \n",
    "import pdb as pb #<-- NOTE: Need to use this to test certain sections of code to debug and make neccessary\n",
    "# End of Necessary imports\n",
    "\n",
    "# NOTE: Make sure certain vars declared are nonlocal so they can be used in other functions!\n",
    "global seriesVersionOn\n",
    "seriesVersionOn = True\n",
    "global resultantDataFrame\n",
    "resultantDataFrame: pd.DataFrame = pd.DataFrame(columns=[\"P/B\", \"P/E\", \"NCAV\", \"Date For Eval\", \"Company\", \"Optimality\"])\n",
    "resultantDataFrame[\"P/B\"].astype(\"Float64\")\n",
    "resultantDataFrame[\"P/E\"].astype(\"Float64\")\n",
    "resultantDataFrame[\"NCAV\"].astype(\"Float64\")\n",
    "# resultantDataFrame[\"Date for Eval\"].astype(datetime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b463424d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class subsys1:\n",
    "    def __init__(self,param1 = \"\"\"Insert any params that may be sufficient here!\"\"\"):\n",
    "        print(\"---Subsystem 1 in Progress---\")\n",
    "        # self.compA(param1)\n",
    "    # component a) [mapping formulas using dep vars and indep vars]\n",
    "    def compA(self,company): #<-- NOTE: Thought about passing in dataframe as a parameter as well for instances where multiple companies need to be added to resultant dataframe. \n",
    "        # PRatioBug = False\n",
    "        PRatioBug = True\n",
    "        print(\"--Component a[Subsystem 1] in progress--\")\n",
    "        global arrOfDataFramesNeeded \n",
    "        #arrOfDataFramesNeeded: list[pd.DataFrame] = []\n",
    "        arrOfDataFramesNeeded = []\n",
    "         \n",
    "        # Obtaining the dataframes from the yfinance api for company i\n",
    "        print(\"-Obtaining the DataFrames from the yfinance api for company i-\")\n",
    "        timePeriod = \"4mo\"\n",
    "        timeInterval = \"1mo\"\n",
    "        # PN: Since Price, Books will be used will need real-time data. \n",
    "        # Obtaining real-time data\n",
    "        if (company == None):\n",
    "            company = \"MSFT\"\n",
    "        \n",
    "        companyAlias = company or \"MSFT\"#<-- For Microsoft[using default operator here to prep function to be generalized to add other companies]\n",
    "        # pb.set_trace()\n",
    "        ticker = yf.Ticker(f\"{companyAlias}\") #<-- NOTE: Will need to replace this with multiple tickers later on. \n",
    "        #ticker.info\n",
    "        #ticker.info.keys() #<-- Cotnains p/b ratio but doesn't have p/e ratio. So, will not be using it. \n",
    "        \"\"\"\n",
    "        listOfAttrs = dir(ticker)\n",
    "        for attrib in listOfAttrs:\n",
    "            # psuedosteps: a) Accessing curr atrib, b) if it rets a dataframe, then checking columns to see if it matches what I Pneed. \n",
    "            #for j in range(6): #<-- replacing later on, just a placeholder rn.\n",
    "           if \"High\" in getattr(ticker,attrib).columns:\n",
    "           # Means that the attrib should be used to obtain data needed:\n",
    "           arrOfAttrsFromExtDataFramesNeeded.append(attrib)\n",
    "           # ^^ NOTE: Above needs to be generalized[may need a nested for loop before this if statement to go through ALL requested col ids]\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        global end_date #<-- Set gloabl so end_date can be accessed. \n",
    "        start_date: datetime = datetime.today() - timedelta(days=10) ; end_date = datetime.today() #+ timedelta(days=1*365) <-- UPDATE: Took this out b/c start_date needs to begin in past, and end date needs to be in present. \n",
    "        yearsRelToDateInQ: datetime =  start_date - timedelta(days=5*365)\n",
    "        start_date = start_date.date().isoformat()\n",
    "        end_date = end_date.date().isoformat()\n",
    "        # UPDATE: Below, I setup environmetn such that, when start_date and end_date are entered, then the pipeline works as expected. \n",
    "        # historical_data = ticker.history(period=timePeriod, interval=timeInterval).tz_localize(None) if start_date == None and end_date == None else ticker.history(start=start_date.date().isoformat(), end=end_date.date().isoformat()).tz_localize(None) <-- commented out for obvious reasons. \n",
    "        historical_data = ticker.history(period=timePeriod, interval=timeInterval).tz_localize(None) if start_date == None and end_date == None else ticker.history(start=start_date, end=end_date).tz_localize(None)\n",
    "        print(historical_data)\n",
    "        # end of Obtaining real-time data\n",
    "        \n",
    "        # PN: I believe NCAV == Net Tangible Assets\n",
    "        # Obtaining historical data\n",
    "        # quarterlyBalanceSheet = ticker.balance_sheet.T \n",
    "        quarterlyBalanceSheet = ticker.quarterly_balance_sheet.T \n",
    "        # UPDATE[NOTE]: Will need body that queries for row tuples in the start_date and end_date range\n",
    "        \n",
    "        arrOfDataFramesNeeded = [historical_data, quarterlyBalanceSheet]\n",
    "        print(quarterlyBalanceSheet)\n",
    "        print(quarterlyBalanceSheet.columns)\n",
    "        # Below involves getting 5 yr data to support condition #2. \n",
    "        # ISSUE #1[complete]\n",
    "        #conditionForDataFrame2 = ticker.history(start=arrOfDataFramesNeeded[0].index[0],period=\"1y\", interval=\"5y\").tz_localize(None) #<-- Here, I set start date to first tuple's date from historical_data var and then get the data 5 yrs BACK relative to that date. \n",
    "        conditionForDataFrame2 = ticker.history(period=\"5y\").tz_localize(None) #<-- Here, I set start date to first tuple's date from historical_data var and then get the data 5 yrs BACK relative to that date. \n",
    "        balanceSheet = ticker.balance_sheet.T \n",
    "        # Body of assigning P/E column\n",
    "\n",
    "        \n",
    "        # End of Body of assigning P/E column\n",
    "        # Below, the dates within the bounds of start date and 5 years succeeding start date index is queried to obtain relevant data to be used later. \n",
    "        print(\"---Setting checkpoint to see what happens to conditionForDataFrame2 variable---\")\n",
    "        # pb.set_trace()\n",
    "        # conditionForDataFrame2 = conditionForDataFrame2 if start_date == None and end_date == None else conditionForDataFrame2.loc[start_date:yearsRelToDateInQ, :];\n",
    "        # NOTE: ^^ Above caused an error due to else body of ternary operator. Thus, statement has been commented out. \n",
    "        \n",
    "        # End of body of testing out bounds interval ideas\n",
    "        print(conditionForDataFrame2)\n",
    "        # END OF ISSUE #1\n",
    "        arrOfDataFramesNeeded.append([conditionForDataFrame2])\n",
    "        # end of Obtaining historical data\n",
    "        print(\"-End of Obtaining the DataFrames from the yfinance api for company i-\")\n",
    "        print(\"Entering Debuging Mode for Checkpoint #1\")\n",
    "        # pb.set_trace() #<-- Will use this   tcheck for vals of variable(s), as well as set certain vars to certain values to cause different behavior.   \n",
    "        # CHECKPOINT #1: In proof, at this point, datafmres will be obtained to begin process of assigning formulas.           # For Everything else, I just need historical data. Correct me if wrong here: .\n",
    "        # End of Obtaining the dataframes from the yfinance api for company i\n",
    "\n",
    "        # Debugging Report [11/13/25] part 1) When running through debugger, found that the columns are assigned properly, BUT some of the computations for the data engineering results in NaN values. Need to figure out why, think it can be solved by syncing datetime(s). Continue progress on line 111 via the debugger. Jump to that line by finding command to jump to line 111. \n",
    "        #return\n",
    "        boolExpMain = \"\"\"Can insert a bool exp that checks for number of values in company column, indicating that dataFrame still exists\"\"\"\n",
    "        alreadyExists = False if boolExpMain == True else True  #<-- setting to 0 by default\n",
    "        print(\"-Beginning of assigning formulas-\")\n",
    "        global independentVars\n",
    "        \n",
    "        # independentVars = [\"High\" if PRatioBug == False else \"Share Issued\",\"Tangible Book Value\",\"High\",\"Retained Earnings\",\"Net Tangible Assets\"]\n",
    "        independentVars = [\"Close\" if PRatioBug == False else \"Open\",\"Tangible Book Value\",\"High\",\"Retained Earnings\",\"Net Tangible Assets\"]\n",
    "        global DependentVars\n",
    "        DependentVars = [\"P/B\", \"P/E\", \"NCAV\", \"Date For Eval\"] #<-- PN: Date for Eval was created to reference the date to use to make relevant decisions.  \n",
    "        DependentVars.append(\"Share Price\")\n",
    "        global dataFramesToBeShipped\n",
    "        dataFramesToBeShipped = [pd.DataFrame(), pd.DataFrame()]\n",
    "        DependentVars.append(\"Company\")\n",
    "        # NOTE: alreadyExists prevents currentDataframe from being updated allowing other companies to be added to resultant dataframe. \n",
    "        dataFramesToBeShipped[0] = pd.DataFrame(columns=DependentVars) if alreadyExists == False else dataFramesToBeShipped[0] #<-- TrainingData\n",
    "        print(dataFramesToBeShipped[0])\n",
    "        DependentVars.append(\"Optimality\")\n",
    "        dataFramesToBeShipped[1] = pd.DataFrame(columns=DependentVars) if alreadyExists == False else dataFramesToBeShipped[1] #<-- TestingData\n",
    "        print(dataFramesToBeShipped[1])\n",
    "        # Assigning the formulas\n",
    "        print(\"-Assigning formulas-\")\n",
    "        # pb.set_trace() <-- COMPLETE![Assinging formulas worked as intended now!]\n",
    "        # NOTE: When assigning, it makes sense to only apply formula to most recent data from quarterlyBalanceSheet aka the first tuple. \n",
    "        # dataFramesToBeShipped[0][DependentVars[0]] = arrOfDataFramesNeeded[0][independentVars[0]]/arrOfDataFramesNeeded[1][independentVars[1]] #<-- Causing error due to: KeyError: 'Tangible Book Value' .[UPDATE: To solve, incremented index in second operand from 0 to 1. part 2) IT WORKED!] #<-- NOTE: References old version. New version adheres to update. \n",
    "        # dataFramesToBeShipped[0][DependentVars[0]] = arrOfDataFramesNeeded[0].loc[arrOfDataFramesNeeded[0].index[0].date().isoformat(),independentVars[0]]/arrOfDataFramesNeeded[1].loc[arrOfDataFramesNeeded[0].index[0].date().isoformat(),independentVars[1]] #<-- Causing error due to: KeyError: 'Tangible Book Value' .[UPDATE: To solve, incremented index in second operand from 0 to 1. part 2) IT WORKED!][UPDATE as of 11/19/25: Error involving key entry. Not sure why, make sure this is focus in next session][part 2) Removing isoformat to see what happens]\n",
    "        # NOTE: Need a conditional change here, based on PBRatio boolean. \n",
    "        pullingFromTickerInfo = True\n",
    "        if not PRatioBug:\n",
    "            dataFramesToBeShipped[0].loc[0,DependentVars[0]] = arrOfDataFramesNeeded[0].loc[arrOfDataFramesNeeded[0].index[0].date().isoformat(),independentVars[0]]/arrOfDataFramesNeeded[1].loc[arrOfDataFramesNeeded[1].index[0].date().isoformat(),independentVars[1]] #<-- Causing error due to: KeyError: 'Tangible Book Value' .[UPDATE: To solve, incremented index in second ope rand from 0 to 1. part 2) IT WORKED!][UPDATE as of 11/19/25: Error involving key entry. Not sure why, make sure this is focus in next session]\n",
    "            print(\"---\")\n",
    "            print(arrOfDataFramesNeeded[0].columns)\n",
    "            print(\"---\")\n",
    "            print(arrOfDataFramesNeeded[0].loc[arrOfDataFramesNeeded[0].index[0].date().isoformat(),independentVars[0]])\n",
    "            print(arrOfDataFramesNeeded[1].loc[arrOfDataFramesNeeded[1].index[0].date().isoformat(),independentVars[1]])\n",
    "            dataFramesToBeShipped[0].loc[0,DependentVars[1]] = arrOfDataFramesNeeded[0].loc[arrOfDataFramesNeeded[0].index[0].date().isoformat(),independentVars[2]]/arrOfDataFramesNeeded[1].loc[arrOfDataFramesNeeded[1].index[0].date().isoformat(),independentVars[3]]\n",
    "            dataFramesToBeShipped[0].loc[0,DependentVars[2]] = arrOfDataFramesNeeded[1].loc[arrOfDataFramesNeeded[1].index[0].date().isoformat(),independentVars[4]] #<-- Here, NCAV comes from balance sheet. \n",
    "            print(dataFramesToBeShipped[0])\n",
    "            # seriesVersionOn = True <-- This is now a global variable. \n",
    "            # end of Assigning the formulas\n",
    "        elif PRatioBug and pullingFromTickerInfo == True:\n",
    "            # Need to chang below based on fact that High turns into Shares Issued from arrOfDataFramesNeeded[1] . \n",
    "            # print(\"--DEBUGGING CHECKPOINT: Adressing behavior causing error involving Share Price issue---\")\n",
    "            # pb.set_trace()\n",
    "            # Decided to replace Price with ticker.info[\"regularMarketPrice\"]\n",
    "            # dataFramesToBeShipped[0].loc[0,DependentVars[0]] = ticker.info[\"regularMarketPrice\"]/arrOfDataFramesNeeded[1].loc[arrOfDataFramesNeeded[1].index[0].date().isoformat(),independentVars[1]] #<-- Causing error due to: KeyError: 'Tangible Book Value' .[UPDATE: To solve, incremented index in second ope rand from 0 to 1. part 2) IT WORKED!][UPDATE as of 11/19/25: Error involving key entry. Not sure why, make sure this is focus in next session]\n",
    "            dataFramesToBeShipped[0].loc[0,DependentVars[0]] = ticker.info[\"priceToBook\"] #<-- Causing error due to: KeyError: 'Tangible Book Value' .[UPDATE: To solve, incremented index in second ope rand from 0 to 1. part 2) IT WORKED!][UPDATE as of 11/19/25: Error involving key entry. Not sure why, make sure this is focus in next session]\n",
    "            print(\"---\")\n",
    "            print(arrOfDataFramesNeeded[0].columns)\n",
    "            print(\"---\")\n",
    "            print(ticker.info[\"regularMarketPrice\"])\n",
    "            print(arrOfDataFramesNeeded[1].loc[arrOfDataFramesNeeded[1].index[0].date().isoformat(),independentVars[1]])\n",
    "            dataFramesToBeShipped[0].loc[0,DependentVars[1]] = ticker.info[\"regularMarketPrice\"]/ticker.info[\"earningsQuarterlyGrowth\"]\n",
    "\n",
    "            \n",
    "            dataFramesToBeShipped[0].loc[0,DependentVars[2]] = arrOfDataFramesNeeded[1].loc[arrOfDataFramesNeeded[1].index[0].date().isoformat(),independentVars[4]] #<-- Here, NCAV comes from balance sheet. \n",
    "            # dataFramesToBeShipped[0].loc[0, DependentVars[3]] = ticker.info[\"regularMarketPrice\"]\n",
    "            print(dataFramesToBeShipped[0])\n",
    "            print(\"--DEBUGGING CHECKPOINT: Ensuring that share price returns corrrect value aka ensuring that attrubte is populated properly--\")\n",
    "            # pb.set_trace()\n",
    "            dataFramesToBeShipped[0].loc[0,\"Share Price\"] = ticker.info[\"regularMarketPrice\"]\n",
    "            # seriesVersionOn = True <-- This is now a global variable. \n",
    "            # end of Assigning the formulas\n",
    "        else:\n",
    "            # Need to chang below based on fact that High turns into Shares Issued from arrOfDataFramesNeeded[1] . \n",
    "            print(\"--DEBUGGING CHECKPOINT: Adressing behavior causing error involving Share Price issue---\")\n",
    "            # pb.set_trace()\n",
    "            # Decided to replace Price with ticker.info[\"regularMarketPrice\"]\n",
    "            dataFramesToBeShipped[0].loc[0,DependentVars[0]] = ticker.info[\"regularMarketPrice\"]/arrOfDataFramesNeeded[1].loc[arrOfDataFramesNeeded[1].index[0].date().isoformat(),independentVars[1]] #<-- Causing error due to: KeyError: 'Tangible Book Value' .[UPDATE: To solve, incremented index in second ope rand from 0 to 1. part 2) IT WORKED!][UPDATE as of 11/19/25: Error involving key entry. Not sure why, make sure this is focus in next session]\n",
    "            print(\"---\")\n",
    "            print(arrOfDataFramesNeeded[0].columns)\n",
    "            print(\"---\")\n",
    "            print(ticker.info[\"regularMarketPrice\"])\n",
    "            print(arrOfDataFramesNeeded[1].loc[arrOfDataFramesNeeded[1].index[0].date().isoformat(),independentVars[1]])\n",
    "            dataFramesToBeShipped[0].loc[0,DependentVars[1]] = ticker.info[\"regularMarketPrice\"]/arrOfDataFramesNeeded[1].loc[arrOfDataFramesNeeded[1].index[0].date().isoformat(),independentVars[3]]\n",
    "\n",
    "            \n",
    "            dataFramesToBeShipped[0].loc[0,DependentVars[2]] = arrOfDataFramesNeeded[1].loc[arrOfDataFramesNeeded[1].index[0].date().isoformat(),independentVars[4]] #<-- Here, NCAV comes from balance sheet. \n",
    "            print(dataFramesToBeShipped[0])\n",
    "            # seriesVersionOn = True <-- This is now a global variable. \n",
    "            # end of Assigning the formulas\n",
    "        print(\"-End of assigning formulas-\")\n",
    "        if(seriesVersionOn):\n",
    "            dataFramesToBeShipped[0].loc[0, \"Company\"] = companyAlias\n",
    "        print(\"--Entering Debuging Mode for Checkpoint #2--\")\n",
    "        # pb.set_trace() #<-- Will use this to check for vals of variable(s), as well as set certain vars to certain values to cause different behavior.   \n",
    "        # CHECKPOINT #2: In proof, at this point, formulas will be assigned to neccessary dependent variables for datafrmae that will be output. \n",
    "        # At this point, the testSet and trainingSet dataframes will be shipped off. \n",
    "        print(dataFramesToBeShipped[0].columns)\n",
    "        print(\"--End of Component a[Subsystem 1]--\")\n",
    "        # resultantSeries = dataFramesToBeShipped[0].loc[0, :]\n",
    "        resultantSeries = dataFramesToBeShipped[0]\n",
    "        # resultantSeries = <-- will reference a row-tuple involving \"P/B\", \"P/E\", \"NCAV\", \"Date For Eval\", \"company\" \n",
    "        print(\"---Debugging checkpoint seeing if P/E key assignment workds for condition #2---\")\n",
    "        # pb.set_trace()\n",
    "        # NOTE: To solve issue, need to cast dates to only referenced years instead!\n",
    "        # Body of casting dates to cause assignment to be successful \n",
    "        conditionForDataFrame2.index = conditionForDataFrame2.index.year\n",
    "        balanceSheet.index = balanceSheet.index.year\n",
    "        # End of Body of casting dates to cause assignment to be successful \n",
    "        # NOTE: To solve issue, need to cast dates to only referenced years instead!\n",
    "        conditionForDataFrame2[\"P/E\"] = conditionForDataFrame2[independentVars[0]]/balanceSheet.loc[:, \"Retained Earnings\"]\n",
    "        # Below returns values that are not null, allowing conditional to work properly. FUTURE NOTE: Once rest of pipeline is verified, then may need to change what the PRICE should be. I think it should be share price opposed to high, but we shall see....\n",
    "        conditionForDataFrame2 = conditionForDataFrame2[conditionForDataFrame2[\"P/E\"].notnull()]\n",
    "        # Proposed solution: conditionForDataFrame2[conditionForDataFrame2[\"P/E\"].notnull()] test out in next session![UPDATE: This works properly!]\n",
    "        \n",
    "        \n",
    "        newVersion = True\n",
    "        return (dataFramesToBeShipped if seriesVersionOn == False else resultantSeries) if newVersion == False else (dataFramesToBeShipped if seriesVersionOn == False else [resultantSeries, conditionForDataFrame2])\n",
    "        # NOTE: compA() is logically correct! Refer to reports for additional steps regarding this!\n",
    "        # end of component a)\n",
    "        # component b) [Populating the new dataframe(s) and preparing dataFrames to be written to files[PN: Will mean that resultant dataframes must be copied to nonlocal variables so subsystem1 can access them(UPDATE: Decided to return the array of these dataframes instead)]\n",
    "        # end of component b)\n",
    "\n",
    "    # NOTE: Below was replaced by adding system to class's constructor!\n",
    "    #compB() <-- THis component was abandoned!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d995cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class subsys2:\n",
    "    def __init__(self,param1 = \"\"\"Insert any params that may be sufficient here!\"\"\"):\n",
    "        print(\"---Subsystem 2 in Progress---\")\n",
    "        # self.compA()\n",
    "    # component a)[creating algorithm for assigning optimality for each company's value]\n",
    "    def compA(self, param1 = \"\"\"Insert any params that may be sufficient here!\"\"\", arrOfCompanies = []): #<-- Not sure if I Need a parameter for this one since I am relying on global variables. AND since its dependent on subsys1. \n",
    "        iterators = len(arrOfCompanies) +1 if len(arrOfCompanies) < 1 else len(arrOfCompanies) #2 #<-- Here, just in case for loop is used to iterate through each company. \n",
    "        # NOTE: Need to add a condition that ensures that dataframe is updated based on companies passed in. \n",
    "        # UPDATE: replacing compA with version where only each company is required. \n",
    "        global dataFrameReffingCompanyData\n",
    "        version0 = False\n",
    "        if version0 == True:\n",
    "            for i in range(iterators):\n",
    "                print(\"--Component a[Subsystem 2] in progress--\")\n",
    "                # Body of executing algorithm for strategy responsible for assigning optimality[NOTE: This only applies to retrievedDataFrame[0] since that'll be the training set one]\n",
    "                # Insert psuedosteps here: .\n",
    "                # predicate wffs for conditions: i) stock(P/B) <= 1.0, ii) stock(P/E) < max(stockPE(P/E,5)), iii) stock(share price) < 0.67*tangible per-share book value[which can be found in historical_data], iv)  (cont here). where stockPE(x,y) = stock's x ratio in the past 5 years and returns z_i, where z_i = x ratio in year i and i \\le y .  [predicate wffs written!]\n",
    "                # NOTE: Share Price == Share Issued and Tangible per-share book value == Tangible Book Value which are both located in the historical_data dataframe. \n",
    "                # Body of creating boolExp Array for respective conditions\n",
    "                arrOfCompanies[i] = param1 #<-- CROSSROADS #1: Here, in the event that I make the iteration take place in the class opposed to main function. \n",
    "                # currCompany = \"MSFT\" or arrOfCompanies[i] #<-- Used default operator here again, thought about replacing None with a param referencing company i.[UPDATE: replaced noe wtih arrOfCompanies[i]] \n",
    "                currCompany = arrOfCompanies[i] \n",
    "                dataFrameWithCompanyAsAColumnId = subsys1.compA(company=currCompany)[0] #<-- Returns the dataframe referencing company data. [IDEA: THink I need to replace dateInQuestion with companyName. The date should still be included but it shouldn't determine the P/B value. It is only relevant when pulling data]\n",
    "                # pb.set_trace()\n",
    "                dataFrameReffingCompanyData = pd.DataFrame() or dataFrameWithCompanyAsAColumnId #<-- REPLACEMENT PENDING: This will be changed to dataframe containing the relevant data. [replacement complete][old version: \"\"\"retrievedDataFrames[0]\"\"\" ][UPDATE: Here, dataFrmaeWithCompanyAsAColumnId refers to the resultant dataframe after assinging rest of columns EXCEPT the optimality]\n",
    "                dataFrameReffingCompanyData = pd.DataFrame() or dataFrameWithCompanyAsAColumnId #<-- REPLACEMENT PENDING: This will be changed to dataframe containing the relevant data. [replacement complete][old version: \"\"\"retrievedDataFrames[0]\"\"\" ][UPDATE: Here, dataFrmaeWithCompanyAsAColumnId refers to the resultant dataframe after assinging rest of columns EXCEPT the optimality]\n",
    "                dateInQuestion: datetime = end_date.fromisoformat(end_date) or \"\" #<-- Using this since vetting process is date-specific. [UPDATE: May not need this variable, but I believe the date should be included as a column with one value?]\n",
    "                yr = 365;\n",
    "                yearsRelToDateInQ: datetime = dateInQuestion - timedelta(days=5*yr)\n",
    "                    # Obtaining stocks highest P/E over previous five years[for assistance, use this search query: `? does ticker object have a start date parameter`]\n",
    "                    # end of Obtaining Stocks highest P/E over previous five years\n",
    "                # arrForCondTwo: pd.DataFrame = arrOfDataFramesNeeded[len(arrOfDataFramesNeeded) - 1] #<-- Did this since I added the dataframe specifically for cond 2 at the END of arrOfDataFramesNeeded. \n",
    "                arrForCondTwo: pd.DataFrame = arrOfDataFramesNeeded[len(arrOfDataFramesNeeded) - 1] #<-- Did this since I added the dataframe specifically for cond 2 at the END of arrOfDataFramesNeeded. [UPDATE: Need to replace this]\n",
    "            #    return #<-- Testing version of return statement[anything above this return statement is sucessful and everything below hasn't been tested yet. THis is relative to each function][for subsys2(compA())][\n",
    "                highestP_EOvrFiveYrs: float = 5 or arrForCondTwo[\"P/E\"].max() #<-- REPLACEMENT PENDING: will be replaced when above is filled in. [UPDATE: Using default operator logic to set val to trusy val if ither operand is undef. Will involve querying dataframe for maximum val in column]\n",
    "                # POTENTIAL IMPROVEMENT PENDING: Need to work on modifying bool exps below by ensuring that they reference the dep var attribute for that PARTICULAR day in which decision must be made. \n",
    "                #highestP_EOvrFiveYrs = \n",
    "                # UPDATE: THe inclusion of ternary operations below allows me to create env for determining optimality based on the day these decisions are being made. \n",
    "                # NOTE: Below will reference command for replacing dateInQuestion with currCompany: \n",
    "                # . `157m a; `.,'as/dateInQ\\w\\+/currCompany/g`  by running this on line of boolExps. [Replaced, in the case where multiply companies are iterated. \n",
    "                boolExps: list[bool] = [\n",
    "                        dataFrameReffingCompanyData[\"P/B\" if currCompany == None else (\"P/B\",currCompany)] <= 1.0,\n",
    "                    dataFrameReffingCompanyData[\"P/E\" if currCompany == None else (\"P/E\",currCompany)] <= 0.4*highestP_EOvrFiveYrs,\n",
    "                    dataFrameReffingCompanyData[\"Share Price\" if currCompany == None else (\"Share Price\",currCompany)] < 0.67*dataFrameReffingCompanyData[\"Tangible Book Value\" if currCompany == None else (\"Tangible Book Value\",currCompany)]\n",
    "                ]\n",
    "                    # End of Body of creating boolExp Array for respective conditions\n",
    "                score = 0\n",
    "                if boolExps[0]:\n",
    "                    score += 1\n",
    "                if boolExps[1]:\n",
    "                    score += 1\n",
    "                if boolExps[2]:\n",
    "                    score += 1\n",
    "                #scoreTable[f\"{currCompany}\"] = score; #<-- Will be used later to assign optimality [UPDATE: May not be needed, need to make sure it is added to optimality col for comapny name[also, will make sense to have companies be the index!]\n",
    "                # dataFrameReffingCompanyData[i, \"company\"] = f\"{currCompany}\"; # NOTE: This is under assumption that this refers to row tuple i and assigns company to that tuple. DEBUGGING OPPORTUNITY #1: May need to set a breakpoint/pdb.set_trace command below this! [UPDATE: This is not needed anymore since subsys1(compA) covers this already!]\n",
    "                dataFrameReffingCompanyData[\"Optimality\"] = score; #<-- CROSSROADS OPPORTUNITY: a) This process works on assumption that each company is iteratively having the optimality algorithm applied to it. \n",
    "                # End of Body of executing algorithm for strategy responsible for assigning optimality\n",
    "               # NOTE: This component will have to take in a company as a parameter. \n",
    "        else:\n",
    "            # Version of code where only company[i] is proprioritezed and a series is returned with optimality there. \n",
    "            print(\"--Component a[Subsystem 2] in progress--[VERSION #2]\")\n",
    "            # Body of executing algorithm for strategy responsible for assigning optimality[NOTE: This only applies to retrievedDataFrame[0] since that'll be the training set one]\n",
    "            # Insert psuedosteps here: .\n",
    "            # predicate wffs for conditions: i) stock(P/B) <= 1.0, ii) stock(P/E) < max(stockPE(P/E,5)), iii) stock(share price) < 0.67*tangible per-share book value[which can be found in historical_data], iv)  (cont here). where stockPE(x,y) = stock's x ratio in the past 5 years and returns z_i, where z_i = x ratio in year i and i \\le y .  [predicate wffs written!]\n",
    "            # NOTE: Share Price == Share Issued and Tangible per-share book value == Tangible Book Value which are both located in the historical_data dataframe. \n",
    "            # Body of creating boolExp Array for respective conditions\n",
    "            # arrOfCompanies[i] = param1 #<-- CROSSROADS #1: Here, in the event that I make the iteration take place in the class opposed to main function. [UPDATE: Decided to have iteration take place in the main function opposed to this function]\n",
    "            # currCompany = \"MSFT\" or arrOfCompanies[i] #<-- Used default operator here again, thought about replacing None with a param referencing company i.[UPDATE: replaced noe wtih arrOfCompanies[i]] \n",
    "            currCompany = param1\n",
    "            # dataFrameWithCompanyAsAColumnId = subsys1.compA(company=currCompany)[0] #<-- Returns the dataframe referencing company data. [IDEA: THink I need to replace dateInQuestion with companyName. The date should still be included but it shouldn't determine the P/B value. It is only relevant when pulling data] <-- UPDATE: THis may not be needed. \n",
    "            # dataFrameReffingCompanyData = pd.Series() or dataFrameWithCompanyAsAColumnId #<-- REPLACEMENT PENDING: This will be changed to dataframe containing the relevant data. [replacement complete][old version: \"\"\"retrievedDataFrames[0]\"\"\" ][UPDATE: Here, dataFrmaeWithCompanyAsAColumnId refers to the resultant dataframe after assinging rest of columns EXCEPT the optimality]\n",
    "            # dataFrameWithCompanyAsAColumnId = subsys1().compA(company=currCompany) #<-- Returns the dataframe referencing company data. [IDEA: THink I need to replace dateInQuestion with companyName. The date should still be included but it shouldn't determine the P/B value. It is only relevant when pulling data] [UPDATE: Trying to have compA(subsys1) return seriess AND the dataframe used for conditions!]\n",
    "            \"\"\"\n",
    "            dataFrameWithCompanyAsAColumnId = subsys1().compA(company=currCompany) #<-- Returns the dataframe referencing company data. [IDEA: THink I need to replace dateInQuestion with companyName. The date should still be included but it shouldn't determine the P/B value. It is only relevant when pulling data]\n",
    "            dataFrameReffingCompanyData = dataFrameWithCompanyAsAColumnId\n",
    "            # Body of version where compA(subsys1) returns TWO things[uncomment below once version is established!] \n",
    "            \"\"\"\n",
    "            # pb.set_trace()\n",
    "            arrOfDataFramesNeeded = subsys1().compA(company=currCompany) \n",
    "            dataFrameWithCompanyAsAColumnId = arrOfDataFramesNeeded[0] #<-- Returns the dataframe referencing company data. [IDEA: THink I need to replace dateInQuestion with companyName. The date should still be included but it shouldn't determine the P/B value. It is only relevant when pulling data]\n",
    "            dataFrameReffingCompanyData = arrOfDataFramesNeeded[1]\n",
    "            arrForCondTwo = arrOfDataFramesNeeded[1]            \n",
    "            \n",
    "            \n",
    "            # Body of version where compA(subsys1) returns TWO things\n",
    "            # pb.set_trace()\n",
    "            dateInQuestion: datetime = datetime.fromisoformat(end_date) or \"\" #<-- Using this since vetting process is date-specific. [UPDATE: May not need this variable, but I believe the date should be included as a column with one value?]\n",
    "            yr = 365;\n",
    "            yearsRelToDateInQ: datetime = dateInQuestion - timedelta(days=5*yr)\n",
    "                # Obtaining stocks highest P/E over previous five years[for assistance, use this search query: `? does ticker object have a start date parameter`]\n",
    "                # end of Obtaining Stocks highest P/E over previous five years\n",
    "            # arrForCondTwo: pd.DataFrame = arrOfDataFramesNeeded[len(arrOfDataFramesNeeded) - 1] #<-- Did this since I added the dataframe specifically for cond 2 at the END of arrOfDataFramesNeeded. \n",
    "            # ticker = yf.Ticker(f\"{currCompany}\") #<-- NOTE: Will need to replace this with multiple tickers later on.            \n",
    "            # arrForCondTwo: pd.DataFrame = ticker.history(period=\"5y\").tz_localize(None)\n",
    "            # arrForCondTwo: pd.DataFrame = [<-- may or may not need to use this again. Mentioned it a few lines above] \n",
    "            #    return #<-- Testing version of return statement[anything above this return statement is sucessful and everything below hasn't been tested yet. THis is relative to each function][for subsys2(compA())][\n",
    "            # highestP_EOvrFiveYrs: float = 5 or arrForCondTwo[\"P/E\"].max() #<-- REPLACEMENT PENDING: will be replaced when above is filled in. [UPDATE: Using default operator logic to set val to trusy val if ither operand is undef. Will involve querying dataframe for maximum val in column]\n",
    "            # print(\"---Debugging Checkpoint: Checking if P/E is a key in arrForCOndTwo\")\n",
    "            # pb.set_trace() <-- NOTE: P/E solution works properly!\n",
    "            highestP_EOvrFiveYrs = arrForCondTwo[\"P/E\"].max() #<-- REPLACEMENT PENDING: will be replaced when above is filled in. [UPDATE: Using default operator logic to set val to trusy val if ither operand is undef. Will involve querying dataframe for maximum val in column]\n",
    "            # POTENTIAL IMPROVEMENT PENDING: Need to work on modifying bool exps below by ensuring that they reference the dep var attribute for that PARTICULAR day in which decision must be made. \n",
    "            #highestP_EOvrFiveYrs = \n",
    "            # UPDATE: THe inclusion of ternary operations below allows me to create env for determining optimality based on the day these decisions are being made. \n",
    "            # NOTE: Below will reference command for replacing dateInQuestion with currCompany: \n",
    "            # . `157m a; `.,'as/dateInQ\\w\\+/currCompany/g`  by running this on line of boolExps. [Replaced, in the case where multiply companies are iterated. \n",
    "            \"\"\" boolExps: list[bool] = [\n",
    "                    dataFrameReffingCompanyData[\"P/B\" if currCompany == None else (\"P/B\",currCompany)] <= 1.0,\n",
    "                dataFrameReffingCompanyData[\"P/E\" if currCompany == None else (\"P/E\",currCompany)] <= 0.4*highestP_EOvrFiveYrs,\n",
    "                dataFrameReffingCompanyData[\"Share Price\" if currCompany == None else (\"Share Price\",currCompany)] < 0.67*dataFrameReffingCompanyData[\"Tangible Book Value\" if currCompany == None else (\"Tangible Book Value\",currCompany)]\n",
    "                ]\"\"\"\n",
    "                # NOTE: No errors occur until up to THIS point. Following error is prod by the boolExps: pandas.errors.IndexingError: Too many indexers\n",
    "            \"\"\"             boolExps: list[bool] = [\n",
    "                    dataFrameReffingCompanyData.loc[\"P/B\" if currCompany == None else (\"P/B\",currCompany)] <= 1.0,\n",
    "                dataFrameReffingCompanyData.loc[\"P/E\" if currCompany == None else (\"P/E\",currCompany)] <= 0.4*highestP_EOvrFiveYrs,\n",
    "                dataFrameReffingCompanyData.loc[\"Share Price\" if currCompany == None else (\"Share Price\",currCompany)] < 0.67*dataFrameReffingCompanyData[\"Tangible Book Value\" if currCompany == None else (\"Tangible Book Value\",currCompany)]\n",
    "            ]\n",
    "            \"\"\" \n",
    "            print(\"---DEBUGGING SUBCHECKPOINT: CHecking for the Boolean Expressions---\")\n",
    "            # pb.set_trace()\n",
    "            # NOTE: dataFrameReffingCompanyData is a pd.Series so the querying used Above is NOT needed!\n",
    "            # dataFrameThatRefsSharePrice = pd.DataFrame() #<-- This variable is self explanatory. Need to modify soon. \n",
    "            dataFrameThatRefsSharePrice = dataFrameWithCompanyAsAColumnId #<-- This variable is self explanatory. Need to modify soon. \n",
    "            \"\"\" boolExps: list[bool] = [\n",
    "                    dataFrameReffingCompanyData[\"P/B\"] <= 1.0,\n",
    "                dataFrameReffingCompanyData[\"P/E\"] <= 0.4*highestP_EOvrFiveYrs,\n",
    "                dataFrameReffingCompanyData[\"Share Price\"] < 0.67*dataFrameThatRefsSharePrice[\"Tangible Book Value\"]\n",
    "            ] \"\"\"\n",
    "            \"\"\" boolExps: list[bool] = [\n",
    "                    dataFrameWithCompanyAsAColumnId[\"P/B\"] <= 1.0,\n",
    "                dataFrameWithCompanyAsAColumnId[\"P/E\"] <= 0.4*highestP_EOvrFiveYrs,\n",
    "                dataFrameWithCompanyAsAColumnId[\"Share Price\"] < 0.67*dataFrameThatRefsSharePrice[\"Tangible Book Value\"]\n",
    "            ] \"\"\"\n",
    "            boolExps: list[bool] = [\n",
    "                    dataFrameWithCompanyAsAColumnId[\"P/B\"] <= 1.0,\n",
    "                dataFrameWithCompanyAsAColumnId[\"P/E\"] <= 0.4*highestP_EOvrFiveYrs,\n",
    "                dataFrameWithCompanyAsAColumnId[\"Share Price\"] < 0.67*(dataFrameWithCompanyAsAColumnId[\"Share Price\"]/dataFrameThatRefsSharePrice[\"P/B\"])\n",
    "            ]\n",
    "            # UPDATE: Problem originates from dataFrameThatRefsSharePrice[\"Tangible Book Value\"][UPDATE: Problem fixed!]\n",
    "                # End of Body of creating boolExp Array for respective conditions\n",
    "            \"\"\"             \n",
    "            if boolExps[0]:\n",
    "                score += 1\n",
    "            if boolExps[1]:\n",
    "                score += 1\n",
    "            if boolExps[2]:\n",
    "                score += 1\n",
    "            \"\"\"            \n",
    "            # `N`OTE: May need to change some things in future...NOT sure[particularly, involving ][NOTE: Will need to work on validating boolean exps since all optimality cols are equal to 0]\n",
    "            score = 0\n",
    "            if boolExps[0].any():\n",
    "                score += 1\n",
    "            if boolExps[1].any():\n",
    "                score += 1\n",
    "            if boolExps[2].any():\n",
    "                score += 1\n",
    "            #scoreTable[f\"{currCompany}\"] = score; #<-- Will be used later to assign optimality [UPDATE: May not be needed, need to make sure it is added to optimality col for comapny name[also, will make sense to have companies be the index!]\n",
    "            # dataFrameReffingCompanyData[i, \"company\"] = f\"{currCompany}\"; # NOTE: This is under assumption that this refers to row tuple i and assigns company to that tuple. DEBUGGING OPPORTUNITY #1: May need to set a breakpoint/pdb.set_trace command below this![UPDATE as of 11/21/25: May NOT need this at all!]\n",
    "            # print(\"---Testing if the optimality actually works[at least the assignments]---\")\n",
    "            # pb.set_trace()\n",
    "            # dataFrameReffingCompanyData[f\"{currCompany}\", \"Optimality\"] = score; #<-- CROSSROADS OPPORTUNITY: a) This process works on assumption that each company is iteratively having the optimality algorithm applied to it. [UPDATE: Not needed anymore since series are being returned opposed to entire dataframe] \n",
    "            # Below's verison references the pd.Series() so .loc isn't needed here. \n",
    "            dataFrameWithCompanyAsAColumnId[\"Optimality\"] = score; # UPDATE: Works as intended. Focu sshould shift back towards ensuring that Share Price thing works properly. \n",
    "            # pb.set_trace()\n",
    "            # End of Body of executing algorithm for strategy responsible for assigning optimality\n",
    "            # NOTE[EXTREMELY IMPORTANT]: Succeeding this point, another component will be responsible for combining the resultant dataframes, ordering the companies by optimality, and then assigning ranks to the companies, and then finishing off my adding the rank numbers to the resultant dataframe. \n",
    "            # NOTE: This component will have to take in a company as a parameter. \n",
    "\n",
    "            print(\"--End of Component a[Subsystem 2] in progress--\")\n",
    "            return dataFrameWithCompanyAsAColumnId\n",
    "# end of component a)\n",
    "    # component b)\n",
    "    def compB(): # NOTE: I believe compB(subsystem2) is complete! \n",
    "        print(\"--Component b[Subsystem 2] in progress--\")\n",
    "        global retreivedDataFrames \n",
    "        #retreivedDataFrames = dataFramesReffingCompanyData or pd.DataFrame()#compA() #<-- Will be replaced soon[REPLACEMENT PENDING][UPDATE: Replaced with global var referencign resultant dataframe(s) with optimality columns]. \n",
    "        retreivedDataFrames = [dataFrameReffingCompanyData,dataFrameReffingCompanyData] or pd.DataFrame()#compA() #<-- Will be replaced soon[REPLACEMENT PENDING][UPDATE: Replaced with global var referencign resultant dataframe(s) with optimality columns]. [UPDATE: Used an array of two insts of Company Data, since one of them will be used for trainingSet and testSet respectively(assuming it isn't done in Model Dev file)]\n",
    "        # Body of writing resp dataframes to files[need to use to_csv I believe]\n",
    "        filePathToTrainingSetDir: str = \"MLLifecycle/ModelDevelopment/TrainingSets\" #<-- fill in later[complete]\n",
    "        filePathToTestSetDir: str = \"MLLifecycle/ModelDevelopment/TestingSets\" #<-- fill in later[complete]\n",
    "        print(\"--End of Component b[Subsystem 2]--\")\n",
    "        print(\"Entering Debuging Mode for Checkpoint #3\")\n",
    "        # pb.set_trace() #<-- Will use this to check for vals of variable(s), as well as set certain vars to certain values to cause different behavior.   \n",
    "        # CHECKPOINT #3: In proof, at this point, dataframes will be written to the neccessary files to be ingested by the Model.  \n",
    "        print(\"--Writing Dataframes for Model Ingestion--\")\n",
    "        retreivedDataFrames[0].to_csv(f'{filePathToTrainingSetDir}/trainingSet{setNum if setNum != None else 1}')\n",
    "        retreivedDataFrames[1].to_csv(f'{filePathToTrainingSetDir}/testSet{setNum if setNum != None else 1}')\n",
    "        # End of Body of writing resp dataframes to files\n",
    "        print(\"--End of Writing Dataframes for Model Ingestion--\")\n",
    "        print(\"--End of Component b[Subsystem 2]--\")\n",
    "        return #<-- Testing version of return statement[anything above this return statement is sucessful and everything below hasn't been tested yet. THis is relative to each function]\n",
    "    # end of component b)\n",
    "\n",
    "    # NOTE: Below was replaced by adding system to class's constructor!\n",
    "    #return\n",
    "    #compA() \n",
    "    #compB()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346e7aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"---Starting Data Prep Process---\")\n",
    "\n",
    "    companies: list[str] = [\"GOOG\",\"AAPL\", \"AMZN\", \"MSFT\"]\n",
    "    \n",
    "    listOfSeriesToCreateDataFrame = []\n",
    "    for i in range(len(companies)):\n",
    "        print(f\"----Adding company {companies[i]} to engineered dataset----\")\n",
    "        # \"\"\"\n",
    "        # NOTE: Will uncomment, once everything with the functions used here is situated[add a checklist here: ]\n",
    "        print(\"---Starting Subsystem 1---\")\n",
    "        # pb.set_trace(); #<-- Adding breakpoint here to see what happens. \n",
    "        # Call function referencing topmost subsystem #1 here: \n",
    "        a: str = \"\"; b: str = \"\"\n",
    "        # subsys1(param1=company_i)\n",
    "        # subsys1().compA(company=company_i)\n",
    "        # resultantDataFrame = pd.concat([resultantDataFrame, subsys1().compA(company=companies[i])])\n",
    "        # resultantDataFrame.loc[i, [\"P/B\", \"P/E\", \"NCAV\", \"Company\"]] = subsys1().compA(company=companies[i])\n",
    "        subsys1().compA(company=companies[i])\n",
    "        # companySeries= subsys1().compA(company=companies[i])\n",
    "        # pb.set_trace()\n",
    "        # subsys2().compA(param1=companySeries)\n",
    "        subsys2().compA(param1=companies[i])\n",
    "        retSeries = True\n",
    "        if retSeries:\n",
    "            # ^^ NOTE: Above is returning a series each time[at least this is the assumption]\n",
    "            companySeries = subsys2().compA(param1=companies[i])\n",
    "            listOfSeriesToCreateDataFrame.append(companySeries)\n",
    "        else:\n",
    "            # ^^ NOTE: Above is NOT retruning a series\n",
    "            print(\"---Assumption that series is NOT returned---\")\n",
    "            # NOTE: Not sure what to put here. Will default to returning series from compA(subsys2)\n",
    "            # companySeries = subsys2().compA(param1=companies[i])\n",
    "            # listOfSeriesToCreateDataFrame.append(companySeries)\n",
    "            \n",
    "        # resultantDataFrame.loc[i, :] = subsys1().compA(company=companies[i]) #<-- This line is still causing problems. \n",
    "        # Will replace above with this: subsys1(company_i)\n",
    "        \n",
    "        print(\"---End of Subsystem 1---\")\n",
    "        \n",
    "        \"\"\"\n",
    "        print(\"---Starting Subsystem 2---\")\n",
    "        # Call function referencing topmost subsystem #2 here: \n",
    "        # subsys2(b)\n",
    "        # subsys2.compA(b) #<-- Here, need to ensure that \n",
    "        # Will replace above with this: subsys2(company_i)\n",
    "        print(\"---End of Subsystem 2---\")\n",
    "        \"\"\"\n",
    "        print(f\"----End of Adding company {companies[i]} to engineered dataset----\")\n",
    "\n",
    "    pb.set_trace()\n",
    "    # resultantDataFrame = pd.DataFrame(listOfSeriesToCreateDataFrame) #<-- Causing following error: *** ValueError: Must pass 2-d input. shape=(1, 1, 6)[need to figure out how to resolve error][UPDATE: Error fixed! Imp below resolved issue. NOW: Main focus goes back to setting up optimality!]\n",
    "    resultantDataFrame = pd.concat([pd.DataFrame(x) for x in listOfSeriesToCreateDataFrame]).reset_index() #<-- used list comprehension to transform listOfSeries to resultantDataFrame. \n",
    "    del resultantDataFrame['index']\n",
    "    print(resultantDataFrame) #<-- THis dataframe will reference the dataframe that adheres to the follwowing object: company(CompanyName, \"P/B\", \"P/E\", \"NCAV\", \"Date For Eval\", \"Optimality\", (cont here if applicable))[NOTE: Will be wise to make a Entity via ERDs for documentation when writing paper at end]\n",
    "    inNoteBook = False\n",
    "    filePathToModelDir = \"C:/Users/adoct/Notes for CSCE Classes[Fall 2025]/Notes for CSCE 585/ProjectRepo/projectCode/MLLifecycle/ModelDevelopment/preparedDataset.csv\" if inNoteBook == False else \"preparedDataset.csv\"\n",
    "    # Body of handling edge case where all of them are same optimality\n",
    "\n",
    "    resultantDataFrame.to_csv(f\"{filePathToModelDir}\")\n",
    "    if((resultantDataFrame[\"Optimality\"] == 0).all() == True):\n",
    "        # Setting optimality column to be based on alphabetical ordering \n",
    "        print(\"---DEBUGGING CHECKPOINT #3: Validating process of creating resultant dataframe and sorting columns and assigning optimality, iff all stocks chosen all have same optimality---\")\n",
    "        pb.set_trace()\n",
    "        # UPDATE: Below works as expected!\n",
    "        resultantDataFrame.sort_values(by='Company',inplace=True)\n",
    "        resultantDataFrame = resultantDataFrame.set_index(np.arange(4))\n",
    "        resultantDataFrame.loc[:,\"Optimality\"] = pd.Series(np.arange(resultantDataFrame[\"Optimality\"].shape[0]))\n",
    "\n",
    "        # End of Setting optimality column to be based on alphabetical ordering \n",
    "        resultantDataFrame.to_csv(f\"{filePathToModelDir}\")\n",
    "    \n",
    "     \n",
    "    \n",
    "    \n",
    "    # End of Body of handling edge case where all of them are same optimality\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# End of portion referencing data engineering procedure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8ce9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: As of 11/30/25, MODEL DEV AND TRAINING IS COMPELTE! [Need to partition below as well. Ensure that respective plots come out one at a time AT LEAST!] \n",
    "# NOTE: In regards to testing out program, use following command. Refer to its comment for reference: $ find ProjectCode/components ProjectCode/routes -name \"*.py\" | xargs --delimiter=\"\\n\" grep -E \"^(import|from)\" #<-- NOTE: Use this code to find the files to: a) determine neccessary downloads for pip, and b) determine where to call model and send output etc from. \n",
    "\n",
    "# Purpose: This file will contain code that helps with Model Development. \n",
    "\n",
    "# Body of neccessary imports\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np #<-- May be optional not sure as of 10/13/25. \n",
    "import pdb as pb\n",
    "# import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Input\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "\n",
    "# end of body of neccessary imports\n",
    "\n",
    "# Important Steps in Model Development: 1) Obtaining the training data, 2) Create the model containing initialized weights and a bais[which would be very involved with using numbers from certain columns], 3) Observe model's performance before training, 4) Defining a loss function for model, 5) Write a basic Training Loop\n",
    "def attempt1():\n",
    "    # Section 1\n",
    "    # Purpose of section: 1) Obtaining the training data\n",
    "\n",
    "    # end of purpose of section\n",
    "\n",
    "    # Psuedosteps for exercising purpose of section\n",
    "    # - Need to make sure data is clean\n",
    "    # - Need to have an object that references clean dataset\n",
    "    # - Training data \n",
    "    # end of psuedosteps for exercising purpose of section \n",
    "\n",
    "    # body of section 1\n",
    "\n",
    "\n",
    "\n",
    "    # end of body of section 1\n",
    "    # End of Section 1\n",
    "\n",
    "\n",
    "    # Section 2\n",
    "    # Purpose of section: 2) Create the model containing initialized weights and bias\n",
    "\n",
    "    # end of purpose of section\n",
    "\n",
    "    # Psuedosteps for exercising purpose of section\n",
    "\n",
    "    # end of psuedosteps for exercising purpose of section \n",
    "\n",
    "    # body of section 2\n",
    "    # NOTE: Code is subject to change, just acting as starting point right now.\n",
    "    class Model(tf.Module):\n",
    "        def __init__(self):\n",
    "            # \"Randomly genearte weight and bias terms\"\n",
    "            rand_init = tf.random.uniform(shape=[3], minval=0., maxval=5., seed = 22)\n",
    "            # \"Initialize Model Parameters\"\n",
    "            self.w_q = tf.Variable(rand_init[0])\n",
    "            self.w_l = tf.Variable(rand_init[1])\n",
    "            self.b = tf.Variable(rand_init[2])\n",
    "        @tf.function\n",
    "        def __call__(self,x):\n",
    "            # \"Quadratic Model: quadratic_weight * x^2 + linear_weight*x + bias\"\n",
    "            return self.w_q * (x**2) + self.w_l * x + self.b\n",
    "\n",
    "    # end of body of section 2\n",
    "\n",
    "    # End of Section 2\n",
    "\n",
    "\n",
    "    # Section 3\n",
    "    # Purpose of section:  3) Observe model's performance before training\n",
    "\n",
    "    # end of purpose of section\n",
    "\n",
    "    # Psuedosteps for exercising purpose of section\n",
    "\n",
    "    # end of psuedosteps for exercising purpose of section \n",
    "\n",
    "    # body of section 3\n",
    "\n",
    "    # NOTE: Code is subject to change, just acting as starting point right now.\n",
    "    quad_model = Model() #<-- instantiation of model\n",
    "    def plot_preds(x,y,f,model,title):\n",
    "       plt.figure() \n",
    "       plt.plot(x,y, '.', label='Data')\n",
    "       plt.plot(x,f(x), label='Ground Truth')\n",
    "       plt.plot(x,model(x), label='Predictions')\n",
    "       plt.title(title)\n",
    "       plt.legend()\n",
    "\n",
    "    plot_preds(x,y,f,quad_model,'Before Tranining')\n",
    "\n",
    "\n",
    "\n",
    "    # End of body of section 3\n",
    "    # End of Section 3\n",
    "\n",
    "\n",
    "    # Section 4\n",
    "    # Purpose of section: 4) Defining the loss function\n",
    "\n",
    "    # end of purpose of section\n",
    "\n",
    "    # Psuedosteps for exercising purpose of section\n",
    "\n",
    "    # end of psuedosteps for exercising purpose of section \n",
    "\n",
    "    # End of Section 4\n",
    "\n",
    "\n",
    "    # Section 5\n",
    "    # Purpose of section: 5) Write a basic Training Loop\n",
    "\n",
    "    # end of purpose of section\n",
    "\n",
    "    # Psuedosteps for exercising purpose of section\n",
    "\n",
    "    # end of psuedosteps for exercising purpose of section \n",
    "\n",
    "    # Body of Section 5\n",
    "    batch_size = 32\n",
    "    # NOTE: This may be a good reference for sending in clean training data for model to use. \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "    dataset = dataset.shuffle(buffer_size=x.shape[0].batch(batch_size))\n",
    "\n",
    "    # a) \"Set Training Parameters\"\n",
    "    epochs = 100\n",
    "    learning_rate = 1e-2\n",
    "    losses= []\n",
    "\n",
    "    # b) \"Format Training Loop\"\n",
    "    for epoch in range(epochs):\n",
    "        for x_batch, y_batch in dataset:\n",
    "            with tf.GradientTape() as tape:\n",
    "                batch_loss = mse_loss(quad_model(x_batch),y_batch) \n",
    "            # 1) \"Update parameters with respect to the gradient calculations\"\n",
    "            grads = tape.gradient(batch_loss,quad_model.variables)\n",
    "            for g,v in zip(grads, quad_model.variables):\n",
    "                v.assign_sub(learning_rate*g)\n",
    "        # 2) \"Keep track of model loss per epoch\"\n",
    "        loss = mse_loss(quad_model(x), y)\n",
    "        losses.append(loss)\n",
    "        if epoch %10 == 0:\n",
    "            print(f'Mean squared error for step {epoch}: {loss.numpy():0.3f}')\n",
    "\n",
    "    # c) \"Plot Model Results\"\n",
    "    print(\"\\n\")\n",
    "    plt.plot(range(epochs), losses)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Mean Squared Error (MSE)\")\n",
    "    plt.title(\"MSE loss vs Training Iterations\");\n",
    "\n",
    "    plt.show()\n",
    "    # Now, observe your model's performacne after training: \n",
    "    plot_preds(x,y,f,quad_model, 'After Training')\n",
    "    # End of Body of Section 5\n",
    "    # End of Section 5\n",
    "\n",
    "\n",
    "    ## NOTE: Below references an alternative approach. This ranges from getting data to providing statistcis for model eval. Does not include data visualization portion. \n",
    "\"\"\"\n",
    "# Attempt 2\n",
    "# Modificaitons needed: a) Need to find place where to put modified dataset, b) Need to ensure that a training data set is created from the dataset(s) that we have[need to make sure an additional row or column is provided for the classification], (cont here if applicable) \n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "\n",
    "# \"Helper Libraries\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "# 1) Import Dataset\n",
    "#fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "#(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "filepathToTrainingSet: str\n",
    "filepathToTestSet: str\n",
    "filePathToTrainingSet += f\"/testSetStrat{1}.py\" #<-- number will change based on strat being used.\n",
    "train_stocks = pd.read_csv(f\"{filepathToTrainingSet}); train_labels = test_labels = [\"will reference company names\"];\n",
    "test_stocks = pd.read_csv(f\"{filepathToTrainingSet}); test_labels = [\"will reference company names\"];\n",
    "\n",
    "# end of 1)\n",
    "# 2) Provide class names for ML Model to make predictions\n",
    "#class_names = ['T-shirt/top', 'Trourser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot']\n",
    " class_names = ['Apple', 'Google', 'Amazon']\n",
    "\n",
    "# end of 2)\n",
    "\n",
    "# 3) Exploring the data\n",
    "len(train_labels)\n",
    "\n",
    "test_images.shape\n",
    "\n",
    "\n",
    "len(test_labels)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(train_images[0])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i],cmap=plt.cm.binary)\n",
    "    plt.xlabel(class_names[train_labels[i]])\n",
    "\n",
    "#plt.show()\n",
    "# end of 3)\n",
    "# 4) Creating the Model[can be created using keras OR created manually by inherting the tf.Module object][aka the Neural Network][UPDATE: Think it'd make sense to create aa FNN first using things below]\n",
    "model = tf.keras.Sequential([\n",
    "tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "tf.keras.layers.Dense(128,activation='relu'),\n",
    "tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "metrics=['accuracy']) #<-- NOTE: Metrics here, can probably be modified[UPDATE: No need, accuracy is what is important right now]\n",
    "\n",
    "# end of 4)\n",
    "# 5) Evaluating the Model\n",
    "model.fit(train_images, train_labels, epochs=10)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels,verbose=2)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)\n",
    "\n",
    "\n",
    "probability_model = tf.keras.Sequential([model, \n",
    "tf.keras.layers.Softmax()])\n",
    "\n",
    "# a) Obtaining the accuarcy of the predictions\n",
    "predictions = probability_model.predict(test_images)\n",
    "\n",
    "predictions[0]\n",
    "\n",
    "\n",
    "np.argmax(predictions[0]) #<-- Returns the prediction that Neural Network was most comfortable with. \n",
    "\n",
    "# 6) Verifying and Visualzing the Predictions\n",
    "def plot_image(i, predictions_array, true_label, img):\n",
    "    true_label, img = true_label[i], img[i]\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(img, cmap=plt.cm.binary)\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "\n",
    "    if predicted_label == true_label:\n",
    "        color = 'blue'\n",
    "    else:\n",
    "        color = 'blue'\n",
    "    \n",
    "    plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label], 100*np.max(predicitons_array), class_names[true_label], color=color)\n",
    "\n",
    "def plot_image(i, predictions_array, true_label):\n",
    "    true_label = true_label[i]\n",
    "    plt.grid(False)\n",
    "    plt.xticks(range(10))\n",
    "    plt.yticks([])\n",
    "    thisplot = plt.bar(range(10), predictions_array, color=\"#7777777\")\n",
    "    plt.ylim([0,1])\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "\n",
    "    thisplot[predicted_label].set_color('red')\n",
    "    thisplot[true_label].set_color('blue')\n",
    "\n",
    "i = 0\n",
    "plt.figure(figsize(6,3))\n",
    "plt.subplot(1,2,1)\n",
    "plot_image(i,predictions[i],test_labels,test_images)\n",
    "plt.subplot(1,2,2)\n",
    "plot_image(i,predictions[i],test_labels)\n",
    "plt.show()\n",
    "\n",
    "# end of 6)\n",
    "\n",
    "\"\"\"\n",
    "def attempt3(): #<-- NOTE: This attempt is what will be used for Model Portion. \n",
    "    inNoteBook = False\n",
    "    \n",
    "    import numpy as np\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "    from tensorflow.keras.utils import to_categorical, plot_model\n",
    "    from tensorflow.keras.datasets import mnist\n",
    "    # NOTE: Make sure to NOT worry about imports. \n",
    "    # NOTE: Below is used to modify network parameters using experiment setup func(s). \n",
    "    global x_train; global x_test\n",
    "\n",
    "    # \"load mnist dataset\"\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    # filePathToTrainingSetDir: str = \"MLLifecycle/ModelDevelopment/TrainingSets\" #<-- fill in later[complete] UPDATE as of 11/10/25: Replaced since I am using script.py for running pipeline. \n",
    "    \"\"\"     \n",
    "filePathToTrainingSetDir: str = \"./ModelDevelopment/TrainingSets\" #<-- fill in later[complete]\n",
    "    filepathToTrainingSet: str = filePathToTrainingSetDir\n",
    "    filePathToTrainingSet += f\"/trainingSetStrat{1}.py\" #<-- number will change based on strat being used.\n",
    "    filePathToTestSetDir: str = \"MLLifecycle/ModelDevelopment/TestingSets\" #<-- fill in later[complete]\n",
    "    filepathToTestSet: str = filePathToTestSetDir\n",
    "    filePathToTestSet += f\"/testSetStrat{1}.py\" #<-- number will change based on strat being used.\n",
    "    \"\"\"    \n",
    "    filePathToModelDir = \"C:/Users/adoct/Notes for CSCE Classes[Fall 2025]/Notes for CSCE 585/ProjectRepo/projectCode/MLLifecycle/ModelDevelopment/preparedDataset.csv\" if inNoteBook == False else \"preparedDataset.csv\"\n",
    "    # print(\"---DEBUGGING CHECKPOINT #1: Ensuring that train_stocks & train_labels reference the right things---\")\n",
    "    # pb.set_trace() [COMPLETE]\n",
    "    # train_stocks = pd.read_csv(f\"{filePathToModelDir}\"); train_labels = test_labels = train_stocks.loc[\"Company\"] or [\"will reference company names\"];#<-- References labels which are derived from custom engineered dataset. \n",
    "    train_stocks = pd.read_csv(f\"{filePathToModelDir}\"); train_labels = test_labels = train_stocks.loc[:,\"Company\"] #<-- References labels which are derived from custom engineered dataset. \n",
    "    del train_stocks[\"Unnamed: 0\"]\n",
    "    test_stocks = pd.read_csv(f\"{filePathToModelDir}\"); test_labels = train_stocks.loc[:,\"Company\"] # or [\"will reference company names\"]; #<-- References labels which are derived from custom engineered dataset. \n",
    "    del test_stocks[\"Unnamed: 0\"]\n",
    "    x_train = train_stocks.loc[:,train_stocks.columns != \"Optimality\"]\n",
    "    y_train = train_labels\n",
    "    x_test = test_stocks.loc[:,train_stocks.columns != \"Optimality\"]\n",
    "    y_test = test_labels\n",
    "    # ^^ Above ensures that prediction labels are y and x references the data used to make said decision. [UPDATE: Due to this, will need to make a change in data prep folder]\n",
    "            \n",
    "\n",
    "    # \"Compute the number of labels\"\n",
    "    num_labels = len(np.unique(y_train))\n",
    "\n",
    "    # \"Convert to one-hot vector\"[we converted the labels to one-hot vectors using to_categorical]\n",
    "\n",
    "    # resultantDataFrame = pd.concat([pd.DataFrame(x) for x in listOfSeriesToCreateDataFrame]).reset_index() #<-- used list comprehension to transform listOfSeries to resultantDataFrame. \n",
    "    # print(\"---DEBUGGING CHECKPOINT #1: Ensuring that train_stocks & train_labels reference the right things---\")\n",
    "    # pb.set_trace() [COMPLETE!]\n",
    "    # y_train = to_categorical([range(y_train.shape[0]) for x in y_train])\n",
    "    # y_test = to_categorical([ range(y_test.shape[0]) for x in y_test])\n",
    "    y_train = to_categorical(y_train.index)\n",
    "    y_test = to_categorical(y_test.index)\n",
    "    \"\"\"\n",
    "    NOTE: Below is NOT needed\n",
    "    listOfHotEncodings = list(range(y_train.shape[0]))\n",
    "    # y_train = to_categorical([ dict[x] for x in y_train])\n",
    "    for i in range(y_train.shape[0]):\n",
    "        # listOfHotEncodings[i] = 2**(y_train.shape[0]) - 2**i #<-- Uses one hot encoding by using decimal value that refs binary value. 1 = 0001, 2 = 0010, 4 = 0100, 8 = 1000 and so forth. \n",
    "        listOfHotEncodings[i] = 2**i #<-- Uses one hot encoding by using decimal value that refs binary value. 1 = 0001, 2 = 0010, 4 = 0100, 8 = 1000 and so forth. \n",
    "\n",
    "    y_train = listOfHotEncodings\n",
    "    # y_test = to_categorical([ range(y_test.shape[0]) for x in y_test])\n",
    "    for i in range(y_test.shape[0]):\n",
    "        # listOfHotEncodings[i] = 2**(y_test.shape[0]) - 2**i #<-- Uses one hot encoding by using decimal value that refs binary value. 1 = 0001, 2 = 0010, 4 = 0100, 8 = 1000 and so forth. \n",
    "        listOfHotEncodings[i] = 2**i #<-- Uses one hot encoding by using decimal value that refs binary value. 1 = 0001, 2 = 0010, 4 = 0100, 8 = 1000 and so forth. \n",
    "    y_test = listOfHotEncodings\n",
    "    \"\"\"\n",
    "    \n",
    "    # NOTE: Currently here as far as debugging. [ALSO, whilst working make sure that you know that: NOTE: Need to find a dataset that references ranked optimality of stocks based on value investing strategy to use as a benchmark for model results. ]\n",
    "\n",
    "    # ----May snippet below may be optional?---\n",
    "    # \"image dimensions(assumed square)\"\n",
    "    # image_size = x_train.shape[1]\n",
    "    # input_size = image_size * image_size\n",
    "    input_size = len(train_stocks.columns) # #<-- UPDATE: input_size is NOT optional. Set this assuming that input_dim is number of dims/attributes for each feature which refers to number of columns.  However, I believe this is \n",
    "\n",
    "    # \"Resize and Normalize\"\n",
    "    \"\"\" \n",
    "    x_train = np.reshape(x_train, [-1,input_size])\n",
    "    x_train = x_train.astype('float32')/255\n",
    "    x_test = np.reshape(x_test, [-1,input_size])\n",
    "    x_test = x_test.astype('float32')/255\n",
    "    \"\"\"\n",
    "    # ----May snippet above may be optional?[UPDATE: Above is required, need to come up with normalization process]---\n",
    "    # \"Network Parameters\"\n",
    "    # NOTE: The following will reference a list of tuple(s) that will be used to facilitate first experiment. \n",
    "    listOfExperimentSetupFuncs = [\"\"\"Plan to insert functions for doing experiment(s) setup here\"\"\"]\n",
    "    global batch_size, hidden_units, dropout\n",
    "    global model #<-- Have this here, so experimental setup functions can tweak model as needed. \n",
    "    batch_size = 128\n",
    "    # batch_size = experiment1Tuples[0][0] \n",
    "    hidden_units = 256\n",
    "    # hidden_units = experiment1Tuples[0][1]\n",
    "    dropout = 0.45\n",
    "    # dropout  = experiment1Tuples[0][2]\n",
    "    isExperimentSetup3Active = False\n",
    "    # model = Sequential()\n",
    "    model = Sequential()\n",
    "    def experimentSetup0():\n",
    "       # This will reference the default settings irrespective to experiments. \n",
    "        print(\"---Undergoing Experiment Setup #0---\")\n",
    "        # model.add(Dense(hidden_units,input_dim=input_size))\n",
    "        # model.add(Dense(hidden_units,input_shape=(input_size-1,)))\n",
    "        # print(\"---DEBUGGING CHECKPOINT: Checking model's reactions---\")\n",
    "        # pb.set_trace()\n",
    "        # model.add(Input((input_size,)))\n",
    "        model.add(Input((x_train.shape[1],)))\n",
    "        # model.add(Dense(hidden_units,input_shape=(input_size,)))\n",
    "        model.add(Dense(hidden_units))\n",
    "        # NOTE: Above is causing following error: \"ValueError: Exception encountered when calling Sequential.call(). Invalid input shape for input Tensor(\"data:0\", shape=(5,), dtype=float32). Expected shape (None, 6), but input has incompatible shape (5,)\" [UPDATE: Error is originating from fact that x_trainCopy and x_testCopy 's shapes are (5,) and (5,)]\n",
    "        model.add(Activation('relu')) #<-- This is used to add activation function to model[UPDATE: May need to replace Activation('relu') by making them default...not sure to facilitate experimentSetup3 OR I can simply see what happens when the 2nd to LAST acivation is changed] \n",
    "        model.add(Dropout(dropout))\n",
    "        model.add(Dense(hidden_units))\n",
    "        model.add(Activation('relu')) if experimentSetup3() == None else experimentSetup3() #<-- This changes the Activation function, adhering to experimentSetup0![Thus, as of 11/23/25: There are two experiments ready to go for Milestone 1]\n",
    "        model.add(Dropout(dropout))\n",
    "        model.add(Dense(num_labels))\n",
    "        print(\"---End of Experiment Setup #0---\")\n",
    "        return\n",
    "\n",
    "    def experimentSetup1():\n",
    "        # Goal of exp: Want to see how model params affect the acuaracy of the model by modifying batch size and hidden units and dropout[pn: make sure to have a subset of the powerset of params be modified in this exp. ]\n",
    "        print(\"---Undergoing Experiment Setup #1---\")\n",
    "        setN = 0 #<-- Change number for this to get values from resp sets. \n",
    "        experiment1Tuples: list[tuple] = [(128,256,0.45), (64,128,0.45), (\"\"\"NOTE: Other tuples can change one or more parameters whilst keeping at least one constant\"\"\")]\n",
    "        batch_size = experiment1Tuples[setN][0] \n",
    "        hidden_units = experiment1Tuples[setN][1]\n",
    "        dropout = experiment1Tuples[setN][2]\n",
    "        \n",
    "        print(\"---End of Experiment Setup #1---\")\n",
    "        return\n",
    "    def experimentSetup2(numQuantLvls = 2):\n",
    "        # Goal of exp: Want to see model performance based on degree of quantanization of data\n",
    "        print(\"---Undergoing Experiment Setup #2---\")\n",
    "        # NOTE: Below will involve replacing 255 with a different number based on degree of quantanization of data. \n",
    "        # UPDATE: Only thing left is pulling maximum value from x_train and min value from train and test respectively. \n",
    "        widthsOfQuant = []\n",
    "        \n",
    "        widthsOfQuant[0] = (x_trainMax - x_trainMin)/(numQuantLvls - 1)\n",
    "        x_train = np.reshape(x_train, [-1,input_size])\n",
    "        x_trainMax = 0;\n",
    "        x_trainMin = 0;\n",
    "        \n",
    "        x_test = np.reshape(x_test, [-1,input_size])\n",
    "        x_testMax = 0;\n",
    "        x_testMin = 0;\n",
    "        widthsOfQuant[1] = (x_testMax - x_testMin)/(numQuantLvls - 1); \n",
    "        x_train = x_train.astype('float32')/widthsOfQuant[0]\n",
    "        x_test = x_test.astype('float32')//widthsOfQuant[1]\n",
    "        print(\"---End of Experiment Setup #2---\")\n",
    "        return\n",
    "\n",
    "    def experimentSetup3():\n",
    "        # Goal of exp: Want to see model performance based on type of activation function from a subset of all possible activation functions. \n",
    "        print(\"---Undergoing Experiment Setup #3---\" if isExperimentSetup3Active else \"---Experiment Setup #3 was skipped---\")\n",
    "        activationFuncs = ['elu', 'sigmoid', 'tanh' ]\n",
    "        # model.add(Activation(activationFuncs[0])) #<-- May need this since I have a classification problem that I'm wokring on. [UPDATE: Made this a return value instead] \n",
    "\n",
    "        print(\"---End of Experiment Setup #3---\" if isExperimentSetup3Active else \"---End of Experiment Setup #3 was skipped---\")\n",
    "\n",
    "        return model.add(Activation(activationFuncs[0])) if isExperimentSetup3Active == True else None #<-- May need this since I have a classification problem that I'm wokring on. \n",
    "\n",
    "    def experimentSetup4():\n",
    "        print(\"---Undergoing Experiment Setup #4---\")\n",
    "        print(\"---End of Experiment Setup #4---\")\n",
    "        return\n",
    "\n",
    "\n",
    "    # \"Model is a 3-layer ML with ReLU and dropout after each layer\": \n",
    "    #model = Sequential()\n",
    "    #model.add(Dense(hidden_units,input_dim=input_size))\n",
    "    #model.add(Activation('relu')) #<-- This is used to add activation function to model\n",
    "    #model.add(Dropout(dropout))\n",
    "    #model.add(Dense(hidden_units))\n",
    "    #model.add(Activation('relu'))\n",
    "    #model.add(Dropout(dropout))\n",
    "    #model.add(Dense(num_labels))\n",
    "    # \"This is the output for one-hot vector\"\n",
    "    #model.add(Activation('softmax')) #<-- May need this since I have a classification problem that I'm wokring on. \n",
    "    #model.summary()\n",
    "    #plot_model(model,to_file='mlp-mnist.png', show_shapes=True)\n",
    "\n",
    "    # \"Loss Function for one-hot vector\"\n",
    "    # \"Use of adam optimizer\"\n",
    "    # \"Accuracy is good metric for classification tasks\": \n",
    "    # print(\"---DEBUGGING CHECKPOINT #2: Making attempt to test before running Experiment 0---\")\n",
    "    # pb.set_trace()#[experimentSetup0 was successful]\n",
    "    \n",
    "    experimentSetup0() \n",
    "    # \"This is the output for one-hot vector\"\n",
    "    model.add(Activation('softmax')) #<-- May need this since I have a classification problem that I'm wokring on. \n",
    "    model.summary()\n",
    "    #plot_model(model,to_file='mlp-mnist.png', show_shapes=True)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "    # \"Train the network\"\n",
    "    # BUG: There is a bug that occurs here that prods the following message: ValueError: Unrecognized data type: x=  [Apparently, there is an unrecognized data type in x_train variable]\n",
    "    x_train.loc[:,\"Company\"] = pd.Series([2**i for i in range(x_train.shape[0])]) \n",
    "    x_test.loc[:,\"Company\"] = pd.Series([2**i for i in range(x_train.shape[0])]) \n",
    "    x_train = x_train.astype(\"float64\")\n",
    "    x_test = x_test.astype(\"float64\")\n",
    "    x_train.loc[:,\"Company\"] = x_train.loc[:,\"Company\"].astype(\"int32\")\n",
    "    x_test.loc[:,\"Company\"] = x_test.loc[:,\"Company\"].astype(\"int32\")\n",
    "    # y_train = pd.Series(y_train)\n",
    "    # y_test = pd.Series(y_test)\n",
    "    # NOTE: Body of Problem is coming from x_train and x_test!\n",
    "    x_trainCopy = tf.data.Dataset.from_tensor_slices((x_train.values.astype(np.float32),tf.convert_to_tensor(y_train).numpy().astype(np.float32)))\n",
    "    x_testCopy = tf.data.Dataset.from_tensor_slices((x_test.values.astype(np.float32),tf.convert_to_tensor(y_test).numpy().astype(np.float32)))\n",
    "    # NOTE: Body of Problem is coming from x_train and x_test!\n",
    "    # x_train = x_trainCopy \n",
    "    # x_test = x_testCopy \n",
    "\n",
    "    print(\"---DEBUGGING CHECKPOINT #3: Making attempt to test before training network---\")\n",
    "    pb.set_trace()\n",
    "    # UPDATE: FINISHED MODEL!!\n",
    "    train_history = model.fit(x_train,y_train,epochs=20, batch_size=batch_size) #<-- Removing this since I converted dataframe into tensorflow version of dataframe. [UPDATE: Have a problem now coming from mismatching shapes for x_train and y_train. Error is as follows: \"ValueError: Arguments `target` and `output` must have the same shape. Received: target.shape=(None, 1), output.shape=(None, 4)\"]\n",
    "    # train_history = model.fit(x_trainCopy,epochs=20, batch_size=batch_size) #<-- Removing this since I converted dataframe into tensorflow version of dataframe. \n",
    "    acc = model.evaluate(x_test, y_test, batch_size=batch_size,verbose=0)\n",
    "    # acc = model.evaluate(x_testCopy)\n",
    "    # print(\"---DEBUGGING CHECKPOINT #4: Obtaining Test Accuracy---\")\n",
    "    # pb.set_trace() [COMPLETE!]\n",
    "    # print(\"\\nTest accuracy: %.1f%%\" % (100.0 * acc))\n",
    "    print(\"\\nTest accuracy: %.1f%%\" % (100.0 * acc[1]))\n",
    "    test_stocks[\"Company\"] = test_stocks.index; test_stocks.loc[:,\"Company\"] = test_stocks.loc[:,\"Company\"].astype(\"int32\")\n",
    "\n",
    "    # 6) Verifying and Visualzing the Predictions\n",
    "    # a) Obtaining accuarrcy of the predictions: \n",
    "    \n",
    "    # predictions = model.predict(test_stocks) #<-- NOTE: This retunrns an array of probabilities for each class. Thus, for future consumption by person, could assign these predictions to a column added to test_stocks?\n",
    "    predictions = model.predict(x_test) #<-- NOTE: This retunrns an array of probabilities for each class. Thus, for future consumption by person, could assign these predictions to a column added to test_stocks?\n",
    "    \n",
    "    predictions[0]\n",
    "    \n",
    "    \n",
    "    np.argmax(predictions[0]) #<-- Returns the prediction that Neural Network was most comfortable with. \n",
    "    # def plot_image(i, predictions_array, true_label, img):\n",
    "        # true_label, img = true_label[i], img[i]\n",
    "        # plt.grid(False)\n",
    "        # plt.xticks([])\n",
    "        # plt.yticks([])\n",
    "        # plt.imshow(img, cmap=plt.cm.binary)\n",
    "        # predicted_label = np.argmax(predictions_array)\n",
    "\n",
    "        # if predicted_label == true_label:\n",
    "        #     color = 'blue'\n",
    "        # else:\n",
    "        #     color = 'blue'\n",
    "        \n",
    "        # plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label], 100*np.max(predictions_array), class_names[true_label], color=color))\n",
    "                   # NOTE as of 10/28/25: plot_image function will NOT be needed\n",
    "    # Body of original idea for plotting model's output: \n",
    "    def plot_value_array(i, predictions_array, true_label):\n",
    "        true_label = true_label[i]\n",
    "        plt.grid(False)\n",
    "        plt.xticks(range(10))\n",
    "        plt.yticks([])\n",
    "        thisplot = plt.bar(range(10), predictions_array, color=\"#7777777\")\n",
    "        plt.ylim([0,1])\n",
    "        predicted_label = np.argmax(predictions_array)\n",
    "\n",
    "        thisplot[predicted_label].set_color('red')\n",
    "        thisplot[true_label].set_color('blue')\n",
    "        return\n",
    "\n",
    "    # Below can potentially be done iteratively?\n",
    "    \"\"\" \n",
    "    i = 0\n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.subplot(1,2,1)\n",
    "    # plot_image(i,predictions[i],test_labels,test_images) <-- Not needed. \n",
    "    plt.subplot(1,2,2)\n",
    "    plot_value_array(i,predictions[i],test_labels)\n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "    # End of Body of original idea for plotting model's output: [UPDATE: Commented out this stuff above]\n",
    "\n",
    "\n",
    "    # Alternate way of doing plotting above[for clarity, alternative is referenced BELOW]. \n",
    "    print(\"---DEBUGGING CHECKPOINT #5: Tesiting plotting!---\")\n",
    "    pb.set_trace() # [COMPLETE!]\n",
    "    # Body of plotting model[Link for plotting is here: https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/] \n",
    "\n",
    "    ## Listing all data in history: \n",
    "    print(train_history.history.keys())\n",
    "\n",
    "    ## summarize train_history for accuracy\n",
    "    plt.plot(train_history.history['accuracy'])\n",
    "    # print(train_history.history['val_accuracy']) #<-- NOTE: val_accuracy and val_loss are NOT part of the history object's keys.  \n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    ## summarize history for loss\n",
    "    plt.plot(train_history.history['loss'])\n",
    "    # print(train_history.history['val_loss']) #<-- NOTE: val_accuracy and val_loss are NOT part of the history object's keys. \n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "    # End of Body of plotting model \n",
    "    # b) Printing Model Summary, which can be good to go into detail about: \n",
    "    model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # end of 6)\n",
    "# \"\"\"\n",
    "attempt3()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed5d43a",
   "metadata": {},
   "source": [
    "- Context: Then we set up the model parameters[part b)]: [note, make habit to employ content-codeSnippet pairs!]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12d6b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # ----May snippet above may be optional?[UPDATE: Above is required, need to come up with normalization process]---\n",
    "# \"Network Parameters\"\n",
    "# NOTE: The following will reference a list of tuple(s) that will be used to facilitate first experiment. \n",
    "listOfExperimentSetupFuncs = [\"\"\"Plan to insert functions for doing experiment(s) setup here\"\"\"]\n",
    "global batch_size, hidden_units, dropout\n",
    "global model #<-- Have this here, so experimental setup functions can tweak model as needed. \n",
    "batch_size = 128\n",
    "# batch_size = experiment1Tuples[0][0] \n",
    "hidden_units = 256\n",
    "# hidden_units = experiment1Tuples[0][1]\n",
    "dropout = 0.45\n",
    "# dropout  = experiment1Tuples[0][2]\n",
    "isExperimentSetup3Active = False\n",
    "# model = Sequential()\n",
    "model = Sequential()\n",
    "def experimentSetup0():\n",
    "    # This will reference the default settings irrespective to experiments. \n",
    "    print(\"---Undergoing Experiment Setup #0---\")\n",
    "    # model.add(Dense(hidden_units,input_dim=input_size))\n",
    "    # model.add(Dense(hidden_units,input_shape=(input_size-1,)))\n",
    "    # print(\"---DEBUGGING CHECKPOINT: Checking model's reactions---\")\n",
    "    # pb.set_trace()\n",
    "    # model.add(Input((input_size,)))\n",
    "    model.add(Input((x_train.shape[1],)))\n",
    "    # model.add(Dense(hidden_units,input_shape=(input_size,)))\n",
    "    model.add(Dense(hidden_units))\n",
    "    # NOTE: Above is causing following error: \"ValueError: Exception encountered when calling Sequential.call(). Invalid input shape for input Tensor(\"data:0\", shape=(5,), dtype=float32). Expected shape (None, 6), but input has incompatible shape (5,)\" [UPDATE: Error is originating from fact that x_trainCopy and x_testCopy 's shapes are (5,) and (5,)]\n",
    "    model.add(Activation('relu')) #<-- This is used to add activation function to model[UPDATE: May need to replace Activation('relu') by making them default...not sure to facilitate experimentSetup3 OR I can simply see what happens when the 2nd to LAST acivation is changed] \n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(hidden_units))\n",
    "    model.add(Activation('relu')) if experimentSetup3() == None else experimentSetup3() #<-- This changes the Activation function, adhering to experimentSetup0![Thus, as of 11/23/25: There are two experiments ready to go for Milestone 1]\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(num_labels))\n",
    "    print(\"---End of Experiment Setup #0---\")\n",
    "    return\n",
    "\n",
    "def experimentSetup1():\n",
    "    # Goal of exp: Want to see how model params affect the acuaracy of the model by modifying batch size and hidden units and dropout[pn: make sure to have a subset of the powerset of params be modified in this exp. ]\n",
    "    print(\"---Undergoing Experiment Setup #1---\")\n",
    "    setN = 0 #<-- Change number for this to get values from resp sets. \n",
    "    experiment1Tuples: list[tuple] = [(128,256,0.45), (64,128,0.45), (\"\"\"NOTE: Other tuples can change one or more parameters whilst keeping at least one constant\"\"\")]\n",
    "    batch_size = experiment1Tuples[setN][0] \n",
    "    hidden_units = experiment1Tuples[setN][1]\n",
    "    dropout = experiment1Tuples[setN][2]\n",
    "    \n",
    "    print(\"---End of Experiment Setup #1---\")\n",
    "    return\n",
    "def experimentSetup2(numQuantLvls = 2):\n",
    "    # Goal of exp: Want to see model performance based on degree of quantanization of data\n",
    "    print(\"---Undergoing Experiment Setup #2---\")\n",
    "    # NOTE: Below will involve replacing 255 with a different number based on degree of quantanization of data. \n",
    "    # UPDATE: Only thing left is pulling maximum value from x_train and min value from train and test respectively. \n",
    "    widthsOfQuant = []\n",
    "    \n",
    "    widthsOfQuant[0] = (x_trainMax - x_trainMin)/(numQuantLvls - 1)\n",
    "    x_train = np.reshape(x_train, [-1,input_size])\n",
    "    x_trainMax = 0;\n",
    "    x_trainMin = 0;\n",
    "    \n",
    "    x_test = np.reshape(x_test, [-1,input_size])\n",
    "    x_testMax = 0;\n",
    "    x_testMin = 0;\n",
    "    widthsOfQuant[1] = (x_testMax - x_testMin)/(numQuantLvls - 1); \n",
    "    x_train = x_train.astype('float32')/widthsOfQuant[0]\n",
    "    x_test = x_test.astype('float32')//widthsOfQuant[1]\n",
    "    print(\"---End of Experiment Setup #2---\")\n",
    "    return\n",
    "\n",
    "def experimentSetup3():\n",
    "    # Goal of exp: Want to see model performance based on type of activation function from a subset of all possible activation functions. \n",
    "    print(\"---Undergoing Experiment Setup #3---\" if isExperimentSetup3Active else \"---Experiment Setup #3 was skipped---\")\n",
    "    activationFuncs = ['elu', 'sigmoid', 'tanh' ]\n",
    "    # model.add(Activation(activationFuncs[0])) #<-- May need this since I have a classification problem that I'm wokring on. [UPDATE: Made this a return value instead] \n",
    "\n",
    "    print(\"---End of Experiment Setup #3---\" if isExperimentSetup3Active else \"---End of Experiment Setup #3 was skipped---\")\n",
    "\n",
    "    return model.add(Activation(activationFuncs[0])) if isExperimentSetup3Active == True else None #<-- May need this since I have a classification problem that I'm wokring on. \n",
    "\n",
    "def experimentSetup4():\n",
    "    print(\"---Undergoing Experiment Setup #4---\")\n",
    "    print(\"---End of Experiment Setup #4---\")\n",
    "    return\n",
    "\n",
    "\n",
    "# \"Model is a 3-layer ML with ReLU and dropout after each layer\": \n",
    "#model = Sequential()\n",
    "#model.add(Dense(hidden_units,input_dim=input_size))\n",
    "#model.add(Activation('relu')) #<-- This is used to add activation function to model\n",
    "#model.add(Dropout(dropout))\n",
    "#model.add(Dense(hidden_units))\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(Dropout(dropout))\n",
    "#model.add(Dense(num_labels))\n",
    "# \"This is the output for one-hot vector\"\n",
    "#model.add(Activation('softmax')) #<-- May need this since I have a classification problem that I'm wokring on. \n",
    "#model.summary()\n",
    "#plot_model(model,to_file='mlp-mnist.png', show_shapes=True)\n",
    "\n",
    "# \"Loss Function for one-hot vector\"\n",
    "# \"Use of adam optimizer\"\n",
    "# \"Accuracy is good metric for classification tasks\": \n",
    "# print(\"---DEBUGGING CHECKPOINT #2: Making attempt to test before running Experiment 0---\")\n",
    "# pb.set_trace()#[experimentSetup0 was successful]\n",
    "\n",
    "experimentSetup0() \n",
    "experimentSetup2() \n",
    "\n",
    "# \"This is the output for one-hot vector\"\n",
    "model.add(Activation('softmax')) #<-- May need this since I have a classification problem that I'm wokring on. \n",
    "model.summary()\n",
    "#plot_model(model,to_file='mlp-mnist.png', show_shapes=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer='adam',\n",
    "metrics=['accuracy'])\n",
    "\n",
    "# \"Train the network\"\n",
    "# BUG: There is a bug that occurs here that prods the following message: ValueError: Unrecognized data type: x=  [Apparently, there is an unrecognized data type in x_train variable]\n",
    "x_train.loc[:,\"Company\"] = pd.Series([2**i for i in range(x_train.shape[0])]) \n",
    "x_test.loc[:,\"Company\"] = pd.Series([2**i for i in range(x_train.shape[0])]) \n",
    "x_train = x_train.astype(\"float64\")\n",
    "x_test = x_test.astype(\"float64\")\n",
    "x_train.loc[:,\"Company\"] = x_train.loc[:,\"Company\"].astype(\"int32\")\n",
    "x_test.loc[:,\"Company\"] = x_test.loc[:,\"Company\"].astype(\"int32\")\n",
    "# y_train = pd.Series(y_train)\n",
    "# y_test = pd.Series(y_test)\n",
    "# NOTE: Body of Problem is coming from x_train and x_test!\n",
    "x_trainCopy = tf.data.Dataset.from_tensor_slices((x_train.values.astype(np.float32),tf.convert_to_tensor(y_train).numpy().astype(np.float32)))\n",
    "x_testCopy = tf.data.Dataset.from_tensor_slices((x_test.values.astype(np.float32),tf.convert_to_tensor(y_test).numpy().astype(np.float32)))\n",
    "# NOTE: Body of Problem is coming from x_train and x_test!\n",
    "# x_train = x_trainCopy \n",
    "# x_test = x_testCopy \n",
    "\n",
    "print(\"---DEBUGGING CHECKPOINT #3: Making attempt to test before training network---\")\n",
    "pb.set_trace()\n",
    "# UPDATE: FINISHED MODEL!!\n",
    "train_history = model.fit(x_train,y_train,epochs=20, batch_size=batch_size) #<-- Removing this since I converted dataframe into tensorflow version of dataframe. [UPDATE: Have a problem now coming from mismatching shapes for x_train and y_train. Error is as follows: \"ValueError: Arguments `target` and `output` must have the same shape. Received: target.shape=(None, 1), output.shape=(None, 4)\"]\n",
    "# train_history = model.fit(x_trainCopy,epochs=20, batch_size=batch_size) #<-- Removing this since I converted dataframe into tensorflow version of dataframe. \n",
    "acc = model.evaluate(x_test, y_test, batch_size=batch_size,verbose=0)\n",
    "# acc = model.evaluate(x_testCopy)\n",
    "# print(\"---DEBUGGING CHECKPOINT #4: Obtaining Test Accuracy---\")\n",
    "# pb.set_trace() [COMPLETE!]\n",
    "# print(\"\\nTest accuracy: %.1f%%\" % (100.0 * acc))\n",
    "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * acc[1]))\n",
    "test_stocks[\"Company\"] = test_stocks.index; test_stocks.loc[:,\"Company\"] = test_stocks.loc[:,\"Company\"].astype(\"int32\")\n",
    "\n",
    "# 6) Verifying and Visualzing the Predictions\n",
    "# a) Obtaining accuarrcy of the predictions: \n",
    "\n",
    "# predictions = model.predict(test_stocks) #<-- NOTE: This retunrns an array of probabilities for each class. Thus, for future consumption by person, could assign these predictions to a column added to test_stocks?\n",
    "predictions = model.predict(x_test) #<-- NOTE: This retunrns an array of probabilities for each class. Thus, for future consumption by person, could assign these predictions to a column added to test_stocks?\n",
    "\n",
    "predictions[0]\n",
    "\n",
    "\n",
    "np.argmax(predictions[0]) #<-- Returns the prediction that Neural Network was most comfortable with. \n",
    "# def plot_image(i, predictions_array, true_label, img):\n",
    "    # true_label, img = true_label[i], img[i]\n",
    "    # plt.grid(False)\n",
    "    # plt.xticks([])\n",
    "    # plt.yticks([])\n",
    "    # plt.imshow(img, cmap=plt.cm.binary)\n",
    "    # predicted_label = np.argmax(predictions_array)\n",
    "\n",
    "    # if predicted_label == true_label:\n",
    "    #     color = 'blue'\n",
    "    # else:\n",
    "    #     color = 'blue'\n",
    "    \n",
    "    # plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label], 100*np.max(predictions_array), class_names[true_label], color=color))\n",
    "                # NOTE as of 10/28/25: plot_image function will NOT be needed\n",
    "# Body of original idea for plotting model's output: \n",
    "def plot_value_array(i, predictions_array, true_label):\n",
    "    true_label = true_label[i]\n",
    "    plt.grid(False)\n",
    "    plt.xticks(range(10))\n",
    "    plt.yticks([])\n",
    "    thisplot = plt.bar(range(10), predictions_array, color=\"#7777777\")\n",
    "    plt.ylim([0,1])\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "\n",
    "    thisplot[predicted_label].set_color('red')\n",
    "    thisplot[true_label].set_color('blue')\n",
    "    return\n",
    "\n",
    "# Below can potentially be done iteratively?\n",
    "\"\"\" \n",
    "i = 0\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.subplot(1,2,1)\n",
    "# plot_image(i,predictions[i],test_labels,test_images) <-- Not needed. \n",
    "plt.subplot(1,2,2)\n",
    "plot_value_array(i,predictions[i],test_labels)\n",
    "plt.show()\n",
    "\"\"\"\n",
    "# End of Body of original idea for plotting model's output: [UPDATE: Commented out this stuff above]\n",
    "\n",
    "\n",
    "# Alternate way of doing plotting above[for clarity, alternative is referenced BELOW]. \n",
    "# print(\"---DEBUGGING CHECKPOINT #5: Tesiting plotting!---\")\n",
    "# pb.set_trace() # [COMPLETE!]\n",
    "\"\"\" \n",
    "NOTE: Not needed below since I need to have model plotting be visible everywhere. \n",
    "# Body of plotting model[Link for plotting is here: https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/] \n",
    "\n",
    "## Listing all data in history: \n",
    "print(train_history.history.keys())\n",
    "\n",
    "## summarize train_history for accuracy\n",
    "plt.plot(train_history.history['accuracy'])\n",
    "# print(train_history.history['val_accuracy']) #<-- NOTE: val_accuracy and val_loss are NOT part of the history object's keys.  \n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "## summarize history for loss\n",
    "plt.plot(train_history.history['loss'])\n",
    "# print(train_history.history['val_loss']) #<-- NOTE: val_accuracy and val_loss are NOT part of the history object's keys. \n",
    "plt.title('model loss')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    " \"\"\"\n",
    "\n",
    "\n",
    "# End of Body of plotting model \n",
    "# b) Printing Model Summary, which can be good to go into detail about: \n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# end of 6)\n",
    "# \"\"\"\n",
    "# attempt3() <-- NOT needed for jupyter notebook!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea57340",
   "metadata": {},
   "source": [
    "### Results \n",
    "- Context: [note, make habit to employ content-codeSnippet pairs![Make sure results section consists of python code that utilizes data visualization]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cd76cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## summarize history for loss\n",
    "plt.plot(train_history.history['loss'])\n",
    "# print(train_history.history['val_loss']) #<-- NOTE: val_accuracy and val_loss are NOT part of the history object's keys. \n",
    "plt.title('model loss')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# End of Body of plotting model \n",
    "# b) Printing Model Summary, which can be good to go into detail about: \n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bda019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Body of plotting model[Link for plotting is here: https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/] \n",
    "\n",
    "## Listing all data in history: \n",
    "print(train_history.history.keys())\n",
    "\n",
    "## summarize train_history for accuracy\n",
    "plt.plot(train_history.history['accuracy'])\n",
    "# print(train_history.history['val_accuracy']) #<-- NOTE: val_accuracy and val_loss are NOT part of the history object's keys.  \n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44da4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code that displays data visualization here[COMPLETE, can be seen above. At this point, need to copy and paste above, as template for next experiment below!]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddc46e4",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "- Context: [make sure this portion consists of summary of results and determine if hypothesis was met or unmet]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
